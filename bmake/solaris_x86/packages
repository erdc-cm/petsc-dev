# $Id: base.site,v 1.4 2001/03/27 22:17:30 balay Exp $ 

#
#  This file contains site-specific information.  The definitions below
#  should be changed to match the locations of libraries at your site.
#  The following naming convention is used:
#     XXX_LIB - location of library XXX
#     XXX_INCLUDE - directory for include files needed for library XXX
#
# Location of BLAS and LAPACK.  These libraries are available via Netlib,
# or see ${PETSC_DIR}/docs/installation.html for information on retrieving them.
#
# If your machine has the file /opt/SUNWspro/SC4.0/lib/libsunperf.a you 
# can use -lsunperf or /opt/SUNWspro/SC4.0/lib/libsunperf.a below instead of
# installing BLAS and LAPACK yourself.
#
BLASLAPACK_LIB = -L/home/barad-dur/jwd/users/bfsmith/blaslapack -lflapack -lfblas
#
# Location of MPI (Message Passing Interface) software
#
# Note: if ch_shmem version of mpich is used, replace -lsocket by -lthread
#
MPI_LIB        = -L/usr/now/mpi/mpich/lib/solaris/am2i86pc -L/usr/now/lib \
                 -lmpi -lam2 -lens -leutl -lglunix -lthread -lposix4 \
                 -lsocket -lnsl
MPI_INCLUDE    = -I/usr/now/mpi/mpich/include
MPIRUN         = /usr/now/mpi/mpich/bin/mpirun
#
# ----------------------------------------------------------------------------------------  
#  Locations of OPTIONAL packages. Comment out those you do not have.
# ----------------------------------------------------------------------------------------  
#
# Location of X-windows software
#
X11_INCLUDE    = -I/usr/openwin/include
X11_LIB        = -L/usr/openwin/lib -lX11
PETSC_HAVE_X11 = -DPETSC_HAVE_X11
#
# Location of MPE
# If using MPICH version 1.1.2 or higher use the flag -DPETSC_HAVE_MPE_INITIALIZED_LOGGING
#
#MPE_INCLUDE   = -I/home/petsc/mpich/mpe
#MPE_LIB       = -L/home/petsc/mpich/lib/solaris/ch_p4/ -lmpe -lpmpi
#PETSC_HAVE_MPE = -DPETSC_HAVE_MPE
#
# Location of BlockSolve (MPI version)
#
#BLOCKSOLVE_INCLUDE = -I/home/petsc/BlockSolve95/include
#BLOCKSOLVE_LIB     = -L/home/petsc/BlockSolve95/lib/libO/${PETSC_ARCH} -lBS95
#PETSC_HAVE_BLOCKSOLVE = -DPETSC_HAVE_BLOCKSOLVE
#
# Matlab location
#
#MATLAB_MEX            = 
#MATLAB_CC            = 
#MATLAB_COMMAND  = matlab
#PETSC_HAVE_MATLAB =  -DPETSC_HAVE_MATLAB
#
# Location where adiC is installed
#
#ADIC_DEFINES    = 
#ADIC_CC         = adiC -a -d gradient
#PETSC_HAVE_ADIC = -DPETSC_HAVE_ADIC
#
# Location of PVODE; Alan Hindmarsh's parallel ODE solver
# 
#PVODE_INCLUDE = -I/home/petsc/software/MPI_PVODE/include
#PVODE_LIB     = /home/petsc/software/MPI_PVODE/lib/solaris/libpvode.a
#PETSC_HAVE_PVODE = -DPETSC_HAVE_PVODE
#
# Location of ParMetis
#
#PARMETIS_INCLUDE = -I/home/bsmith/libraries/ParMetis.v1.0
#PARMETIS_LIB     = /home/bsmith/libraries/ParMetis.v1.0/libparmetis_solaris.a 
#PETSC_HAVE_PARMETIS = -DPETSC_HAVE_PARMETIS
#
# ---------------------------------------------------------------------------------------
#
# If you are using shared version of any external libraries you must make this
# point to the directories where all your shared libraries are stored.
#
#C_DYLIBPATH     = 
#F_DYLIBPATH     = 
