# $Id: packages,v 1.94 2001/08/07 03:01:20 balay Exp $ 

#
#  This file contains site-specific information.  The definitions below
#  should be changed to match the locations of libraries at your site.
#  The following naming convention is used:
#     XXX_LIB - location of library XXX
#     XXX_INCLUDE - directory for include files needed for library XXX
#
# Location of BLAS and LAPACK.  See ${PETSC_DIR}/docs/installation.html
# for information on retrieving them.
#
# If your machine has the file /opt/SUNWspro/SC*/lib/libsunperf.a you 
# can use -lsunperf or /opt/SUNWspro/SC*/lib/libsunperf.a below instead of
# installing BLAS and LAPACK yourself.
#
BLASLAPACK_LIB  =  -lsunperf
#
# Location of MPI (Message Passing Interface) software
#
#                    !!!!!! WARNING !!!!!!!! 
# run 'mpich/lib/solaris/ch_p4/mpicc -show' to find the list of 
# libs required by MPI, and use them in the SAME order!
# If using ch_shmem, run 'mpich/lib/solaris/ch_shmem/mpicc -show'
#
#MPI_LIB        = -L/home/petsc/mpich/lib/solaris/ch_p4/ -lmpi -lsocket -lnsl
#MPI_INCLUDE    = -I/home/petsc/mpich/include 
#MPIRUN         = /home/petsc/mpich/lib/solaris/ch_p4/mpirun
MPI_HOME       = /home/petsc/software/mpich-1.1.2
#MPI_HOME       = /home/MPI/releases/mpich/
MPI_BUILD_HOME = ${MPI_HOME}/build/solaris/ch_p4

MPI_LIB        = -L${MPI_BUILD_HOME}/lib -lpmpich -lmpich -lsocket -lnsl
MPI_INCLUDE    = -I${MPI_HOME}/include -I${MPI_BUILD_HOME}/include
MPIRUN         = ${MPI_BUILD_HOME}/bin/mpirun -machinefile ${PETSC_DIR}/maint/hosts.local
#
#
# ----------------------------------------------------------------------------------------  
#  Locations of OPTIONAL packages. Comment out those you do not have.
# ----------------------------------------------------------------------------------------  
#
# Location of X-windows software
#
X11_INCLUDE    = -I/usr/openwin/include
X11_LIB        = -L/usr/openwin/lib -lX11
PETSC_HAVE_X11 = -DPETSC_HAVE_X11
#
# Location of MPE
# If using MPICH version 1.1.2 or higher use the flag -DPETSC_HAVE_MPE_INITIALIZED_LOGGING
#
MPE_INCLUDE    = -I${MPI_HOME}/mpe -DPETSC_HAVE_MPE_INITIALIZED_LOGGING
MPE_LIB        = -L${MPI_BUILD_HOME}/lib -lmpe
PETSC_HAVE_MPE = -DPETSC_HAVE_MPE
#
# Location of BlockSolve (MPI version)
#
BLOCKSOLVE_INCLUDE    = -I/home/petsc/software/BlockSolve95/include
BLOCKSOLVE_LIB        = ${CLINKER_SLFLAG}/home/petsc/software/BlockSolve95/lib/libO/solaris -L/home/petsc/software/BlockSolve95/lib/libO/${PETSC_ARCH} -lBS95
PETSC_HAVE_BLOCKSOLVE = -DPETSC_HAVE_BLOCKSOLVE
#
# Matlab location
#
MATLAB_ROOT       = /software/solaris-2/com/packages/matlab-6.1
CMEX              = ${MATLAB_ROOT}/bin/mex
#CMEX             = mex
MCC               = cc
MATLABCOMMAND     = matlab
PETSC_HAVE_MATLAB =  -DPETSC_HAVE_MATLAB
#
#  The Matlab include file have some bugs in them with C++,  they have a 
# #include <stdlib.h> INSIDE an extern "C" {} :-(. You can fix this by moving the 
# the #include <stdio.h> and #include "matrix.h" outside the extern "C" {} in engine.h
# and moving the #include <stddef.h> and #include <stdlib.h> outside the extern "C" {}
# in matrix.h (sigh)
#
#MATLAB_INCLUDE    = -I${MATLAB_ROOT}/extern/include
MATLAB_INCLUDE    = -I/home/petsc/software/matlab/include
MATLAB_LIB        = ${CLINKER_SLFLAG}${MATLAB_ROOT}/extern/lib/sol2 -L${MATLAB_ROOT}/extern/lib/sol2 -leng -lmx -lmat -lmi -lut
PETSC_HAVE_MATLAB_ENGINE =  -DPETSC_HAVE_MATLAB_ENGINE
#
# Location where adiC is installed
#
#ADIC_DEFINES    =  -Dad_GRAD_MAX=1
#ADIC_CC         = adiC -E -aeud gradient -i ${PETSC_DIR}/bmake/adicmastercontrol
#PETSC_HAVE_ADIC = -DPETSC_HAVE_ADIC
#
# Location of PVODE; Alan Hindmarsh's parallel ODE solver
# 
#PVODE_INCLUDE    = -I/home/petsc/software/MPI_PVODE/include
#PVODE_LIB        = /home/petsc/software/MPI_PVODE/lib/solaris/libpvode.a
#PETSC_HAVE_PVODE = -DPETSC_HAVE_PVODE
#
# Location of ParMetis
#
PARMETIS_INCLUDE    = -I/home/petsc/software/ParMetis-2.0
PARMETIS_LIB        = -L/home/petsc/software/ParMetis-2.0/lib/solaris -lparmetis -lmetis
PETSC_HAVE_PARMETIS = -DPETSC_HAVE_PARMETIS
#
#  Location for ALICE Memory Snooper
#
AMS_HOME       = /home/alice/ams
AMS_INCLUDE    = -I${AMS_HOME}/include
AMS_LIBDIR     = ${AMS_HOME}/lib/lib/solaris
AMS_LIB        = -L${AMS_LIBDIR} -lamspub -lamsutilmt -lamsutil -lthread -lldap
PETSC_HAVE_AMS = -DPETSC_HAVE_AMS
#
# Location of SPAI;
#
SPAI_INCLUDE    = -I/home/petsc/software/spai_3.0/lib
SPAI_LIB        = /home/petsc/software/spai_3.0/lib/${PETSC_ARCH}/libspai.a
PETSC_HAVE_SPAI = -DPETSC_HAVE_SPAI
#
# Location of JAVA requires at least java 1.3
# 
#JAVAC           = /usr/java1.2/bin/javac
#JAVA            = /usr/java1.2/bin/java
#JAR             = /usr/java1.2/bin/jar
#PETSC_HAVE_JAVA = -DPETSC_HAVE_JAVA
#
# Location/name of Python
#
PETSC_PYTHON    = -DPETSC_PYTHON="python1.5"
#
# Location of the LUSOL sparse LU factorization code (part of MINOS)
# developed by Michael Saunders, saunders@stanford.edu at the
# Systems Optimization Laboratory, Stanford University.
#  http://www.sbsi-sol-optimize.com/
# Uses the two files mi25bfac.f and mi15blas.f (or LUSOL.f LUSOL_BLAS.f
# depending on how they are named)
#
#PETSC_HAVE_LUSOL     = -DPETSC_HAVE_LUSOL
#LUSOL_LIB
#
#
# Location of DSCPACKS
#
DSCPACK_INCLUDE     = -I/home/petsc/software/DSCPACK-S/DSC_LIB -DDBL_R_NUM
DSCPACK_LIB         = /home/petsc/software/DSCPACK-S/DSC_LIB/dsclibdbl.a
PETSC_HAVE_DSCPACKS  = -DPETSC_HAVE_DSCPACKS


RAMG_LIB        = /home/petsc/software/ramg/libramg_solaris.a
PETSC_HAVE_RAMG = -DPETSC_HAVE_RAMG
