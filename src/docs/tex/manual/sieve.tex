\section{Handling Unstructured Grids}

    The main idea behind the \emph{Sieve} system for handling unstructured grids in PETSc is to separate the problem
into two pieces: local interaction with unstructured meshes and global interaction between meshes distribute across a
group of processes. In fact, the second piece may be used to stitch together pieces of a DA in exactly the same way, as
will be described below. Our goal is to provide an interface independent of spatial dimension, discretization, and
details of the mesh, but does provide more structure than a purely linear-algebraic interface.

\subsection{Global Interaction}

    The \code{VecScatter} mechanism in PETSc provides a structured way to move values among a group of
processes. However, when applied at the application level is has a few drawbacks. \code{VecScatter} demands that the
user provide indices to describe the mapping between vectors, but most application codes manipulate higher level
objects, such as elements, making this mechanism too fine-grained. Moreover, \code{VecScatter} allows only two methods
for combining remote and local values, addition and replacement, and only allows a one-to-one mapping between values.

    The Sieve model is to split the scatter information into two parts, a mapping between Sieve points on different
processes, called an \code{Overlap}, and an assignment of degrees of freedom (dof) to sieve points, called an
\code{Atlas}. When these mappings are composed, we recover the dof matching used to create a \code{VecScatter}. However,
dividing them allows the user more flexibility when creating parallel constructs.

There are three levels of data representation (provide picture)
\begin{itemize}
  \item Local

  \item Process

  \item Global
\end{itemize}
At the local level, we present data in a form specifically tied to the geometry and discretization. For instance, finite
element methods will be given an element vector, finite volume methods a vector of fluxes, and finite difference methods
a stencil. At the process level, we create storage which is partially assembled over the process subdomain, but
unassembled between processes. The Sieve calls these objects \code{Sections}, which become PETSc \code{VecGhost} when
reduced to linear algebra operations. A single piece of the process level representation is called \emph{local vector}
by the DMDA. Global representatives are completely assembled, and stored in \code{Vec} objects.

    The simplest example of communication is the trading of ghost values in a DMDA. These values can be communicated in a
number of ways. First, the user can just call
\begin{quote}
  \code{Mesh}
\end{quote}


\subsection{Local Interaction}

    Here we deviate from the DMDA model in order to allow more flexible data layout, and also an access interface which is
independent of mesh dimension, element shape, and discretization.

