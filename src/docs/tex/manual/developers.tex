% $Id: developers.tex,v 1.18 2000/09/22 21:35:11 balay Exp $ 
%
% LATEX version of the PETSc users manual.
%
% manual_tex.tex is the base file for LaTeX format, while manual.tex is
% the corresponding base HTML format file.
%
\documentclass[twoside,12pt]{../sty/report_petsc}
%\usepackage{psfig}
\usepackage{../sty/verbatim}
\usepackage{../sty/tpage}
\usepackage{../sty/here}
\usepackage{../sty/anlhelper}

\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{0.0in}
\setlength{\evensidemargin}{0.0in}
\setlength{\textheight}{9.2in}
\setlength{\topmargin}{-.8in}

\newcommand{\findex}[1]{\index{FUNCTION #1}}
\newcommand{\sindex}[1]{\index{#1}}
\newcommand{\F}{\mbox{\boldmath \(F\)}}
\newcommand{\x}{\mbox{\boldmath \(x\)}}
\newcommand{\rr}{\mbox{\boldmath \(r\)}}

\makeindex
 

\begin{document}

\ANLTitle{PETSc Developers Manual}{\em Satish Balay\\
William Gropp\\ Lois Curfman McInnes\\ Barry Smith\\
\hspace{0.3in}\\
Mathematics and Computer Science Division \\
http://www.mcs.anl.gov/petsc
\vspace{0.5in} \\
{\rm This document is intended for use with PETSc 2.2.0}}
{\Large This is not yet an official ANL document}{1998}

\newpage

\hbox{ }

\vspace{1in}
\date{\today}

% Blank page makes double sided printout look bettter.
\newpage


\addcontentsline{toc}{chapter}{Abstract}

\medskip \medskip


%
%   Next line temp removed
%
\noindent {\bf Abstract:} 

\medskip \medskip
PETSc is 
a set of extensible software libraries for scientific computation.
PETSc is designed using a component-based object-oriented
architecture. This means that libraries consist of {\em objects} that
have a certain, defined functionality. This document defines how these
objects are implemented.

This manual discusses the PETSc component library design and describes
how to develop new library codes that are compatible with other PETSc components 
including PETSc 2.2.
The idea is not to develop one massive library that everyone shoves code 
into; rather, to develop an architecture that allows many people
to (as painlessly as possible) contribute (and maintain) their own libraries,
in a distributed fashion.

The text assumes
that you are familiar with PETSc, have a copy of the PETSc users
manual, and have access to PETSc source code and documentation
(available via {\tt http://www.mcs.anl.gov/petsc}).


\vspace{1cm}

This is the first draft of the PETSc developers manual;
therefore, it is very sketchy and incomplete. Please
direct all comments and questions regarding PETSc design and
development to {\tt petsc-developers@mcs.anl.gov}.  Note that all {\em
bug reports and questions regarding the use of PETSc} should continue
to be directed to {\tt petsc-maint@mcs.anl.gov}.

The PETSc team is part of a larger group (mostly at the DOE labs)
funded by the DOE2000 initiative to develop a Common Component
Architecture (CCA), http://www.... In addition, we are involved in a
related DOE lab initiative to develop a standard object interface for
scalable linear solvers. Already as a result of these efforts we have modified
the PETSc software design and will continue to evolve PETSc as these
initiatives progress. 

%
% NOTES:  
%  - Be sure to place captions BEFORE labels in figures and tables!
%    Otherwise, numbering will be incorrect.  For example, use the following:
%       \caption{PETSc Vector Operations}
%       \label{fig:vectorops}
%  - Use \break to indicate a line break (needed to prevent long strings in
%    \tt mode from running of the page)
%

% Blank page makes double sided printout look bettter.
\newpage
\hbox{ }
\newpage

% -----------------------------------------------------------------------------------
\chapter{The PETSc Kernel}
\label{chapter:kernel}
PETSc provides a variety of basic services for writing scalable, component 
based libraries; these are referred to as the PETSc kernel. The source
code for the kernel is in {\tt src/sys/src}. It contains systematic support for
\begin{itemize}
  \item error handling
  \item memory management
  \item profiling
  \item object management
  \item file IO
  \item options database
\end{itemize}
Each of these is discussed in a section below.

% -------------------------------------------------------------------------------
\section{Implementation of Error Handling}

PETSc  uses a ``call error handler; then (depending on result) return
error code'' model when problems are detected in the running code. 

The public include file for error handling is {\tt include/petscerror.h}, the 
source code for the PETSc error handling is in  
{\tt src/sys/src/error/}.

\subsection{Simplified Interface}

The simplified C/C++ macro-based interface consists of the following three calls
\begin{itemize}
\item SETERRQ(generic error code,specific error code,''Error message'');
\item CHKERRQ(ierr);
\item CHKPTRQ(ptr);
\end{itemize}

The macro {\tt SETERRQ()} is given by 
\begin{verbatim}

return PetscError(__LINE__,__FUNC__,__FILE__,__SDIR__,generic,specific,''Error message'');

\end{verbatim}
It calls the error handler with the current function name and location: line number,
file and directory, plus two integer error codes and an error message.
The macro {\tt CHKERRQ()} is defined by
\begin{verbatim}

  if (ierr) SETERRQ(ierr,0,(char *)0);

\end{verbatim}
and {\tt CHKPTRQ()} by 
\begin{verbatim}

  if (!ptr) SETERRQ(PETSC_ERR_MEM,0,(char*)0);

\end{verbatim}

Different versions of the macros must be used in the {\tt main()} program:
{\tt SETERRA(), CHKERRA()}, and {\tt CHKPTRA()}. The reason for the different versions
is that when an error is detected in the main program one cannot return, because 
this would not call the {\tt MPI\_Abort()} routine and would leave MPI processes hanging.
Hence these versions of the macros call {\tt MPI\_Abort()}, rather than returning.

In addition to {\tt SETERRQ()} are the macros {\tt SETERRQ1()}, {\tt SETERRQ2()}, {\tt SETERRQ3()}
and {\tt SETERRQ4()} that allow one to include additional arguments that the message
string is formated. For example,
\begin{verbatim}
  SETERRQ2(1,26,''Iteration overflow: its %d norm %g'',its,norm);
\end{verbatim}
The reason for the numbered format is because CPP macros cannot handle variable number
of arguments.
 
\subsection{Error Handlers}
The error handling function {\tt PetscError()} calls the ``current'' error handler
with the code
\begin{verbatim}
int PetscError(int line,char *func,char* file,char *dir,int n,int p,char *mess)
{
  int ierr;

  PetscFunctionBegin;
  if (!eh)     ierr = PetscTraceBackErrorHandler(line,func,file,dir,n,p,mess,0);
  else         ierr = (*eh->handler)(line,func,file,dir,n,p,mess,eh->ctx);
  PetscFunctionReturn(ierr);
}
\end{verbatim}
The variable {\tt eh} is the current error handler context and is defined in 
{\tt src/sys/src/error/err.c} as
\begin{verbatim}
typedef struct _EH* EH;
struct _EH {
  int    cookie;
  int    (*handler)(int, char*,char*,char *,int,int,char*,void *);
  void   *ctx;
  EH     previous;
};
\end{verbatim}

One can set a new error handler with the command 
\begin{verbatim}
int PetscPushErrorHandler(int (*handler)(int,char *,char*,char*,int,int,char*,void*),
                          void *ctx )
{
  EH neweh = (EH) PetscMalloc(sizeof(struct _EH)); CHKPTRQ(neweh);

  PetscFunctionBegin;
  if (eh) {neweh->previous = eh;} 
  else    {neweh->previous = 0;}
  neweh->handler = handler;
  neweh->ctx     = ctx;
  eh             = neweh;
  PetscFunctionReturn(0);
}
\end{verbatim}
which maintains a linked list of error handlers. The most recent error handler is removed
via
\begin{verbatim}
int PetscPopErrorHandler(void)
{
  EH tmp;

  PetscFunctionBegin;
  if (!eh) PetscFunctionReturn(0);
  tmp = eh;
  eh  = eh->previous;
  PetscFree(tmp);

  PetscFunctionReturn(0);
}
\end{verbatim}

PETSc provides several default error handlers
\begin{itemize}
\item {\tt PetscTraceBackErrorHandler()},
\item {\tt PetscAbortErrorHandler()}, 
\item {\tt PetscStopErrorHandler()}, and
\item {\tt PetscAttachDebuggerErrorHandler()}. 
\end{itemize}

\subsection{Error Codes}

The PETSc error handler take a generic error code.
The generic error codes are defined in {\tt include/petscerror.h}, the same generic
error code would be used many times in the libraries. For example the 
generic error code {\tt PETSC\_ERR\_MEM} is used whenever requested memory allocation
is not available. 

\subsection{Detailed Error Messages}
In a modern parallel component oriented application code it does not make sense
to simply print error messages to the screen (more than likely there is no 
``screen'', for example with Windows applications).
PETSc provides the replaceable function pointer 
\begin{verbatim}

   (*PetscErrorPrintf)(``Format'',...);

\end{verbatim}
that, by default prints to standard error. Thus error messages should not
be printed with printf() or fprintf() rather than should be printed with
{\tt (*PetscErrorPrintf)()}.


% -----------------------------------------------------------------------------------
\section{Implementation of Profiling}
\label{sec:profimpl}

This section provides details about the implementation of event
logging and profiling within the PETSc kernel.
The interface for profiling in PETSc is contained in the file 
{\tt \$\{PETSC\_DIR\}/include/petsclog.h}. The source code for the profile logging
is in {\tt src/sys/src/plog/}.

\subsection{Profiling Object Create and Destruction}

The creation of objects may be profiled with the command
 \findex{PLogObjectCreate()}
\begin{verbatim}
   PLogObjectCreate(PetscObject h);
\end{verbatim}
which logs the creation of any PETSc object. 
Just before an object is destroyed, it should be  logged with
with \findex{PLogObjectDestroy()} 
\begin{verbatim}
   PLogObjectDestroy(PetscObject h);
\end{verbatim}

If an object has a clearly defined parent object (for instance, when 
a work vector is generated for use in a Krylov solver), this information
is logged with the command, \findex{PLogObjectParent()}
\begin{verbatim}
   PLogObjectParent(PetscObject parent,PetscObject child);
\end{verbatim}
It is also useful to log information about the state of an object, as can
be done with the command \findex{PLogObjectState()}
\begin{verbatim}
   PLogObjectState(PetscObject h,char *format,...);
\end{verbatim}
For example, for sparse matrices we usually log the matrix 
dimensions and number of nonzeros.

\subsection{Profiling Events}

Events are logged using the 
pair \findex{PLogEventBegin()}
\begin{verbatim}
   PLogEventBegin(int event,PetscObject o1,PetscObject o2,PetscObject o3,PetscObject o4);
   PLogEventEnd(int event,PetscObject o1,PetscObject o2,PetscObject o3,PetscObject o4);
\end{verbatim}
This logging is usually done in the abstract
interface file for the operations, for example, {\tt src/mat/src/matrix.c}.

\subsection{Controling Profiling}

Several routines that control the default profiling available in PETSc are
\findex{PLogPrintSummary()}
are \findex{PLogBegin()} \findex{PLogDump()} \findex{PLogAllBegin()} 
\begin{verbatim}
   PLogBegin();
   PLogAllBegin();
   PLogDump(char *filename);
   PLogPrintSummary(FILE *fd);
\end{verbatim}
These routines are normally called by the {\tt PetscInitialize()}
and {\tt PetscFinalize()} routines when the option {\tt -log}, 
{\tt -log\_summary}, or 
{\tt -log\_all} is given.

\subsection{Setting Profile Handlers}
It is possible to set new profile handlers using the commands:

% -----------------------------------------------------------------------------------
\chapter{Basic Object Design}
\label{chapter:design}

PETSc is designed using strong data encapsulation.  Hence,
any collection of data (for instance, a sparse matrix) is stored in 
a way that is completely private from the application code. The application 
code can manipulate the data only through a well-defined interface, as it 
does {\em not} ``know'' how the data is stored internally. 

\section{Introduction}

PETSc is designed around several components (e.g. {\tt Vec} (vectors),
{\tt Mat} (matrices, both dense and sparse)). These components are each 
implemented using C {\tt struct}s, that contain the data and function pointers
for operations on the data (much like virtual functions in classes in C++). 
Each components consists of three parts: 
\begin{itemize}
\item a (small) common part shared by all PETSc compatible libraries.
\item another common part shared by all PETSc implementations of the component and
\item a private part used by only one particular implementation written in PETSc.
\end{itemize}
For example, all matrix (Mat) components share a function table of operations that 
may be performed on the matrix; all PETSc matrix implementations share some additional
data fields,including matrix size; while a particular matrix implementation in PETSc
(say compressed sparse row) has its own data fields for storing the actual
matrix values and sparsity pattern. This will be explained in more detail
in the following sections. People providing new component implementations {\bf must}
use the (small) PETSc common
part, but need not use the PETSc common part.

We will use {\tt <component>\_<implementation>} to denote the actual source code and 
data structures used for a particular implementation of an object that has the 
{\tt <component>} interface.

\section{Organization of the Source Code}

Each component has
\begin{itemize}
\item Its own, application public, include file {\tt \$\{PETSC\_DIR\}/include/<component>.h} 
\item Its own directory, {\tt \$\{PETSC\_DIR\}/src/<component>}
\item A data structure defined in  the file
      {\tt \$\{PETSC\_DIR\}/src/<component>/<component>impl.h}.
      This data structure is shared by all the different PETSc implementations of the 
      component. For example, for matrices it is shared by dense,
      sparse, parallel, and sequential formats.
\item An abstract interface that defines the application callable 
      functions for the component. These are defined in the directory
      {\tt \$\{PETSC\_DIR\}/src/<component>/interface}.
\item One or more actual implementations of the components (for example,
      sparse uniprocessor and parallel matrices implemented with the AIJ storage format).
      These are each in a subdirectory of 
      \break {\tt \$\{PETSC\_DIR\}/src/<component>/impls}. Except in rare
      circumstances data 
      structures defined here should not be referenced from outside this 
      directory.
\end{itemize}

Each type of object, for instance a vector, is defined in its own
include file, by 
\begin{verbatim}
   typedef _p_<component>* <component>; (for example, typedef _p_Vec* Vec;).
\end{verbatim}
  This organization
allows the compiler to perform type checking on all subroutine calls
while at the same time
completely removing the details of the implementation of {\tt
\_p\_<component>} from the application code.  This capability is extremely important
because it allows the library internals to be changed
without altering or recompiling the application code.

Polymorphism is supported through the directory 
{\tt \$\{PETSC\_DIR\}/src/<component>/interface},
which contains the code that implements the abstract interface to the
operations on the object.  Essentially, these routines do some error
checking of arguments and logging of profiling information 
and then call the function appropriate for the
particular implementation of the object. The name of the abstract
function is {\tt <component>Operation}, for instance, {\tt MatMult} or {\tt PCCreate}, while
the name of a particular implementation is 
{\tt <component>Operation\_<implementation>}, for instance, 
{\tt MatMult\_SeqAIJ} or {\tt PCCreate\_ILU}. These naming
conventions are used to simplify code maintenance.

\section{Common Object Header}

All PETSc/PETSc objects have the following common header structures.

\begin{verbatim}

/* Function table common to all PETSc compatible components */

typedef struct {
   int (*getcomm)(PetscObject,MPI_Comm *);
   int (*view)(PetscObject,Viewer);
   int (*destroy)(PetscObject);
   int (*query)(PetscObject,char *,PetscObject *);
   int (*compose)(PetscObject,char*,PetscObject);
   int (*reference)(PetscObject);
   int (*composefunction)(PetscObject,char *,char *,void *);
   int (*queryfunction)(PetscObject,char *, void **);
   int (*composelanguage(PetscObject,PetscLanguage,void *);
   int (*querylanguage(PetscObject,PetscLanguage,void **);
} PetscOps;

/* Data structure header common to all PETSc compatible components */

struct _p_<component> {
  int              cookie;                                  
  PetscOps         *bops;                                   
  <component>Ops   *ops;       
  OTHER STUFF SPECIFIC TO THAT PARTICULAR LIBRARY
};

/* Data structure header common to all PETSc components; but not all PETSc components */

struct _p_<component> {
  int              cookie;                                  
  PetscOps         *bops;                                   
  <component>Ops   *ops;                                    
  MPI_Comm         comm;                                    
  int              type;                                    
  PLogDouble       flops,time,mem;                          
  int              id;                                      
  int              refct;                                   
  int              tag;                                     
  DLList           qlist;                                   
  OList            olist;                                   
  char             *type_name;                              
  PetscObject      parent;                                  
  char             *name;                                    
  char             *prefix;                                 
  void             *cpp;
  void             **fortran_func_pointers;       
  COMPONENT-SPECIFIC DATASTRUCTURES
}; 

\end{verbatim}
Here {\tt <component>ops} is a function table (like the {\tt PetscOps} above) that 
contains the function pointers for the operations specific to that component.
For example, the PETSc vector component object looks like

\begin{verbatim}

/* Function table common to all PETSc compatible vector objects */

typedef struct _VecOps* VecOps;
struct _VecOps {
  int  (*duplicate)(Vec,Vec*),           /* get single vector */
       (*duplicatevecs)(Vec,int,Vec**),  /* get array of vectors */
       (*destroyvecs)(Vec*,int),         /* free array of vectors */
       (*dot)(Vec,Vec,Scalar*),          /* z = x^H * y */
       (*mdot)(int,Vec,Vec*,Scalar*),    /* z[j] = x dot y[j] */
       (*norm)(Vec,NormType,double*),    /* z = sqrt(x^H * x) */
       (*tdot)(Vec,Vec,Scalar*),         /* x'*y */
       (*mtdot)(int,Vec,Vec*,Scalar*),   /* z[j] = x dot y[j] */
       (*scale)(Scalar*,Vec),            /* x = alpha * x   */
       (*copy)(Vec,Vec),                 /* y = x */
       (*set)(Scalar*,Vec),              /* y = alpha  */
       (*swap)(Vec,Vec),                 /* exchange x and y */
       (*axpy)(Scalar*,Vec,Vec),         /* y = y + alpha * x */
       (*axpby)(Scalar*,Scalar*,Vec,Vec),/* y = y + alpha * x + beta * y*/
       (*maxpy)(int,Scalar*,Vec,Vec*),   /* y = y + alpha[j] x[j] */
       (*aypx)(Scalar*,Vec,Vec),         /* y = x + alpha * y */
       (*waxpy)(Scalar*,Vec,Vec,Vec),    /* w = y + alpha * x */
       (*pointwisemult)(Vec,Vec,Vec),    /* w = x .* y */
       (*pointwisedivide)(Vec,Vec,Vec),  /* w = x ./ y */
       (*setvalues)(Vec,int,int*,Scalar*,InsertMode),
       (*assemblybegin)(Vec),            /* start global assembly */
       (*assemblyend)(Vec),              /* end global assembly */
       (*getarray)(Vec,Scalar**),        /* get data array */
       (*getsize)(Vec,int*),(*getlocalsize)(Vec,int*),
       (*getownershiprange)(Vec,int*,int*),
       (*restorearray)(Vec,Scalar**),    /* restore data array */
       (*max)(Vec,int*,double*),         /* z = max(x); idx=index of max(x) */
       (*min)(Vec,int*,double*),         /* z = min(x); idx=index of min(x) */
       (*setrandom)(PetscRandom,Vec),    /* set y[j] = random numbers */
       (*setoption)(Vec,VecOption),
       (*setvaluesblocked)(Vec,int,int*,Scalar*,InsertMode),
       (*destroy)(Vec),
       (*view)(Vec,Viewer);
};

/* Data structure header common to all PETSc vector components */

struct _p_Vec {
  int                    cookie;                                  
  MPI_Comm               comm;                                    
  PetscOps               *bops;                                   
  VecOps                 *ops;                                    
  MPI_Comm               comm;
  int                    type;                                    
  PLogDouble             flops,time,mem;                          
  int                    id;                                      
  int                    refct;                                   
  int                    tag;                                     
  DLList                 qlist;                                   
  OList                  olist;                                   
  char                   *type_name;                              
  PetscObject            parent;                                  
  char*                  name;                                    
  char                   *prefix;                                 
  void**                 fortran_func_pointers;       
  void                   *data;     /* implementation-specific data */
  int                    N, n;      /* global, local vector size */
  int                    bs;
  ISLocalToGlobalMapping mapping;   /* mapping used in VecSetValuesLocal() */
  ISLocalToGlobalMapping bmapping;  /* mapping used in VecSetValuesBlockedLocal() */
};
\end{verbatim}

Several routines are provided for manipulating data within the header,
including
\begin{verbatim}
   int PetscObjectGetComm(PetscObject object,MPI_Comm *comm) 
\end{verbatim}
which returns in {\tt comm}  the MPI communicator associated with the
specified object.

\section{Common Object Functions}

We now discuss the specific functions in the PETSc common function table.

\begin{itemize}
\item {\tt getcomm(PetscObject,MPI\_Comm *)} obtains the MPI communicator associated
      with this object.

\item {\tt view(PetscObject,Viewer)} allows one to store or visualize the data inside
      an object. If the {\tt Viewer} is null than should cause the object to print 
      information on the object to standard out. PETSc provides a variety of simple
      viewers.

\item {\tt destroy(PetscObject)} causes the reference count of the object to be decreased
      by one or the object to be destroyed and all memory used by the object to be freed.
      If the object has any other objects composed with it then they are each sent a
      {\tt destroy()}, i.e. the {\tt destroy()} function is called on them also.

\item {\tt reference(PetscObject)} increases the reference count on the object by one.

\item {\tt compose(PetscObject,char *name, PetscObject)} associates the second object with 
      the first object and increases the reference count of the second object. If an
      object with the 
      same name was previously composed that object is dereferenced and replaced with 
      the new object. If the 
      second object is null and and object with the same name has already been 
      composed that object is dereferenced (the {\tt destroy()} function is called on 
      it, and that object is removed from the first object); i.e. this is a way to 
      remove, by name, an object that was previously composed. 

\item {\tt query(PetscObject,char *name, PetscObject *)} retrieves an object that was 
      previously composed with the first object. Retreives a null if no object with 
      that name was previously composed.

\item {\tt composefunction(PetscObject,char *name,char *fname,void *func)} associates a function
      pointer to an object. If the object already had a composed function with the 
      same name, the old one is replace. If the fname is null it is removed from 
      the object. The string {\tt fname} is the  character string name of the function;
      it may include the path name or URL of the dynamic library where the function is located.
      The argument {\tt name} is a ``short'' name of the function to be used with the 
      {\tt queryfunction()} call. On systems that support dynamic libraries the {\tt func}
      argument is ignored; otherwise {\tt func} is the actual function pointer.

      For example, {\tt fname} may be {\tt libpetscksp:PCCreate\_LU} or 
      {\tt http://www.mcs.anl.gov/petsc/libpetscksp:PCCreate\_LU}.

\item {\tt queryfunction(PetscObject,char *name,void **func)} retreives a function pointer that
      was associated with the object. If dynamic libraries are used the function is loaded
      into memory at this time (if it has not been previously loaded), not when the
      {\tt composefunction()} routine was called.

\item {\tt composelanguage(PetscObject obj,PetscLanguage lang,void *interface)} sets an
      alternative language interface for an object. Currently languages are {\tt PETSC\_C}
      {\tt PETSC\_CPP}, though the C++ version is not yet supported.

\item {\tt querylanguage(PetscObject obj,PetscLanguage lang,void **interface)} requests
      an interface to an objects data from an alternative language.

\end{itemize}

Since the object composition allows one to {\bf only} compose PETSc objects
with PETSc objects rather than any arbitrary pointer, PETsc provides
the convenience object {\tt PetscObjectContainer}, created with the
routine {\tt PetscObjectContainerCreate(MPI\_Comm,PetscObjectContainer)}
to allow one to wrap any kind of data into a PETSc object that can then be
composed with a PETSc object.

% --------------------------------------------------------------------------------------
\section{PETSc Implementation of the  Object Functions}

This sections discusses how PETSc implements the {\tt compose()}, {\tt query()}, {\tt
composefunction()}, and {\tt queryfunction()} functions for its object implementations. 
Other PETSc compatible component implementations are free to manage these functions in any 
manner; but generally they would use the PETSc defaults so that the library writer does
not have to ``reinvent the wheel.''

\subsection{Compose and Query}

In {\tt src/sys/src/objects/olist.c} PETSc defines a C struct
\begin{verbatim}

typedef struct _OList *OList;
struct _OList {
    char        name[128];
    PetscObject obj;
    OList       next;
};
\end{verbatim}
from which linked lists of composed objects may be constructed. The routines
to manipulate these elementary objects are
\begin{verbatim}
  int OListAdd(OList *fl,char *name,PetscObject obj );
  int OListDestroy(OList *fl );
  int OListFind(OList fl, char *name, PetscObject *obj)
  int OListDuplicate(OList fl, OList *nl);
\end{verbatim}
The function {\tt OListAdd()} will create the initial {\tt OList} if the argument 
{\tt fl} points to a null.

The PETSc object {\tt compose()} and {\tt query()} functions are then simply
(defined in {\tt src/sys/src/objects/inherit.c})
\begin{verbatim}
  int PetscObjectCompose_Petsc(PetscObject obj,char *name,PetscObject ptr)
  {
    int ierr;

    PetscFunctionBegin;
    ierr = OListAdd(&obj->olist,name,ptr); CHKERRQ(ierr);
    PetscFunctionReturn(0);
  }

  int PetscObjectQuery_Petsc(PetscObject obj,char *name,PetscObject *ptr)
  {
    int ierr;

    PetscFunctionBegin;
    ierr = OListFind(obj->olist,name,ptr); CHKERRQ(ierr);
    PetscFunctionReturn(0); 
  }
\end{verbatim}

\subsection{Compose and Query Function}

PETSc allows one to compose functions by string name (to be loaded later from 
a dynamic library) or by function pointer. In {\tt src/sys/src/dll/reg.c}
PETSc defines the C structure

\begin{verbatim}

typedef struct _FList* FList;
struct _FList {
  int    (*routine)(void *);
  char   *path;
  char   *name;               
  char   *rname;            /* name of create function in link library */
  FList  *next;
};
\end{verbatim}

The {\tt FList} object is a linked list of function data; each 
of which contains
\begin{itemize}
\item a function pointer (if it has already been loaded into memory from the dynamic library)
\item the ``path'' (directory and library name) where the function exists (if it is 
      loaded from a dynamic library)
\item the ``short'' name of the function,
\item the actual name of the function as a string (for dynamic libraries this string is used
      to load in the actual function pointer).
\end{itemize}

Each PETSc object contains a {\tt FList} object. The {\tt composefunction()} and 
{\tt queryfunction()} are given by 

\begin{verbatim}
int PetscObjectComposeFunction_Petsc(PetscObject obj,char *name,char *fname,void *ptr)
{
  int ierr;

  PetscFunctionBegin;
  ierr = FListAdd(&obj->qlist,name,fname,(int (*)(void *))ptr);CHKERRQ(ierr);
  PetscFunctionReturn(0);
}

int PetscObjectQueryFunction_Petsc(PetscObject obj,char *name,void **ptr)
{
  int ierr;

  PetscFunctionBegin;
  ierr = FListFind(obj->comm,obj->qlist,name,( int(**)(void *)) ptr);CHKERRQ(ierr);
  PetscFunctionReturn(0);
}
\end{verbatim}

  Because we need to support function composition on systems both {\bf with} and {\bf without} 
dynamic link libraries the actual source code is a little messy. The idea is that
on systems with dynamic libraries all PETSc ``register'' and ``composefunction''
function calls that take the actual 
function pointer argument must eliminate this argument in the preprocessor step before 
the code is compiled. Otherwise, since the compiler sees the function pointer, it will 
compile it in and link in all those functions; thus one could not take advantage of the
dynamic libraries. This is done with macros like the following
\begin{verbatim}
#if defined(USE_DYNAMIC_LIBRARIES)
#define       FListAdd(a,b,p,c) FListAdd_Private(a,b,p,0)
#else
#define       FListAdd(a,b,p,c) FListAdd_Private(a,b,p,(int (*)(void *))c)
#endif
\end{verbatim}
Thus when the code is compiled with the dynamic link library flag the function pointer 
argument is removed from the code; otherwise it is retained. Ugly, but neccessary.

The {\tt FListAdd\_Private()} and all related routines can be found in the directory
{\tt src/sys/src/dll}.

In addition to using the {\tt FList} mechanism to compose functions into PETSc objects, it is
also used to allow registration of new component implementations; for example, new
preconditioners, see Section \ref{sec:registeringnewmethods}. 

\subsection{Compose and Query Language}

PETSc implements the compose and query language feature using the routines
\begin{verbatim}
int PetscObjectComposeLanguage_Petsc(PetscObject obj,PetscLanguage lang,void *vob)
{
  PetscFunctionBegin;
  if (lang == PETSC_LANGUAGE_CPP) {
    obj->cpp = vob;
  } else {
    SETERRQ(1,1,"No support for this language yet");
  }
  PetscFunctionReturn(0);
}
\end{verbatim}
and
\begin{verbatim}
int PetscObjectQueryLanguage_Petsc(PetscObject obj,PetscLanguage lang,void **vob)
{
  PetscFunctionBegin;
  if (lang == PETSC_LANGUAGE_C) {
    *vob = (void *) obj;
  } else if (lang == PETSC_LANGUAGE_CPP) {
    if (obj->cpp) {
      *vob = obj->cpp;
    } else {
      SETERRQ(1,1,"No C++ wrapper generated");
    }
  } else {
    SETERRQ(1,1,"No support for this language yet");
  }
  PetscFunctionReturn(0);
}
\end{verbatim}
As one can see the implementation is very basic. Each PETSc object has a
``place-holder'' for the C++ interface object which may be set or accessed.

Currently PETSc does not provide C++ wrapper objects that could be
composed into our C objects, but we hope to provide them someday.

\chapter{Mimimal Component Standards}
This chapter discusses the miminal functionality and format required of any 
component that is compatible with PETSc. 

% -----------------------------------------------------------------------------------
\chapter{PetscObjects}

\section{Elementary Objects: IS, Vec, Mat}

\section{Solver Objects: PC, KSP, SNES, TS}

The PETSc solver objects have not yet been completely cast into the common component 
header 
\begin{verbatim}
struct _p_<component> {
  int              cookie;                                  
  PetscOps         *bops;                                   
  <component>Ops   *ops;       
  OTHER STUFF SPECIFIC TO THAT PARTICULAR LIBRARY
};
\end{verbatim}
model; rather the {\tt <component>Ops} field is empty and the member functions
are scattered in the {\tt OTHER STUFF SPECIFIC TO THAT PARTICULAR LIBRARY}
fields. We hope to rectify this in the next release. Regardless, this does not
effect (except superficially) the development of the objects.

\subsection{Preconditioners: PC}
The base PETSc PC object is defined in the {\tt src/ksp/pc/pcimpl.h} include file. 
A carefully commented implementation of a PC object can be found in 
{\tt src/ksp/pc/impls/jacobi/jacobi.c}. 

\subsection{Krylov Solvers: KSP}
The base PETSc KSP object is defined in the {\tt src/ksp/ksp/kspimpl.h} include file. 
A carefully commented implementation of a KSP object can be found in 
{\tt src/ksp/ksp/impls/cg/cg.c}. 

\subsection{Registering New Methods}
\label{sec:registeringnewmethods}

See {\tt src/ksp/examples/tutorials/ex12.c} for an example of registering a new
preconditioning (PC) method.


% -----------------------------------------------------------------------------------
\chapter{Style Guide}

The PETSc team uses certain conventions to make our source code consistent. Groups
developing components compatible with PETSc are, of course, free to organize their
own source code anyway they like.

\section{Names}
Consistency of names for variables, functions, etc. is extremely 
important in making the package both usable and maintainable.
We use several conventions:
\begin{itemize}
\item All function names and enum types consist of words, each of 
      which is capitalized, for example {\tt KSPSolve()} and 
      {\tt MatGetOrdering()}.
\item All enum elements and macro variables are capitalized. When
      they consist of several complete words, there is an underscore between each word.
\item Functions that are private to PETSc (not callable by the 
      application code) either
      \begin{itemize}
        \item have an appended {\tt \_Private} (for example, 
           {\tt StashValues\_Private}) or
        \item have an appended {\tt \_<component>Subtype} (for example,
           {\tt MatMult\_SeqAIJ}).
      \end{itemize}

      In addition, functions that are not intended for use outside
      of a particular file are declared static.
\item Function names in structures are the same as the base application
      function name without the object prefix, and all are in small letters. 
      For example, {\tt MatMultTrans()} has a structure name of 
      {\tt multtrans()}.
\item Each application usable function begins with the name of the component object, 
      for example, {\tt ISInvertPermutation} or {\tt MatMult}. 
\end{itemize}

\section{Coding Conventions and Style Guide}

Within the PETSc source code, we adhere to the following guidelines
so that the code is uniform and easily maintainable:

\begin{itemize}
\item All PETSc function bodies are indented two characters.
\item Each additional level of loops, if statements, etc. is indented
      two more characters.
\item Wrapping lines should be avoided whenever possible.
\item Source code lines should not be more than 120 characters wide.
\item The macros {\tt SETERRQ()} and {\tt CHKERRQ()} should be on the 
      same line as the routine to be checked unless this violates the 
      120 character width rule. Try to make error messages short, but 
      informative.
\item The local variable declarations should be aligned. For example,
      use the style
\begin{verbatim}
   int    i,j;
   Scalar a;
\end{verbatim}
instead of
\begin{verbatim}
   int i,j;
   Scalar a;
\end{verbatim}
\item All local variables of a particular type (e.g., {\tt int}) should be 
      listed on the same line if possible; otherwise, they should be listed
      on adjacent lines.
\item Equal signs should be aligned in regions where possible.
\item There {\em must} be a single blank line
      between the local variable declarations and the body of the function.
\item The first line of the executable statments must be {\tt PetscFunctionBegin;}
\item The following text should be before each function
\begin{verbatim}
#undef __FUNC__
#define __FUNC__ ``FunctionName''
\end{verbatim}
this is used by various macros (for example the error handlers) to always know
what function one is in.
\item Use {\tt PetscFunctionReturn(returnvalue);} not {\tt return(returnvalue);}
\item {\em Never} put a function call in a return statment; do not do
\begin{verbatim}
   PetscFunctionReturn( somefunction(...) );
\end{verbatim}
\item Do {\bf not} put a blank line immediately after {\tt PetscFunctionBegin;} or 
a blank line immediately before {\tt PetscFunctionReturn(0);}.
\item Indentation for {\tt if} statements {\em must}  be done  as
as
\begin{verbatim}
   if (  ) {
     ....
   } else {
     ....
   }
\end{verbatim}
\item {\em Never}  have 
\begin{verbatim}
   if (  ) 
     a single indented line
\end{verbatim}
or
\begin{verbatim}
   for (  )
     a single indented line
\end{verbatim}
instead use either 
\begin{verbatim}
   if (  ) a single line
\end{verbatim}
or 
\begin{verbatim}
   if (  ) {
     a single indented line
   }
\end{verbatim}
\item {\em No} tabs are allowed in {\em any} of the source code.
\item The open bracket \{ should be on the same line as the {\em if ()} test, {\em for ()}, etc. never on 
      its own line. The closing bracket \} should {\bf always} be on its own line. 
\item In function declaration the open bracket \{ should be on the {\bf next} line, not on the same line as the function name and
      arguments. This is an exception to the rule above.
\item {\em No} space after a ( or before a ). No space before the CHKXXX(). That is, do not write
\begin{verbatim}
   ierr = PetscMalloc( 10*sizeof(int),&a ); CHKERRQ(ierr);
\end{verbatim}
instead write
\begin{verbatim}
   ierr = PetscMalloc(10*sizeof(int),&a);CHKERRQ(ierr);
\end{verbatim}
\item {\em No} space after the ) in a cast, no space between the type and the * in a caste.
\item {\em No} space before or after a , in lists
That is, do not write
\begin{verbatim}
    int a, b,c;
    ierr = func(a, 22.0);CHKERRQ(ierr);
\end{verbatim}
instead write
\begin{verbatim}
    int a,b,c;
    ierr = func(a,22.0);CHKERRQ(ierr);
\end{verbatim}
\item Do not use the {\em register} directive.
\item Do not use {\em if (rank == 0)} or {\em if (v == PETSC\_NULL)} or {\em if (flg == PETSC\_TRUE)} or {\em if (flg == PETSC\_FALSE)} 
instead use {\em if (!rank)} or {\em if (!v)} or {\em if (flg)} or {\em if (!flg)}.
\item Do not use {\em \#ifdef} or {\em \#ifndef} rather use {\em \#if defined(...} or {\em \#if !defined(...}
\end{itemize}

\section{Option Names}

Since consistency simplifies usage and code maintenance, the names of
PETSc routines, flags, options, etc. have been selected with great care.
The default option names are of the form {\tt -<component>\_sub<component>\_name}.  
For example, the option name for the basic convergence tolerance for 
the KSP package is {\tt -ksp\_atol}. In addition, operations in different 
packages of a similar nature have a similar name.  For example, the option
name for the basic convergence tolerance for the SNES package is 
{\tt -snes\_atol}.

% -----------------------------------------------------------------------------------
\chapter{The Various Matrix Classes}
\label{sec:matclasses}

PETSc provides a variety of matrix implementations, since no single
matrix format is appropriate for all problems.  This section first
discusses various matrix blocking strategies, and then 
describes the assortment of matrix types within PETSc.

\subsection{Matrix Blocking Strategies}
\sindex{matrix blocking} 
\sindex{blocking} 

In today's computers, the time to perform an arithmetic operation is
dominated by the time to move the data into position, not the time to
compute the arithmetic result.  For example, the time to perform a
multiplication operation may be one clock cycle, while the time to
move the floating point number from memory to the arithmetic unit may
take 10 or more cycles. To help manage this difference in time scales,
most processors have at least three levels of memory: registers,
cache, and random access memory, RAM. (In addition, some processors
have external caches, and the complications of paging introduce
another level to the hierarchy.)

Thus, to achieve high performance, a code should first move data into
cache, and from there move it into registers and use it repeatedly
while it remains in the cache or registers before returning it to main
memory. If one reuses a floating point number 50 times while it is in
registers, then the ``hit'' of 10 clock cycles to bring it into the
register is not important. But if the floating point number is used
only once, the ``hit'' of 10 clock cycles becomes very noticeable,
resulting in disappointing flop rates.

Unfortunately, the compiler controls the use of the registers, and the
hardware controls the use of the cache. Since the user has essentially
no direct control, code must be written in such a way that the
compiler and hardware cache system can perform well. Good quality code
is then be said to respect the memory hierarchy.

The standard approach to improving the hardware utilization is to use
blocking. That is, rather than working with individual elements in
the matrices, one employs blocks of elements.  Since the use of
implicit methods in PDE-based simulations leads to matrices with a
naturally blocked structure (with a block size equal to the number of
degrees of freedom per cell), blocking is extremely advantageous.  The
PETSc (and BlockSolve95) sparse matrix representations use a variety
of techniques for blocking, including

\begin{itemize}
\item storing the matrices using a generic sparse matrix format, but 
   storing additional information about adjacent rows with identical 
   nonzero structure (so called I-nodes); this I-node information is 
   used in the key computational routines to improve performance
   (the default for the {\tt MATSEQAIJ} and {\tt MATMPIAIJ} formats);
\item storing the matrices using a fixed (problem dependent) block size
   (via the {\tt MATSEQBAIJ} and {\tt MATMPIBAIJ} formats); and
\item storing the matrices using a variable block size, that can be 
   different for different parts of the matrix
   (supported by the BlockSolve95 matrix format {\tt MATMPIROWBS}).
\end{itemize}

The advantage of the first approach is that it is a minimal change
from a standard sparse matrix format and brings a large percent of the
improvement one obtains via blocking.  Using a fixed block size gives
the best performance, since the code can be hardwired with that
particular size (for example, in some problems the size may be 3, in
others 5, etc.), so that the compiler will then optimize for that
size, removing the overhead of small loops entirely. Variable block
size is, of course, appropriate for problems where the natural matrix
block size is different in different parts of the domain. It is
slightly less efficient than the fixed block size code due to overhead
of checking block sizes.

The following table presents the floating point performance
for a basic matrix-vector product using these four approaches: a basic
compressed row storage format (using the PETSc runtime options 
{\tt -mat\_seqaij -mat\_no\_unroll)}; the same compressed row format using
I-nodes (with the option {-mat\_seqaij}); a fixed block size code,
with a block size of three for these problems (using the option 
{-mat\_seqbaij}); and the BlockSolve95 variable block size code (using
PETSc option {-mat\_mpirowbs}). The rates were computed on one
node of an older IBM SP, using two test matrices.  The first matrix
(ARCO1), courtesy of Rick Dean of Arco, arises in multiphase flow
simulation; it has 1501 degrees of freedom, 26,131 matrix nonzeros
and, a natural block size of 3, and a small number of well terms. The
second matrix (CFD), arises in a three-dimensional Euler flow
simulation and has 15,360 degrees of freedom, 496,000 nonzeros, and a
natural block size of 5. In addition to displaying the flop rates for
matrix-vector products, we also display them for triangular solve
obtained from an ILU(0) factorization.

\medskip
\centerline{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Problem & Block size & Basic & I-node version & Fixed block size &Variable block size \\
\hline
\multicolumn{6}{c}{{\em Matrix-Vector Product (Mflop/sec)}} \\
\hline
Multiphase & 3 & 27 & 43 & 70 & 22 \\
Euler & 5 &  28 & 58 & 90 & 39 \\
\hline
\multicolumn{6}{c}{{\em Triangular Solves from ILU(0) (Mflop/sec)}}\\
\hline
Multiphase & 3 & 22 & 31 & 49 & 15 \\
Euler & 5 & 22 & 39 & 65 & 24\\
\hline
\end{tabular}
}
\medskip

These examples demonstrate that careful implementations of the basic
sequential kernels in PETSc can dramatically improve overall floating
point performance, and users can immediately benefit from such
enhancements without altering a single line of their application
codes.  Note that the speeds of the I-node and fixed block operations
are several times that of the basic sparse implementations.  The
disappointing rates for the variable block size code occur because
even on a sequential computer, the code performs the matrix-vector
products and triangular solves using the coloring introduced above and
thus does not utilize the cache particularly efficiently.  This is an
example of improving the parallelization capability at the expense of
using each processor less efficiently.

\subsection{Sequential AIJ Sparse Matrices}

The default matrix representation within PETSc is the general sparse
AIJ format (also called the Yale sparse matrix format or compressed
sparse row format, CSR).

\subsection{Parallel AIJ Sparse Matrices}

This matrix type, which is the
default parallel matrix format; additional implementation details are
given in \cite{efficient}.

\subsection{Sequential Block AIJ Sparse Matrices}

The sequential and parallel block AIJ formats, which are extensions of
the AIJ formats described above, are intended especially for use with
multicomponent PDEs.  The block variants store matrix elements by
fixed-sized dense {\tt nb} $\times$ {\tt nb} blocks.  The stored row
and column indices begin at zero.

The routine for creating a sequential block AIJ matrix with {\tt m} 
rows, {\tt n} columns, and a block size of {\tt nb} is
\begin{verbatim}
   ierr = MatCreateSeqBAIJ(MPI_Comm comm,int nb,int m,int n,int nz,int *nnz, Mat *A)
\end{verbatim}
\findex{MatCreateSeqBAIJ} 
The arguments {\tt nz} and {\tt nnz} can be used to preallocate matrix
memory by indicating the number of {\em block} nonzeros per row.  For good
performance during matrix assembly, preallocation is crucial; however, the
user can set {\tt nz=0} and {\tt nzz=PETSC\_NULL} for PETSc to dynamically
allocate matrix memory as needed.  The PETSc users manual 
discusses preallocation for the AIJ format; extension to the block AIJ
format is straightforward.

Note that the routine {\tt MatSetValuesBlocked()}
\findex{MatSetValuesBlocked()} can be used for more efficient matrix assembly
when using the block AIJ format.
 
\subsection{Parallel Block AIJ Sparse Matrices}

Parallel block AIJ matrices with block size {\t nb} can be created with
the command \findex{MatCreateMPIBAIJ()}
\begin{verbatim}
   ierr = MatCreateMPIBAIJ(MPI_Comm comm,int nb,int m,int n,int M,int N,int d_nz,
                          int *d_nnz, int o_nz,int *o_nnz,Mat *A);
\end{verbatim}
{\tt A} is the newly created matrix, while the arguments {\tt m}, {\tt n}, 
{\tt M}, and {\tt N}, indicate the number of local rows and columns and
the number of global rows and columns, respectively. Either the local or
global parameters can be replaced with {\tt PETSC\_DECIDE}, so that 
PETSc will determine \findex{PETSC_DECIDE} them.
The matrix is stored with a fixed number of rows on 
each processor, given by {\tt m}, or determined by PETSc if {\tt m} is
{\tt PETSC\_DECIDE}.

If {\tt PETSC\_DECIDE} is not used for
{\tt m} and {\tt n} then the user must ensure that they are chosen to be
compatible with the vectors. To do this, one first considers the product 
$y = A x$. The {\tt m} that one uses in {\tt MatCreateMPIBAIJ()}
must match the local size used in the {\tt VecCreateMPI()} for {\tt y}.
The {\tt n} used must match that used as the local size in 
{\tt VecCreateMPI()} for {\tt x}. 

The user must set {\tt d\_nz=0}, {\tt o\_nz=0}, {\tt d\_nnz=PETSC\_NULL}, and 
{\tt o\_nnz=PETSC\_NULL} for PETSc to control dynamic allocation of matrix
memory space.  Analogous to {\tt nz} and {\tt nnz} for the routine 
{\tt MatCreateSeqBAIJ()}, these arguments optionally specify 
block nonzero information for the diagonal ({\tt d\_nz} and {\tt d\_nnz}) and 
off-diagonal ({\tt o\_nz} and {\tt o\_nnz}) parts of the matrix. 
For a square global matrix, we define each processor's diagonal portion 
to be its local rows and the corresponding columns (a square submatrix);  
each processor's off-diagonal portion encompasses the remainder of the
local matrix (a rectangular submatrix).  
The PETSc users manual gives an example of preallocation for
the parallel AIJ matrix format; extension to the block parallel AIJ case
is straightforward.

\subsection{Sequential Dense Matrices}

PETSc provides both sequential and parallel dense matrix formats,
where each processor stores its entries in a column-major array in the
usual Fortran77 style. 

\subsection{Parallel Dense Matrices}

The parallel dense matrices are partitioned by rows across the
processors, so that each local rectangular submatrix is stored in the
dense format described above.

\subsection{Parallel BlockSolve Sparse Matrices}

PETSc provides a parallel, sparse, row-based matrix format that is
intended for use in conjunction with the ILU and ICC preconditioners
in BlockSolve95.  

\subsection{Block Diagonal Sparse Matrices}
\label{sec:bdiag}

Storage \sindex{block diagonal matrix storage} by block diagonals is
available in both uniprocessor and parallel versions, although currently
only a subset of matrix operations is supported.  Each element of
a block diagonal is defined to be a square dense block of size {\tt
nb} $\times$ {\tt nb}, where conventional diagonal storage results for
{\tt nb}=1.  Such storage is particularly useful for multicomponent PDEs
discretized on regular grids.

The routine for creating a uniprocessor block diagonal matrix with {\tt m} 
rows, {\tt n} columns, and a block size of {\tt nb} is
\begin{verbatim}
   ierr = MatCreateSeqBDiag(PETSC_COMM_SELF,int m,int n,int nd,int nb,int *diag,
                                   Scalar **diagv,Mat *A);
\end{verbatim}
The \findex{MatCreateSeqBDiag} argument {\tt nd} is the number of 
block diagonals, and {\tt diag} is
an array of block diagonal numbers.  For the matrix element $A_{ij}$,
where $i$ and $j$ respectively denote the row and column number of the 
element, the block diagonal number is computed using integer division by
\[ {\tt diag} = i/nb - j/nb. \]
If matrix storage space is allocated by the user, the argument {\tt diagv} 
is a pointer to the actual diagonals (in the same order as the {\tt diag} 
array).  For PETSc to control memory allocation, the user should merely
set {\tt diagv=PETSC\_NULL}.

A simple example of this storage format is illustrated below for block
size {\tt nb}=1. 
Here {\tt nd} = 4 and {\tt diag} = [2, 1, 0, -3]. The
diagonals need not be listed in any particular order, so that
{\tt diag} = [-3, 0, 1, 2] or {\tt diag} = [0, 2, -3, 1] would also
be valid values for the {\tt diag} array. 

\begin{center}
\begin{tabular}{| c c c c c c |}
\hline
a00  &0    &0    &a03  &0    &0\\
a10  &a11  &0    &0    &a14  &0\\
a20  &a21  &a22  &0    &0    &a25\\
0    &a31  &a32  &a33  &0    &0\\
0    &0    &a42  &a43  &a44  &0\\
0    &0    &0    &a53  &a54  &a55\\
\hline
\end{tabular}
\end{center}

\subsection{Parallel Block Diagonal Sparse Matrices}

The parallel block diagonal matrices are partitioned by rows across
the processors, so that each local rectangular submatrix is stored by
block diagonals as described above.  The routine for creating a
parallel block diagonal matrix with {\tt m} local rows, {\tt M} global
rows, {\tt n} global columns, and a block size of {\tt nb} is
\begin{verbatim}
   ierr = MatCreateMPIBDiag(PETSC_COMM_SELF,int m,int M,int N,int nd,int nb,int *diag,
                            Scalar **diagv,Mat *A);
\end{verbatim}
Either the {\tt m} or {\tt M} can be set to {\tt PETSC\_DECIDE} for PETSc
to determine the corresponding quantity.


\bibliographystyle{plain}
\bibliography{../petsc}

\end{document}
