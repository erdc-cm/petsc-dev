% $Id: part2.tex,v 1.368 2001/08/27 17:38:09 bsmith Exp $ 
%
% NOTES:  
%  - Be sure to place captions BEFORE labels in figures and tables!
%    Otherwise, numbering will be incorrect.  For example, use the following:
%       \caption{PETSc Vector Operations}
%       \label{fig:vectorops}
%  - Use \break to indicate a line break (needed to prevent long strings in
%    \tt mode from running of the page)
%

\chapter{Vectors and Distributing Parallel Data} 
\label{chapter:vectors}
\sindex{vectors}

The vector (denoted by \trl{Vec}) is one of the simplest PETSc
objects.  Vectors are used to store discrete PDE solutions, right-hand
sides for linear systems, etc. This chapter is organized as follows:

\begin{itemize}
\item (Vec) Sections~\ref{sec:veccreate} and \ref{sec:vecbasic} - basic usage of vectors
\item Section \ref{sec:indexingandordering} - management of the various numberings of
               degrees of freedom, vertices, cells, etc.
  \begin{itemize} 
  \item (AO) Mapping between different global numberings
  \item (ISLocalToGlobalMapping) Mapping between local and global numberings
  \end{itemize}
\item (DA) Section \ref{sec:struct} - management of structured grids
\item (IS, VecScatter) Section \ref{sec:unstruct} - management of vectors related to unstructured grids
\end{itemize}

% --------------------------------------------------------------------------------------
\section{Creating and Assembling Vectors}
\label{sec:veccreate}

PETSc currently provides two basic vector types: sequential and parallel
(MPI based). To create a sequential vector with \trl{m} components,
one can
use the command \findex{VecCreateSeq()} \findex{Vec}
\begin{verse}
  VecCreateSeq(PETSC\_COMM\_SELF,int m,Vec *x);
\end{verse}
To create a parallel vector one can either specify the number of 
components that will be stored on each processor or let PETSc decide. 
The command \findex{VecCreateMPI()}
\begin{verse}
  VecCreateMPI(MPI\_Comm comm,int m,int M,Vec *x);
\end{verse}
creates a vector that is distributed over all processors in the communicator,
\trl{comm}, where \trl{m} indicates the number 
of components to store on the local processor, and \trl{M} is the 
total number of vector components.  Either the local or global 
dimension, but not both, can be set to \trl{PETSC_DECIDE} to 
\findex{PETSC_DECIDE} indicate that PETSc should determine it.
More generally, one can use the routines \findex{VecCreate()} \findex{VecSetFromOptions()}
\begin{verse}
  VecCreate(MPI\_Comm comm,int m,int M,Vec *v);\\
  VecSetFromOptions(v);
\end{verse}
which automatically generates the appropriate vector type 
(sequential or parallel) over all processors in \trl{comm}.
The option \trl{-vec_type} \trl{mpi} can be used in conjunction with 
\trl{VecCreate()} and \trl{VecSetFromOptions()} to specify the use of MPI \findex{-vec_type}
vectors even for the uniprocessor case.

We emphasize that all processors in \trl{comm} {\em must} call the
vector creation routines, since these routines are collective over all
processors in the communicator. If you are not familar with MPI communicators,
see the discussion in Section \ref{sec:writing} on page \pageref{sec:writing}. 
In addition, if a sequence of \trl{
VecCreateXXX()} routines is used, they must be called in the same
order on each processor in the communicator.

One can assign a single value to all components of a vector with the 
command \findex{VecSet()}
\begin{verse}
  VecSet(PetscScalar *value,Vec x);
\end{verse}
Assigning values to individual components of the vector is more 
complicated, in order to make it possible to write efficient parallel 
code.  Assigning a set of components is a two-step process: one 
first calls  \findex{VecSetValues()} \findex{INSERT_VALUES}
\begin{verse}
  VecSetValues(Vec x,int n,int *indices,PetscScalar *values,INSERT\_VALUES);
\end{verse}
any number of times on any or all of the processors. The argument
\trl{n} gives the number of components being set in this 
insertion. The integer array \trl{indices} contains the {\em global component
indices}, and \trl{values} is the array of values to be inserted.
Any processor can set any components of the vector; PETSc insures that 
they are automatically stored in the correct location.
Once all of the values have been inserted with \trl{VecSetValues()},
one must call \sindex{assembly}
\begin{verse}
  VecAssemblyBegin(Vec x);
\end{verse}
followed by \findex{VecAssemblyBegin()}\findex{VecAssemblyEnd()} 
\begin{verse}
  VecAssemblyEnd(Vec x);
\end{verse}
to perform any needed message passing of nonlocal components.
In order to allow the overlap of communication and calculation,
the user's code can perform any series of other actions between these 
two calls while the messages are in transition. 

Example usage of \trl{VecSetValues()} may be found in 
\trl{${PETSC_DIR}/src/vec/examples/tutorials/ex2.c or ex2f.F}

Often, rather than inserting elements in a vector, one may wish to 
add values. This process \sindex{vector values, setting}
is also done with the command \findex{VecSetValues()}
\begin{verse}
  VecSetValues(Vec x,int n,int *indices, PetscScalar *values,ADD\_VALUES);
\end{verse}
Again \findex{ADD_VALUES} one must call the assembly routines \trl{
VecAssemblyBegin()} and \trl{VecAssemblyEnd()} after all of the values
have been added.  Note that addition and insertion calls to \trl{
VecSetValues()} {\em cannot} be mixed.  Instead, one must add and insert
vector elements in phases, with intervening calls to the assembly
routines. This phased assembly procedure overcomes the nondeterministic
behavior that
would occur if two different processors generated values
for the same location, with one processor adding while the other is inserting
its value.  (In this case the addition and insertion actions could be performed 
in either order,
thus resulting in different values at the particular location. Since
PETSc does not allow the simultaneous use of \trl{INSERT_VALUES} and
\trl{ADD_VALUES} this nondeterministic behavior will not occur in PETSc.)

There is no routine called \trl{VecGetValues()}, since we provide 
an alternative method for extracting some components of a vector using
the vector scatter routines.  See Section~\ref{sec:scatter} for details; see also
below for \trl{VecGetArray()}.

One can examine a vector with the command \findex{VecView()}
\begin{verse}
  VecView(Vec x,PetscViewer v);
\end{verse}
To print the vector to the screen, one can use the viewer
\findex{PETSC_VIEWER_STDOUT_WORLD} \trl{PETSC_VIEWER_STDOUT_WORLD}, which ensures
that parallel vectors are printed correctly to \trl{stdout}. To display the vector in an X-window,
one can use the default X-windows viewer \trl{PETSC_VIEWER_DRAW_WORLD},
\findex{PETSC_VIEWER_DRAW_WORLD}
or one can create a viewer with the routine \trl{PetscViewerDrawOpenX()}.
A variety of viewers are discussed further in Section \ref{sec:viewers}.

To create a new vector of the same format as an existing vector, one uses
the command \findex{VecDuplicate()}
\begin{verse}
  VecDuplicate(Vec old,Vec *new);
\end{verse}
To create several new vectors of the same format as an existing vector,
one uses the command \findex{VecDuplicateVecs()}
\begin{verse}
  VecDuplicateVecs(Vec old,int n,Vec **new);
\end{verse}
This routine creates an array of pointers to vectors. The two routines 
are very useful because they allow one to write library code that does 
not depend on the particular format of the vectors being used. Instead,
the subroutines can automatically correctly create work vectors
based on the specified existing vector.  As discussed in 
Section~\ref{sec:fortvecd}, the Fortran interface for \trl{VecDuplicateVecs()}
differs slightly.

When a vector is no longer needed, it should be destroyed with the 
command \findex{VecDestroy()}
\begin{verse}
  VecDestroy(Vec x);
\end{verse}
To destroy an array of vectors, one should use the command \findex{VecDestroyVecs()}
\begin{verse}
  VecDestroyVecs(Vec *vecs,int n);
\end{verse}
Note that the Fortran interface for \trl{VecDestroyVecs()} differs slightly,
as described in Section~\ref{sec:fortvecd}.

It is also possible to create vectors that use an array provided by the user, 
rather than having PETSc internally allocate the array space. 
\sindex{providing arrays for vectors}
\sindex{vectors, user-supplied arrays} 
Such vectors can be created with the routines
\begin{verse}
  VecCreateSeqWithArray(PETSC\_COMM\_SELF,int m,PetscScalar *array,Vec *x);
\end{verse}
and  \findex{VecCreateMPIWithArray()} \findex{VecCreateSeqWithArray()}
\begin{verse}
  VecCreateMPIWithArray(MPI\_Comm comm,int m,int M,,PetscScalar *array,Vec *x);
\end{verse}
Note that here one must provide the value \trl{m}, it cannot be \trl{PETSC_DECIDE} and
the user is responsible for providing enough space in the array; \trl{m*sizeof(PetscScalar)}.


% --------------------------------------------------------------------------------------
\section{Basic Vector Operations}  
\label{sec:vecbasic}

\begin{table}[tb]
\begin{center}
\begin{tabular}{ll}
{\bf Function Name} & {\bf Operation} \\
\hline
VecAXPY(PetscScalar *a,Vec x, Vec y); & $ y = y + a*x$ \\
VecAYPX(PetscScalar *a,Vec x, Vec y); & $ y = x + a*y$ \\
VecWAXPY(PetscScalar *a,Vec x,Vec y, Vec w); & $ w = a*x + y$ \\
VecAXPBY(PetscScalar *a,PetscScalar *,Vec x,Vec y); & $ y = a*x + b*y$ \\
VecScale(PetscScalar *a, Vec x); & $ x = a*x $ \\
VecDot(Vec x, Vec y, PetscScalar *r); & $ r = \bar{x}'*y$ \\
VecTDot(Vec x, Vec y, PetscScalar *r); & $ r = x'*y$ \\
VecNorm(Vec x,NormType type,  double *r); & $ r = ||x||_{type}$ \\
VecSum(Vec x,   PetscScalar *r); & $ r = \sum x_{i}$ \\
VecCopy(Vec x, Vec y); & $ y = x $ \\
VecSwap(Vec x, Vec y); & $ y = x $ while $ x = y$ \\
VecPointwiseMult(Vec x,Vec y, Vec w); & $ w_{i} = x_{i}*y_{i} $ \\
VecPointwiseDivide(Vec x,Vec y, Vec w); & $ w_{i} = x_{i}/y_{i} $ \\
VecMDot(int n,Vec x, Vec *y,PetscScalar *r); & $ r[i] = \bar{x}'*y[i]$ \\
VecMTDot(int n,Vec x, Vec *y,PetscScalar *r); & $ r[i] = x'*y[i]$ \\
VecMAXPY(int n, PetscScalar *a,Vec y, Vec *x); \hspace{1cm} & $ y = y + \sum_i a_{i}*x[i] $ \\
VecMax(Vec x,  int *idx, double *r); & $ r = \max x_{i}$ \\
VecMin(Vec x,  int *idx, double *r); & $ r = \min x_{i}$ \\
VecAbs(Vec x); & $ x_i = |x_{i}|$ \\
VecReciprocal(Vec x); & $ x_i = 1/x_{i}$ \\
VecShift(PetscScalar *s,Vec x); & $ x_i = s + x_{i}$ \\
\hline 
\end{tabular}
\end{center}
\caption{\hbox{PETSc Vector Operations}}
\label{fig:vectorops}
\end{table}

As listed in Table \ref{fig:vectorops}, we have chosen certain 
basic vector operations to support within the PETSc vector library.
These operations were selected because they often arise in application 
codes. The \trl{NormType} argument to \trl{VecNorm()} is one of \findex{VecNorm()}
\findex{NormType} \trl{NORM_1}, \trl{NORM_2}, or \trl{NORM_INFINITY}.
\findex{NORM_1} \findex{NORM_2} \findex{NORM_INFINITY} \sindex{2-norm}
\sindex{1-norm} \sindex{infinity norm} The 1-norm is 
$ \sum_i |x_{i}|$, the 2-norm is $( \sum_{i} x_{i}^{2})^{1/2} $ and the 
infinity norm is $ \max_{i} |x_{i}|$. 


For parallel vectors that are distributed across the processors by ranges, 
it is possible to determine \findex{VecGetOwnershipRange()}
a processor's local range with the routine
\begin{verse}
  VecGetOwnershipRange(Vec vec,int *low,int *high);
\end{verse}
The argument \trl{low} indicates the first component owned by the local 
processor, while \trl{high} specifies {\em one more than} the 
last owned by the local processor.
This command is useful, for instance, in assembling parallel vectors.

On occasion, the user needs to access the actual elements of the vector. 
The routine \trl{VecGetArray()} \findex{VecGetArray()}
returns a pointer to the elements local to the processor:
\begin{verse}
  VecGetArray(Vec v,PetscScalar **array);
\end{verse}
When access to the array is no longer
needed, the user should call
\begin{verse}
  VecRestoreArray(Vec v, PetscScalar **array);
\end{verse}
Minor differences exist in the Fortran interface for \trl{VecGetArray()} and 
\trl{VecRestoreArray()}, as discussed in Section \ref{sec:fortranarrays}.  
It is important to note that \trl{VecGetArray()} and \trl{VecRestoreArray()}
do {\em not} copy the vector elements; they merely give users direct
access to the vector elements. Thus, these routines require essentially
no time to call and can be used efficiently.

The number of elements stored locally can be accessed with
\findex{VecGetLocalSize()}
\begin{verse}
  VecGetLocalSize(Vec v,int *size);
\end{verse}
The global vector length can be determined by \findex{VecGetSize()}
\begin{verse}
  VecGetSize(Vec v,int *size);
\end{verse}


In addition to \trl{VecDot()} and \trl{VecMDot()} and \trl{VecNorm()} PETSc provides
split phase versions of these that allow several independent inner products and/or norms
to share the same communication (thus improving parallel efficiency). For example,
one may have code such as 
\begin{verse}
 VecDot(Vec x,Vec y,PetscScalar *dot);\\
 VecNorm(Vec x,NormType NORM\_2,double *norm2);\\
 VecNorm(Vec x,NormType NORM\_1,double *norm1);
\end{verse}
This code works fine, the problem is that it performs three seperate parallel communication
operations. Instead one can write 
\begin{verse}
 VecDotBegin(Vec x,Vec y,PetscScalar *dot);\\
 VecNormBegin(Vec x,NormType NORM\_2,double *norm2);\\
 VecNormBegin(Vec x,NormType NORM\_1,double *norm1);\\
 VecDotEnd(Vec x,Vec y,PetscScalar *dot);\\
 VecNormEnd(Vec x,NormType NORM\_2,double *norm2);\\
 VecNormEnd(Vec x,NormType NORM\_1,double *norm1);
\end{verse}
With this code, \findex{VecDotBegin()} \findex{VecDotEnd()} \findex{VecNormBegin()} 
the \findex{VecNormEnd()} communication is delayed until the first call to \trl{VecxxxEnd()} at which 
a single MPI reduction is used to communicate all the required values. It is required that the
calls to the \trl{VecxxxEnd()} are performed in the same order as the calls to the 
\trl{VecxxxBegin()}; however if you mistakenly make the calls in the wrong order PETSc
will generate an error,
informing you of this. There are two additional routines \trl{VecTDotBegin()} and 
\trl{VecTDotEnd()}. \findex{VecTDotBegin()} \findex{VecTDotEnd()}
These routines where suggested by Victor Eijkhout.

Note: these routines use only MPI 1 functionality; so they do not allow you to overlap 
computation and communication. Once MPI 2 implementations are more common we'll improve these
routines to allow overlap of inner product and norm calculations with other calculations.
Also currently these routines only work for the PETSc built in vector types.

% ----------------------------------------------------------------------------------
\section{Indexing and Ordering}
\label{sec:indexingandordering}

  When writing parallel PDE codes there is extra complexity caused by
having multiple ways of indexing (numbering) and ordering objects such
as vertices and degrees of freedom. For example, a grid generator
or partitioner may renumber the nodes, requiring adjustment of the
other data structures that refer to these objects; see Figure
\ref{fig:daao}.  In addition, local numbering (on a single processor) 
of objects may be different than the global (cross-processor)
numbering. PETSc provides a variety of tools that help to manage the
mapping among the various numbering systems. The two most basic are
the \trl{AO} (application ordering), which enables mapping between
different global (cross-processor) numbering schemes and the \trl{
ISLocalToGlobalMapping}, which allows mapping between local 
(on-processor) and global (cross-processor) numbering.

% ----------------------------------------------------------------------------------
\subsection{Application Orderings}
\label{sec:ao}
\sindex{orderings} \findex{AO}

In many applications it is desirable to work with one or more
``orderings'' (or numberings) of degrees of freedom, cells, nodes,
etc.  \sindex{global numbering} Doing so in a parallel environment is
complicated by the fact that each processor cannot keep complete lists
of the mappings between different orderings. In addition, the
orderings used in the PETSc linear algebra routines (often contiguous 
ranges) may not correspond to the ``natural'' orderings for the application.

PETSc provides certain utility routines that allow one to deal cleanly
and efficiently with the various orderings. To define a new application ordering
(called an \trl{AO} in PETSc), one can call the routine 
\begin{verse}
  AOCreateBasic(MPI\_Comm comm,int n,int *apordering,int *petscordering,AO *ao);
\end{verse}
The \findex{AOCreateBasic} 
arrays \trl{apordering} and \trl{petscordering}, respectively, contain a list of integers
in the application ordering and their corresponding mapped values in the PETSc 
ordering. Each processor can provide whatever subset of the ordering it 
chooses, but multiple processors should never contribute duplicate values. 
The argument \trl{n} indicates the number of local contributed values.

For example, consider a vector of length five, where node 0 in the application ordering
corresponds to node 3 in the PETSc ordering.  In addition, nodes 1, 2, 3, and 4 of
the application ordering correspond, respectively, to nodes 2, 1, 4, and 0 of
the PETSc ordering.
We can write this correspondence as
\[
 { 0, 1, 2, 3, 4 }  \rightarrow  { 3, 2, 1, 4, 0 }. 
\]
The user can create the PETSc-AO mappings in a number of ways.  For example,
if using two processors, one could call
\begin{verse}
  AOCreateBasic(PETSC\_COMM\_WORLD,2,{0,3},{3,4},\&ao);
\end{verse}
on the first processor and 
\begin{verse}
  AOCreateBasic(PETSC\_COMM\_WORLD,3,{1,2,4},{2,1,0},\&ao);
\end{verse}
on the other processor.

Once the application ordering has been created, it can be used
with either of the commands
\begin{verse}
  AOPetscToApplication(AO ao,int n,int *indices);\\
  AOApplicationToPetsc(AO ao,int n,int *indices);
\end{verse}
Upon input, the \trl{n}-dimensional array \trl{indices} specifies 
the indices to be mapped, while upon output, \trl{indices} contains
the mapped values.
\findex{AOPetscToApplication()} \findex{AOApplicationToPetsc()}
Since we, in general, employ a parallel database for the
\trl{AO} mappings, it is crucial that all processors that
called \trl{AOCreateBasic()} also call these routines; these
routines {\em cannot} be called by just a subset of processors
in the MPI communicator that was used in the call to \trl{AOCreateBasic()}.

An alternative routine to create the application ordering, \trl{AO}, is 
\begin{verse}
  AOCreateBasicIS(IS apordering,IS petscordering,AO *ao);
\end{verse}
\findex{AOCreateBasicIS}
where index sets are used instead of integer arrays. 

The 
mapping routines 
\begin{verse}
  AOPetscToApplicationIS(AO ao,IS indices);\\
  AOApplicationToPetscIS(AO ao,IS indices);
\end{verse}
\findex{AOPetscToApplicationIS()} \findex{AOApplicationToPetscIS()}
will map index sets (IS objects) between orderings. Both the \trl{AOXxxToYyy()} and 
\trl{AOXxxToYyyIS()} routines can be used regardless of whether the \trl{AO} was 
created with a \trl{AOCreateBasic()} or \trl{AOCreateBasicIS()}.

\findex{AODestroy()}  \findex{AOView}
The \trl{AO} context should be destroyed with \trl{AODestroy(AO ao)}
and viewed with \trl{AOView(AO ao,PetscViewer viewer)}.

Although we refer to the two orderings as ``PETSc'' and
``application'' orderings, the user is free to use them both for
application orderings and to maintain relationships among a variety of
orderings by employing several \trl{AO} contexts.

The \trl{AOxxToxx()} routines allow negative entries in the input
integer array. These entries are not mapped; they simply remain
unchanged.  This functionality enables, for example, mapping neighbor
lists that use negative numbers to indicate nonexistent neighbors due
to boundary conditions, etc.

%-------------------------------------------------------------------------
\subsection{Local to Global Mappings}
\label{sec:islocaltoglobalmapping}
\sindex{orderings} \findex{ISLocalToGlobalMapping}

In many applications one works with a global representation of a vector
(usually on a vector obtained with \trl{VecCreateMPI()}) \findex{VecCreateMPI()}
and a local representation of the same vector that includes ghost points 
required for local computation. \sindex{ghost points} \sindex{global representation}
\sindex{local representation} \sindex{local to global mapping}
PETSc provides routines to help map indices from a local numbering scheme to 
the PETSc global numbering scheme. This is done via the following routines
\begin{verse}
  ISLocalToGlobalMappingCreate(int N,int* globalnum,\\
                                       ISLocalToGlobalMapping* ctx);\\
  ISLocalToGlobalMappingApply(ISLocalToGlobalMapping ctx,int n,int *in,\\
                                      int *out);\\
  ISLocalToGlobalMappingApplyIS(ISLocalToGlobalMapping ctx,IS isin,\\
                                        IS* isout);\\
  ISLocalToGlobalMappingDestroy(ISLocalToGlobalMapping ctx);
\end{verse}
\findex{ISLocalToGlobalMappingCreate()} \findex{ISLocalToGlobalMappingApply()}
\findex{ISLocalToGlobalMappingApplyIS()} \findex{ISLocalToGlobalMappingDestroy()}
Here \trl{N} denotes the number of local indices, \trl{globalnum} contains the
global number of each local number, and \trl{ISLocalToGlobalMapping} is the 
resulting PETSc object that contains the information needed to apply the mapping with 
either \trl{ISLocalToGlobalMappingApply()} or 
\trl{ISLocalToGlobalMappingApplyIS()}.  \findex{ISLocalToGlobalMapping}

Note that the \trl{ISLocalToGlobalMapping} routines serve a different purpose
than the \trl{AO} \findex{AO} routines. In the former case they provide a mapping 
from  a local numbering scheme (including ghost points) to a global numbering scheme,
while in the latter they provide a mapping between two global numbering schemes.
In fact, many applications may use both \trl{AO} and \trl{ISLocalToGlobalMapping} routines.
The \trl{AO} routines are first used to map from an application global ordering 
(that has no relationship to parallel processing etc.) to the PETSc ordering scheme
(where each processor has a contiguous set of indices in the numbering). Then in order
to perform function or Jacobian evaluations locally on each processor, one works
with a local numbering scheme that includes ghost points.  The mapping from this local
numbering scheme back to the global PETSc numbering can be handled with the 
\trl{ISLocalToGlobalMapping} routines.

If one is given a list of indices in a global numbering, the routine
\begin{verse}
  ISGlobalToLocalMappingApply(ISLocalToGlobalMapping ctx,\\
                              ISGlobalToLocalMappingType type,int nin,int *idxin,int *nout,int *idxout);
\end{verse}
\findex{ISGlobalToLocalMappingApply}  \sindex{global to local mapping} will provide
a new list of indices in the local numbering. Again, negative values in 
\trl{idxin} are left unmapped.  But, in addition, if \trl{type} is set to \trl{IS_GTOLM_MASK},
\findex{IS_GTOLM_MASK}, then \trl{nout} is set to \trl{nin} and all global values
in \trl{idxin} that are not represented in the local to global mapping
are replaced by -1. When \trl{type} is set to \trl{IS_GTOLM_DROP},
\findex{IS_GTOLM_DROP} the values in \trl{idxin} that are not
represented locally in the mapping are not included in \trl{idxout}, so that
potentially \trl{nout} is smaller than \trl{nin}.  One must
pass in an array long enough to hold all the indices. One can call
\trl{ISGlobalToLocalMappingApply()} with \trl{idxout} equal to \trl{
PETSC_NULL} to determine the required length (returned in \trl{nout})
and then allocate the required space and call \trl{
ISGlobalToLocalMappingApply()} a second time to set the values.

Often it is convenient to set elements into a vector using the local node 
numbering rather than the global node numbering (e.g.,  each processor may
maintain its own sublist of vertices and elements and number them locally).
To set values into a vector with the local numbering, one must first call
\begin{verse}
  VecSetLocalToGlobalMapping(Vec v,ISLocalToGlobalMapping ctx);
\end{verse}
\findex{VecSetLocalToGlobalMapping} \sindex{vectors, setting values with local numbering}
and then call \findex{VecSetValuesLocal}
\begin{verse}
  VecSetValuesLocal(Vec x,int n,int *indices,PetscScalar *values,INSERT\_VALUES);
\end{verse}
Now the \trl{indices} use the local numbering, rather than the global.

% ------------------------------------------------------
\section{Structured Grids Using Distributed Arrays}
\label{sec:da} \label{sec:struct}

  Distributed arrays (\trl{DA}s), which are used in
conjunction with PETSc vectors, are intended for use with {\em
logically regular rectangular grids} when communication of nonlocal data is
needed before certain local computations can occur.  PETSc distributed
arrays are designed only for the case in which data can be thought of
as being stored in a standard multidimensional array; thus, \trl{DA}s 
are {\em not} intended for parallelizing unstructured grid problems, etc.
\trl{DA}s are intended for communicating vector (field) information; they 
are not intended for storing matrices. 
% See Section \ref{sec:bdiag} for sparse matrix
% data structures intended specifically for problems defined on rectangular 
% grids; but note that general purpose sparse matrix formats discussed in 
% the next chapter are also appropriate (in fact, on RISC based processors,
% often one sees no performance gain by using grid-specific sparse matrix
% storage schemes).

For example, a typical situation one encounters in solving 
PDEs in parallel is that, to evaluate a local function, \trl{f(x)}, each processor
requires its local portion of the vector \trl{x} as well as its ghost
points \sindex{ghost points} (the bordering portions of the vector
that are owned by neighboring processors).  Figure~\ref{fig:ghosts}
illustrates the ghost points for the seventh processor of a
two-dimensional, regular parallel grid.  Each box represents a
processor; the ghost points for the seventh processor's local part of
a parallel array are shown in gray.

\begin{figure}[tb]
\centerline{ \includegraphics{ghost}}
%\centerline{\psfig{file=ghost.eps,angle=0}}
\caption{Ghost Points for Two Stencil Types on the Seventh Processor}
\label{fig:ghosts}
\end{figure}

\subsection{Creating Distributed Arrays}

The PETSc \trl{DA} object manages the parallel communication required
while working with data stored in regular arrays. The actual data
is stored in approriately sized vector objects; the \trl{DA} object 
only contains the parallel data layout information and communication
information. 

One creates a distributed array communication data structure 
in two dimensions with the command 
\begin{verse}
  DACreate2d(MPI\_Comm comm,DAPeriodicType wrap,DAStencilType st,int M,\\
             int N,int m,int n,int dof,int s,int *lx,int *ly,DA *da);
\end{verse}
The \findex{DACreate2d} \sindex{array, distributed} arguments
\sindex{distributed array} \trl{M} and \trl{N} indicate the global
numbers of grid points in each direction, while \trl{m} and \trl{n}
denote the processor partition in each direction; \trl{m*n} must equal
the number of processors in the MPI communicator, \trl{comm}.  
Instead of specifying the processor layout, one may use
\trl{PETSC_DECIDE} for \trl{m} and \trl{n} 
so that PETSc will determine the partition using MPI. The type of
periodicity of the array is specified by \trl{wrap}, which can be 
\trl{DA_NONPERIODIC} \findex{DA_NONPERIODIC} (no periodicity), 
\trl{DA_XYPERIODIC} \findex{DA_XYPERIODIC} (periodic in
both x- and y-directions), \trl{DA_XPERIODIC} \findex{DA_XPERIODIC}, 
or \trl{DA_YPERIODIC}.\findex{DA_YPERIODIC}  The argument \trl{dof} 
indicates the number of degrees of freedom at each array point,
and \trl{s} is the stencil width (i.e., the width of the ghost point region).
The optional arrays \trl{lx} and \trl{ly} may contain the number of nodes
along the x and y axis for each cell, i.e. the dimension of \trl{lx} is
\trl{m} and the dimension of \trl{ly} is \trl{n}; or \trl{PETSC_NULL} 
may be passed in.

Two types of distributed array communication data structures 
can be created, as specified by \trl{st}.
Star-type stencils that radiate outward only in the coordinate
directions are indicated by \trl{DA_STENCIL_STAR},
\findex{DA_STENCIL_STAR} while box-type stencils are specified by
\trl{DA_STENCIL_BOX}. \findex{DA_STENCIL_BOX} For example, for the
two-dimensional case,
\trl{DA_STENCIL_STAR} with width 1 corresponds to the standard 5-point
stencil, while \trl{DA_STENCIL_BOX} with width 1 denotes the
standard 9-point stencil.  In both instances the ghost points are
identical, the only difference being that with star-type stencils
certain ghost points are ignored, potentially decreasing substantially
the number of messages sent.  Note that the \trl{DA_STENCIL_STAR}
stencils can save interprocessor communication in two and three
dimensions.

These \trl{DA} stencils have nothing directly to do with any finite
difference stencils one might chose to use for a discretization; they
only ensure that the correct values are in place for application of a
user-defined finite difference stencil (or any other
discretization technique).

The commands for creating distributed array communication data structures
in one and three dimensions are analogous:
\findex{DACreate3d()} \findex{DACreate1d()} \findex{DACreate3d()}
\begin{verse}
  DACreate1d(MPI\_Comm comm,DAPeriodicType wrap,int M,int w,int s,int *lc,DA *inra);\\
  DACreate3d(MPI\_Comm comm,DAPeriodicType wrap,DAStencilType stencil\_type,\\
             int M,int N,int P,int m,int n,int p,int w,int s,int *lx,\\
             int *ly,int *lz,DA *inra);
\end{verse}
\trl{DA_ZPERIODIC}, \findex{DA_ZPERIODIC}
\trl{DA_XZPERIODIC}, \findex{DA_XZPERIODIC}
\trl{DA_YZPERIODIC}, and \findex{DA_YZPERIODIC}
\trl{DA_XYZPERIODIC} \findex{DA_XYZPERIODIC}
are additional options in three dimensions for \trl{DAPeriodicType}.
The routines to create distributed arrays are collective, so that all
processors in the communicator \trl{comm} must call \trl{DACreateXXX()}.

%----------------------------------------------------------------------------------
\subsection{Local/Global Vectors and Scatters}

Each \trl{DA} object defines the layout of two vectors: a distributed
global vector and a local vector that includes room for the
appropriate ghost points.  The \trl{DA} object provides information
about the size and layout of these vectors, but does not internally
allocate any associated storage space for field values.  Instead, the
user can create vector objects that use the \trl{DA} layout
information with the routines
\begin{verse}
  DACreateGlobalVector(DA da,Vec *g);\\
  DACreateLocalVector(DA da,Vec *l);
\end{verse}
\findex{DACreateGlobalVector}
\findex{DACreateLocalVector()}
These vectors will generally serve as the building blocks for local
and global PDE solutions, etc.  If additional vectors with such
layout information are needed in a code, they can be obtained by
duplicating \trl{l} or \trl{g} via
\trl{VecDuplicate()} or \trl{VecDuplicateVecs()}.

We emphasize that a distributed array provides the information needed
to communicate the ghost value information between processes.  In most
cases, several different vectors can share the same communication
information (or, in other words, can share a given \trl{DA}).  The
design of the \trl{DA} object makes this easy, as each \trl{DA}
operation may operate on vectors of the appropriate size, as obtained
via \trl{DACreateLocalVector()} and \trl{DACreateGlobalVector()} or as
produced by \trl{VecDuplicate()}.  As such, the \trl{DA}
scatter/gather operations (e.g., \trl{DAGlobalToLocalBegin()}) require
vector input/output arguments, as discussed below.

PETSc currently provides no container for multiple arrays sharing the
same distributed array communication; note, however, that the \trl{dof}
parameter handles many cases of interest.

At certain stages of many applications, there is a need to work 
on a local portion of the vector, including the ghost points. 
This may be done by scattering a global vector into its 
local parts by using the two-stage commands
\begin{verse}
  DAGlobalToLocalBegin(DA da,Vec g,InsertMode iora,Vec l);\\
  DAGlobalToLocalEnd(DA da,Vec g,InsertMode iora,Vec l);
\end{verse}
which allow the overlap of communication and computation.
\findex{DAGlobalToLocalEnd()}  \findex{DAGlobalToLocalBegin()}
Since the global and local vectors, given by \trl{g} and \trl{l}, respectively,
must be compatible with the distributed array, \trl{da}, they should be
generated by \trl{DACreateGlobalVector()} 
\findex{DACreateGlobalVector()} and \trl{DACreateLocalVector()}
\findex{DACreateLocalVector()}
(or be duplicates of such a vector obtained via \trl{VecDuplicate()}).
The \trl{InsertMode} can be either \trl{ADD_VALUES} or \trl{INSERT_VALUES}.

One can scatter the local patches into the distributed vector
with the command \findex{DALocalToGlobal()}
\begin{verse}
  DALocalToGlobal(DA da,Vec l,InsertMode mode,Vec g);
\end{verse}
Note that this function is not
subdivided into beginning and ending phases, since it is purely local.

A third type of distributed array scatter is from a local
vector (including ghost points that contain irrelevant values) to 
a local vector with correct ghost point values. 
This scatter may be done by \findex{DALocalToLocalBegin()}
commands \findex{DALocalToLocalEnd()}
\begin{verse}
  DALocalToLocalBegin(DA da,Vec l1,InsertMode iora,Vec l2);\\
  DALocalToLocalEnd(DA da,Vec l1,InsertMode iora,Vec l2);
\end{verse}
Since both local vectors, \trl{l1} and \trl{l2},
must be compatible with the distributed array, \trl{da}, they should be
generated by \trl{DACreateLocalVector()} \findex{DACreateLocalVector()}
(or be duplicates of such vectors obtained via \trl{VecDuplicate()}).
The \trl{InsertMode} can be either \trl{ADD_VALUES} or \trl{INSERT_VALUES}.

It is possible to directly access the vector scatter contexts (see below)
used in the local-to-global (\trl{ltog}), global-to-local 
(\trl{gtol}), and local-to-local (\trl{ltol})
scatters with the command \findex{DAGetScatter()}
\begin{verse}
  DAGetScatter(DA da,VecScatter *ltog,VecScatter *gtol,VecScatter *ltol);
\end{verse}
Most users should not need to use these contexts.

\subsection{Local (Ghosted) Work Vectors}
In most applications the local ghosted vectors are only needed during user
``function evaluations''. PETSc provides an easy light-weight (requiring 
essentially no CPU time) way to obtain these work vectors and return them when
they are no longer needed. This is done with the routines
\begin{verse}
  DAGetLocalVector(DA da,Vec *l);\\
   .... use the local vector l\\
  DARestoreLocalVector(DA da,Vec *l);
\end{verse}

\subsection{Accessing the Vector Entries for DA Vectors}
PETSc provides an easy way to set values into the DA Vectors and access them using
the natural grid indexing. This is done with the routines 
\begin{verse}
  DAVecGetArray(DA da,Vec l,(void**)array);\\
   .... use the array indexing it with 1 or 2 or 3 dimensions \\
   .... depending on the dimension of the DA\\
  DAVecRestoreArray(DA da,Vec l,(void**)array);
\end{verse}
The vector l can be either a global vector or a local vector.
The {\tt array} is accessed using the usual {\bf global} indexing
on the entire grid.
For example for a scalar problem in two dimensions one could do
\begin{verse}
   PetscScalar **f,**u;\\
   ...\\
  DAVecGetArray(DA da,Vec local,(void**)u);\\
  DAVecGetArray(DA da,Vec global,(void**)f);\\
   ...\\
      f[i][j] = u[i][j] - ...\\
   ...\\
  DAVecRestoreArray(DA da,Vec local,(void**)u);\\
  DAVecRestoreArray(DA da,Vec global,(void**)f);
\end{verse}
See \trl{${PETSC_DIR}/src/snes/examples/tutorials/ex5.c} for a 
complete example and See \trl{${PETSC_DIR}/src/snes/examples/tutorials/ex19.c} for an
example for a multi-component PDE.

%---------------------------------------------------------------------------
\subsection{Grid Information}

The global indices of the lower left corner of the local portion of the array 
as well as the local array size can be obtained with the commands
\findex{DAGetCorners()} \findex{DAGetGhostCorners()}
\begin{verse}
  DAGetCorners(DA da,int *x,int *y,int *z,int *m,int *n,int *p);\\
  DAGetGhostCorners(DA da,int *x,int *y,int *z,int *m,int *n,int *p);
\end{verse}
The first version excludes any ghost points, while the second version
includes them. 
The routine \findex{DAGetGhostCorners()} \trl{DAGetGhostCorners()}
deals with the fact that subarrays along boundaries of the problem
domain have ghost points only on their interior edges, but not on
their boundary edges.

When either type of stencil is used, \trl{DA_STENCIL_STAR} or 
\trl{DA_STENCIL_BOX}, the local vectors (with the ghost points) 
represent rectangular arrays, including the extra corner elements in 
the \trl{DA_STENCIL_STAR} case. This configuration provides simple 
access to the elements by employing two- (or three-) dimensional indexing. 
The only difference between the 
two cases is that when \trl{DA_STENCIL_STAR} is used, the extra 
corner components are {\em not} scattered between the processors and thus
contain undefined values that should {\em not} be used.

To assemble global stiffness matrices, one needs either 

1) to be able to determine the global node number of each local node 
including the ghost nodes. The number may be determined by using the 
command \findex{DAGetGlobalIndices()}
\begin{verse}
  DAGetGlobalIndices(DA da,int *n,int **idx);
\end{verse}
The output argument \trl{n} contains the number of 
local nodes, including ghost nodes, while \trl{idx} contains
a list of the global indices that correspond to the local nodes.
Note that the Fortran interface differs slightly; see Section~\ref{sec:fortranarrays}
for details.

2) or to set up the vectors and matrices so that their entries may be
added using the local numbering. This is done by first calling 
\begin{verse}
  DAGetISLocalToGlobalMapping(DA da,ISLocalToGlobalMapping *map);
\end{verse}
followed by 
\begin{verse}
  VecSetLocalToGlobalMapping(Vec x,ISLocalToGlobalMapping map);\\
  MatSetLocalToGlobalMapping(Vec x,ISLocalToGlobalMapping map);
\end{verse}
Now entries may be added to the vector and matrix using the local numbering
and \trl{VecSetValuesLocal()} and \trl{MatSetValuesLocal()}.


Since the global ordering that PETSc uses to manage its parallel vectors 
(and matrices) does not usually correspond to the ``natural'' ordering 
of a two- or three-dimensional array, the \trl{DA} structure provides 
an application ordering \trl{AO} (see Section \ref{sec:ao}) that maps 
between the natural ordering on a rectangular grid and the ordering PETSc
uses to parallize. This ordering context can be obtained with the command
\begin{verse}
  DAGetAO(DA da,AO *ao);
\end{verse}
\findex{DAGetAO}
In Figure \ref{fig:daao} we indicate the orderings for a two-dimensional distributed 
array, divided among four processors.

\begin{figure}[tb]
\centerline{ \includegraphics{danumbering}}
%\centerline{\psfig{file=danumbering.eps,angle=270,width=4.7in}}
\caption{Natural Ordering and PETSc Ordering for a 2D Distributed Array (Four Processors)}
\label{fig:daao}
\end{figure}

The example
\trl{${PETSC_DIR}/src/snes/examples/tutorials/ex5.c},
illustrates the use of a distributed array in the solution of
a nonlinear problem.  The analogous Fortran program is
\break \trl{${PETSC_DIR}/src/snes/examples/tutorials/ex5f.F};
See Chapter \ref{chapter:snes} for a discussion of the nonlinear
solvers.

%\begin{figure}[H]
%{\small
%\fileinclude{../../../src/snes/examples/tutorials/ex5.c}
%}
%\caption{Use of Distributed Arrays}
%\label{fig:daexample}
%\end{figure}

% -------------------------------------------------------------------------------------- 
\section{Software for Managing Vectors Related to Unstructured Grids}
\label{sec:unstruct}

% -------------------------------------------------------------------------------------- 
\subsection{Index Sets} \sindex{index sets}
\label{sec:indexset}

To facilitate general vector scatters and gathers used, for example, in updating 
ghost points for problems defined on unstructured grids, PETSc employs the 
concept of an index set.  An index set, which is a generalization of a 
set of integer indices, is used to define scatters, gathers, and similar 
operations on vectors and matrices. 

The following command creates a index set based on a list 
of integers: \findex{ISCreateGeneral()}
\begin{verse}
  ISCreateGeneral(MPI\_Comm comm,int n,int *indices, IS *is);
\end{verse}
This routine essentially copies the \trl{n} indices passed 
to it by the integer array \trl{indices}.  
Thus, the user should be sure to free the integer array \trl{indices} 
when it is no longer needed, perhaps directly after the call to 
\trl{ISCreateGeneral()}. The communicator, \trl{comm}, should consist of all 
processors that will be using the \trl{IS}.

Another standard index set is defined by a starting point (\trl{first}) and a
stride (\trl{step}), \sindex{stride} and can be created with the command
\begin{verse}
  ISCreateStride(MPI\_Comm comm,int n,int first,int step,IS *is);
\end{verse}

Index sets can be destroyed with the command \findex{ISDestroy()}
\begin{verse}
  ISDestroy(IS is); 
\end{verse}

On rare occasions the user may have to access information directly 
from an index set. \findex{ISGetSize()} 
Several commands \findex{ISStrideGetInfo()} \findex{ISGetIndices()}
assist in this process:
\begin{verse}
  ISGetSize(IS is,int *size);\\
  ISStrideGetInfo(IS is,int *first,int *stride);\\
  ISGetIndices(IS is,int **indices);
\end{verse}
The function \trl{ISGetIndices()} returns a pointer to a list of the 
indices in the index set. 
For certain index sets, this may be a 
temporary array of indices created specifically for a given routine. 
Thus, once the user finishes using the array of indices, 
the routine \findex{ISRestoreIndices()}
\begin{verse}
  ISRestoreIndices(IS is, int **indices); 
\end{verse}
should be called to ensure that the system can free the space it 
may have used to generate the list of indices.

A blocked version of the index sets can be created with the command
\findex{ISCreateBlock}
\begin{verse}
  ISCreateBlock(MPI\_Comm comm,int bs,int n,int *indices, IS *is);
\end{verse}
This version is used for defining operations in which each element of the index
set refers to a block of \trl{bs} vector entries.  Related routines analogous
to those described above exist as well, including
\trl{ISBlockGetIndices()}, \trl{ISBlockGetSize()}, \trl{ISBlockGetBlockSize()},
and \trl{ISBlock()}. See the man pages for details.
\findex{ISBlockGetIndices()} \findex{ISBlockGetSize()} \findex{ISBlockGetBlockSize()}
\findex{ISBlock()}

% -------------------------------------------------------------------------------------- 
\subsection{Scatters and Gathers} \sindex{scatter} \sindex{gather}
\label{sec:scatter}

PETSc vectors have full support for general scatters and 
gathers. One can select any subset of the components of a vector to
insert or add to any subset of the components of another vector.
We refer to these operations as generalized scatters, though they are 
actually a combination of scatters and gathers. 

\findex{VecScatterCreate()} \findex{VecScatterBegin()}
\findex{VecScatterEnd()} \findex{VecScatterDestroy()}
\findex{INSERT_VALUES} \findex{SCATTER_FORWARD}
To copy selected components from one vector 
to another, one uses the following set of commands:
\begin{verse}
  VecScatterCreate(Vec x,IS ix,Vec y,IS iy,VecScatter *ctx);\\
  VecScatterBegin(Vec x,Vec y,INSERT\_VALUES,SCATTER\_FORWARD,VecScatter ctx);\\
  VecScatterEnd(Vec x,Vec y,INSERT\_VALUES,SCATTER\_FORWARD,VecScatter ctx);\\
  VecScatterDestroy(VecScatter ctx);
\end{verse} 
Here \trl{ix} denotes the index set of the first vector, while \trl{
iy} indicates the index set of the destination vector.  The vectors
can be parallel or sequential. The only requirements are that the
number of entries in the index set of the first vector, \trl{ix},
equal the number in the destination index set, \trl{iy}, and that the
vectors be long enough to contain all the indices referred to in the
index sets.  The argument \trl{INSERT_VALUES} specifies that the
vector elements will be inserted into the specified locations of the
destination vector, overwriting any existing values.  To add the
components, rather than insert them, the user should select the option
\trl{ADD_VALUES} \findex{ADD_VALUES} instead of \trl{INSERT_VALUES}.

To perform a conventional gather operation, the user simply makes
 the destination index set, 
\trl{iy}, be a stride index set with a stride of one.  Similarly, a 
conventional scatter can be done with an initial (sending) index set 
consisting of a stride.  For parallel vectors, all processors that own 
the vector {\em must} call the scatter routines. When scattering from a 
parallel vector to sequential vectors, each processor has its own sequential 
vector that receives values from locations as indicated in its own 
index set. Similarly, in scattering
from sequential vectors to a parallel vector, each processor has its
own sequential vector that makes contributions to the parallel vector.

{\em Caution}: When \trl{INSERT_VALUES} is used, if two different
processors contribute different values to the same component in a
parallel vector, either value may end up being inserted. When \trl{
ADD_VALUES} is used, the correct sum is added to the correct
location.

In some cases one may wish to ``undo'' a scatter, that is perform the 
scatter backwards switching the roles of the sender and receiver. This is 
done by using 
\begin{verse}
  VecScatterBegin(Vec y,Vec x,INSERT\_VALUES,SCATTER\_REVERSE,VecScatter ctx);\\
  VecScatterEnd(Vec y,Vec x,INSERT\_VALUES,SCATTER\_REVERSE,VecScatter ctx);
\end{verse} 
\findex{SCATTER_REVERSE} Note that the roles of the first 
two arguments to these routines must be swapped whenever the \trl{SCATTER_REVERSE}
option is used.

Once a \trl{VecScatter} object has been created it may be used with any vectors
that have the appropriate parallel data layout. That is, one can call 
\trl{VecScatterBegin()} and \trl{VecScatterEnd()} with different vectors than 
used in the call to \trl{VecScatterCreate()} so long as they have the same 
parallel layout (number of elements on each processor are the same). Usually,
these ``different'' vectors would ahve been obtained vai calls to 
\trl{VecDuplicate()} from the original vectors used in the call to 
\trl{VecScatterCreate()}.

\sindex{vector values, getting} \findex{VecGetValues()}
There is no PETSc routine that is the opposite of \trl{VecSetValues()}
\findex{VecSetValues()}, that is, \trl{VecGetValues()}. \findex{VecGetValues()}
Instead, the user should create a new vector where
the components are to be stored and perform the appropriate vector 
scatter. For example, if one desires to obtain the values of the 
100th and 200th entries of a parallel vector, \trl{p}, one could use 
a code such as that within Figure~\ref{fig:vecscatter}.
In this example, the values of the 100th and 200th components are
placed in the array 
\trl{values}. In this example each processor now has the 100th and 
200th component, but obviously each processor could gather any 
elements it needed, or none by creating an index set with no entries.

\begin{figure}[tb]
\begin{verbatim}
   Vec         p, x;         /* initial vector, destination vector */
   VecScatter  scatter;      /* scatter context */
   IS          from, to;     /* index sets that define the scatter */
   PetscScalar *values;
   int         idx_from[] = {100,200}, idx_to[] = {0,1};

   VecCreateSeq(PETSC_COMM_SELF,2,&x);
   ISCreateGeneral(PETSC_COMM_SELF,2,idx_from,&from);
   ISCreateGeneral(PETSC_COMM_SELF,2,idx_to,&to);
   VecScatterCreate(p,from,x,to,&scatter);
   VecScatterBegin(p,x,INSERT_VALUES,SCATTER_FORWARD,scatter);
   VecScatterEnd(p,x,INSERT_VALUES,SCATTER_FORWARD,scatter);
   VecGetArray(x,&values);
   ISDestroy(from);
   ISDestroy(to); 
   VecScatterDestroy(scatter);
\end{verbatim}
\caption{Example Code for Vector Scatters}
\label{fig:vecscatter}
\end{figure}

The scatter comprises two stages, in order to allow overlap of 
communication and computation. The introduction of the 
\trl{VecScatter} context allows the communication patterns for the scatter
to be computed once and then reused repeatedly. Generally, even 
setting up the communication for a scatter requires communication; 
hence, it is best to reuse such information when possible.

% -------------------------------------------------------------------------------
\subsection{Scattering Ghost Values}

The scatters provide a very general method for managing the communication of 
required ghost values for unstructured grid computations. One scatters
the global vector into a local ``ghosted'' work vector, performs the computation
on the local work vectors, and then scatters back into the global solution 
vector. In the simplest case this may be written as
\begin{verse}
   Function: (Input Vec globalin, Output Vec globalout)\\

  VecScatterBegin(Vec globalin,Vec localin,InsertMode INSERT\_VALUES,\\
                          ScatterMode SCATTER\_FORWARD,VecScatter scatter);\\
  VecScatterEnd(Vec globalin,Vec localin,InsertMode INSERT\_VALUES,\\
                        ScatterMode SCATTER\_FORWARD,VecScatter scatter);\\
   /*\\
        For example,  do local calculations from localin to localout \\
   */\\
  VecScatterBegin(Vec localout,Vec globalout,InsertMode ADD\_VALUES,\\
                          ScatterMode SCATTER\_REVERSE,VecScatter scatter);\\
  VecScatterEnd(Vec localout,Vec globalout,InsertMode ADD\_VALUES,\\
                        ScatterMode SCATTER\_REVERSE,VecScatter scatter);
\end{verse}

% -------------------------------------------------------------------------------
\subsection{Vectors with Locations for Ghost Values}

We recommend that application developers skip this section on a first reading.
It contains information about more advanced use of PETSc vectors to improve 
efficiency slightly. Once an application code is fully debugged and optimized
these techniques can be tried to slightly decrease memory use and improve 
computation speed.

There are two minor drawbacks to the basic approach described above:
\begin{itemize}
\item the extra memory requirement for the local work vector, \trl{localin} that
      duplicates the memory in \trl{globalin}, and
\item the extra time required to copy the local values from \trl{localin} to 
      \trl{globalin}.
\end{itemize}

An alternative approach is to allocate global vectors with space preallocated for 
the ghost values; this may be done with either 
\begin{verse}
  VecCreateGhost(MPI\_Comm comm,int n,int N,int nghost,int *ghosts,Vec *vv)
\end{verse}
or
\begin{verse}
  VecCreateGhostWithArray(MPI\_Comm comm,int n,int N,int nghost,int *ghosts,\\
                                  PetscScalar *array,Vec *vv)
\end{verse}
Here \findex{VecCreateGhost()} \findex{VecCreateGhostWithArray()} \trl{n} is the 
number of local vector entries, \trl{N} is the number of \sindex{vectors, with ghost values}
global entries (or \trl{PETSC_NULL}) and \trl{nghost} is the number of 
ghost entries. The array \trl{ghosts} is of size \trl{nghost} and contains the 
global vector location for each local ghost location. Using \trl{VecDuplicate()}
or \trl{VecDuplicateVecs()} on a ghosted vector will generate additional ghosted vectors.

In many ways a ghosted vector behaves just like any other \trl{MPI} vector created 
by \trl{VecCreateMPI()}, the difference is that the ghosted vector has an additional 
``local'' representation that allows one to access the ghost locations. This is done
through the call to \findex{VecGhostGetLocalForm()} 
\begin{verse}
 VecGhostGetLocalForm(Vec g,Vec *l);
\end{verse}
The vector \trl{l} is a \findex{VecGhostRestoreLocalForm()}
sequential representation of the parallel vector \trl{g} 
that shares the same array space (and hence numerical values); but allows one to 
access the ``ghost'' values past ``the end of the'' array. Note that one access the 
entries in \trl{l} using the local numbering of elements and ghosts, while they 
are accessed in \trl{g} using the global numbering.

A common usage of a ghosted vector is given by
\findex{VecGhostUpdateBegin()} \findex{VecGhostUpdateEnd()}
\begin{verse}
  VecGhostUpdateBegin(Vec globalin,InsertMode INSERT\_VALUES,ScatterMode SCATTER\_FORWARD);\\
  VecGhostUpdateEnd(Vec globalin,InsertMode INSERT\_VALUES,ScatterMode SCATTER\_FORWARD);\\
  VecGhostGetLocalForm(Vec globalin,Vec *localin);\\
  VecGhostGetLocalForm(Vec globalout,Vec *localout);\\
   /*\\
      Do local calculations from localin to localout \\
   */\\
  VecGhostRestoreLocalForm(Vec globalin,Vec *localin);\\
  VecGhostRestoreLocalForm(Vec globalout,Vec *localout);
  VecGhostUpdateBegin(Vec globalout,InsertMode ADD\_VALUES,ScatterMode SCATTER\_REVERSE);\\
  VecGhostUpdateEnd(Vec globalout,InsertMode ADD\_VALUES,ScatterMode SCATTER\_REVERSE);
\end{verse}
      
The routines \trl{VecGhostUpdateBegin/End()} are equivalent to the routines \trl{VecScatterBegin/End()}
above except that since they are scattering into the ghost locations, they do not need
to copy the local vector values, which are already in place. In addition, the user does not
have to allocate the local work vector, since the ghosted vector already has allocated 
slots to contain the ghost values.

The input arguments \trl{INSERT_VALUES} and \trl{SCATTER_FORWARD}
cause the ghost values to be correctly updated from the appropriate
processor. The arguments \trl{ADD_VALUES} and \trl{SCATTER_REVERSE }
update the ``local'' portions of the vector from all the other
processors' ghost values.  This would be appropriate, for example,
when performing a finite element assembly of a load vector.

Section \ref{sec:partitioning} discusses the important topic of partitioning 
an unstructured grid.


%-------------------------------------------------------------
%-------------------------------------------------------------
\chapter{Matrices}
\label{chapter:matrices}
\sindex{matrices}

PETSc provides a variety of matrix implementations because no
single matrix format is appropriate for all problems.  Currently we
support dense storage and compressed sparse row storage (both
sequential and parallel versions), as well as several specialized
formats.  Additional formats can be added.

This chapter describes the basics of using PETSc matrices in general
(regardless of the particular format chosen) and discusses tips for
efficient use of the several simple uniprocessor and parallel matrix
types.  The use of PETSc matrices involves the following actions:
create a particular type of matrix, insert values into it, process the
matrix, use the matrix for various computations, and finally destroy
the matrix.  The application code does not need to know or care about
the particular storage formats of the matrices.

\section{Creating and Assembling Matrices}
\label{sec:matcreate}

The simplest routine for forming a PETSc matrix, \trl{A}, is \findex{MatCreate()}
\begin{verse}
  MatCreate(MPI\_Comm comm,int m,int n,int M,int N,Mat *A)
\end{verse}
This routine generates a sequential matrix when running on one
processor and a parallel matrix for two or more processors; the
particular matrix format is set by the user via options database
commands.  The user specifies either or the global matrix dimensions, given
by \trl{M} and \trl{N} and the local dimensions, given by \trl{m} and 
\trl{n} while PETSc completely controls memory allocation.  This routine
facilitates switching among various matrix types, for example, to
determine the format that is most efficient for a certain
application.  By default, \trl{MatCreate()} employs the sparse AIJ
format, which is discussed in detail Section~\ref{sec:matsparse}.  See
the manual pages for further information about available matrix formats.

To insert or add entries to a matrix, one can call a variant of \trl{
MatSetValues}, either \findex{MatSetValues()}
\begin{verse}
  MatSetValues(Mat A,int m,int *im,int n,int *in,PetscScalar *values,INSERT\_VALUES);
\end{verse}
or 
\begin{verse}
  MatSetValues(Mat A,int m,int *im,int n,int *in,PetscScalar *values,ADD\_VALUES);
\end{verse}
This routine inserts or adds a logically dense subblock of dimension
\trl{m*n} into the 
matrix. The integer indices \trl{im} and \trl{in}, respectively, indicate the 
global row and column numbers to be inserted.  \trl{MatSetValues()} uses the 
standard C convention, where the row and column matrix indices begin with 
zero {\em regardless of the storage format employed}.   The array 
\trl{values} is logically two-dimensional, containing the values that are 
to be inserted.   
By default the values are given in row major order, which is the opposite 
of the Fortran convention. To allow the insertion of values in column 
major order, one can call the command \findex{MatSetOption()}
\begin{verse}
  MatSetOption(Mat A,MAT\_COLUMN\_ORIENTED);
\end{verse}
{\bf Warning}: Several of the sparse implementations do {\em not} currently support
the column-oriented option!

This notation should not be a mystery to anyone. For example, 
to insert one matrix into another when using Matlab, one uses the command 
\trl{A(im,in) = B;} where \trl{im} and \trl{in} contain the indices for the
rows and columns. This action is identical to the calls above to 
\trl{MatSetValues()}.

When using the block compressed sparse row matrix format (\trl{MATSEQBAIJ} or
\trl{MATMPIBAIJ}), one can insert elements more efficiently using the block
variant, \trl{MatSetValuesBlocked()}. \findex{MatSetValuesBlocked()}

The function \trl{MatSetOption()} accepts several other inputs; see
the manual page for details. We
discuss two of these options, which are related to the efficiency of the
assembly process.  To indicate to PETSc that the row (\trl{im}) or
column (\trl{in}) indices set with \trl{MatSetValues()} are sorted,
one uses the command \findex{MAT_ROWS_SORTED} \findex{MAT_COLUMNS_SORTED}
\begin{verse}
  MatSetOption(Mat A,MAT\_ROWS\_SORTED);
\end{verse}
or 
\begin{verse}
  MatSetOption(Mat A,MAT\_COLUMNS\_SORTED);
\end{verse}
Note that these flags indicate the format of the data passed in with 
\trl{MatSetValues()}; they do not have anything to do with how the sparse
matrix data is stored internally in PETSc.

After the matrix elements have been inserted or added into the matrix, 
they must be processed before they can be used. The routines for matrix
processing are \findex{MatAssemblyBegin()} \findex{MatAssemblyEnd()}
\begin{verse}
  MatAssemblyBegin(Mat A,MAT\_FINAL\_ASSEMBLY);\\
  MatAssemblyEnd(Mat A,MAT\_FINAL\_ASSEMBLY);
\end{verse}
By placing other code between these two calls, the user can perform
computations while messages are in transition.
Calls to \trl{MatSetValues()} with the \trl{INSERT_VALUES} and \trl{
ADD_VALUES} options {\em cannot} be mixed without intervening calls to
the assembly routines.  For such intermediate assembly calls the
second routine argument  typically should be \trl{MAT_FLUSH_ASSEMBLY},
\findex{MAT_FLUSH_ASSEMBLY} which omits some of the work of the full 
assembly process.  \trl{MAT_FINAL_ASSEMBLY} \findex{MAT_FINAL_ASSEMBLY} is
required only in the last matrix assembly before a matrix is used.

Even though one may insert values into PETSc matrices without regard
to which processor eventually stores them, for efficiency
reasons we usually recommend generating most entries on the
processor where they are destined to be stored.  To help the
application programmer with this task for matrices that are
distributed across the processors by ranges, the routine
\findex{MatGetOwnershipRange()}
\begin{verse}
  MatGetOwnershipRange(Mat A,int *first\_row,int *last\_row);
\end{verse}
informs the user that all rows from \trl{first_row} to 
\trl{last_row-1} will be stored on the local processor.

In the sparse matrix implementations, once the assembly routines have been 
called, the matrices are compressed and can be used for matrix-vector
multiplication, etc.
Inserting new values into the matrix at this point will be expensive, 
since it requires copies and possible memory allocation. Thus, whenever 
possible one should completely set the values in the matrices before 
calling the final assembly routines. 

If one wishes to repeatedly assemble matrices that retain the same
nonzero pattern (such as within a nonlinear or time-dependent
problem), the option
\begin{verse}
  MatSetOption(Mat mat,MAT\_NO\_NEW\_NONZERO\_LOCATIONS);
\end{verse}
should be specified after the first matrix has been fully assembled.
This option ensures that certain data structures and communication
information will be reused (instead of regenerated) during successive
steps, thereby increasing efficiency.  
See \trl{${PETSC_DIR}/src/sles/examples/tutorials/ex5.c} for a simple example of
solving two linear systems that use the same matrix data structure.

\subsection{Sparse Matrices}
\label{sec:matsparse}

\sindex{AIJ matrix format} \sindex{CSR, compressed sparse row format}
The default matrix representation within PETSc is the general sparse
AIJ format (also called the Yale sparse matrix format or compressed
sparse row format, CSR).  This section discusses tips for {\em
efficiently} using this matrix format for large-scale
applications. Additional formats (such as block compressed row and
block diagonal storage, which are generally much more efficient for
problems with multiple degrees of freedom per node) are discussed
below.  Beginning users need not concern themselves initially with
such details and may wish to proceed directly to
Section~\ref{sec:matoptions}.  However, when an application code
progresses to the point of tuning for efficiency and/or generating
timing results, it is {\em crucial} to read this information.

\subsubsection{Sequential AIJ Sparse Matrices}

In the PETSc AIJ matrix formats, we store the nonzero elements
by rows, along with an array of corresponding column numbers and
an array of pointers to the beginning of each row.  Note that the
diagonal matrix entries are stored with the rest of the nonzeros (not
separately). 

To create a sequential AIJ sparse matrix, \trl{A}, 
\findex{MatCreateSeqAIJ()} with \trl{m} rows and \trl{n} columns,
one uses the command
\begin{verse}
  MatCreateSeqAIJ(PETSC\_COMM\_SELF,int m,int n,int nz,int *nnz,Mat *A);
\end{verse}
where \trl{nz} or \trl{nnz} can be used to preallocate matrix memory,
as discussed below. The user can set \trl{nz=0} and \trl{
nnz=PETSC_NULL} for PETSc to control all matrix memory allocation.

The sequential and parallel AIJ matrix storage formats by default
employ {\em i-nodes} (identical nodes) when possible.  We search for
consecutive rows with the same nonzero structure, thereby reusing
matrix information for increased efficiency.  Related options database
keys are \trl{-mat_aij_no_inode} (do not use inodes) and \trl{
-mat_aij_inode_limit <limit>} (set inode limit (max limit=5)).
Note that problems with a single degree of freedom per grid node
will automatically not use I-nodes.

By default the internal data representation for the AIJ formats employs
zero-based indexing.  For compatibility with standard Fortran storage,
thus enabling use of external Fortran software packages such as
SPARSKIT, \sindex{SPARSKIT} the option \trl{-mat_aij_oneindex}
\findex{-mat_aij_oneindex} enables one-based indexing, where the stored
row and column indices begin at one, not zero.  All user calls to
PETSc routines, regardless of this option, use zero-based indexing.

\subsubsection{Preallocation of Memory for Sequential AIJ Sparse Matrices}

The dynamic process of allocating new memory and copying from the old
storage to the new is {\em intrinsically very expensive}.  Thus, to
obtain good performance when assembling an AIJ matrix, it is crucial
to preallocate the memory needed for the sparse matrix.  The user has
two choices for preallocating matrix memory via \trl{
MatCreateSeqAIJ()}. 

One can use the scalar \trl{nz} to specify the expected
number of nonzeros for each row.  This is generally fine if the number
of nonzeros per row is roughly the same throughout the matrix (or as a
quick and easy first step for preallocation).  If one underestimates
the actual number of nonzeros in a given row, then during the assembly
process PETSc will automatically allocate additional needed space.
However, this extra memory allocation can slow the computation,

If different rows have very different numbers of nonzeros, one
should attempt to indicate (nearly) the exact number of elements
intended for the various rows with the optional array, \trl{nnz} of
length \trl{m}, where \trl{m} is the number of rows, for example
\begin{verse}
   int nnz[m];\\
   nnz[0] = <nonzeros in row 0>\\
   nnz[1] = <nonzeros in row 1>\\
   ....\\
   nnz[m-1] = <nonzeros in row m-1>
\end{verse}
In this case, the assembly process will require no additional memory
allocations if the \trl{nnz} estimates are correct. If, however,
the \trl{nnz} estimates are incorrect, PETSc will automatically
obtain the additional needed space, at a slight loss of efficiency.

Using the array \trl{nnz} to preallocate memory is especially
important for efficient matrix assembly if the number of nonzeros
varies considerably among the rows.  One can generally set \trl{nnz}
either by knowing in advance the problem structure (e.g., the stencil
for finite difference problems on a structured grid) or by
precomputing the information by using a segment of code similar to
that for the regular matrix assembly.  The overhead of determining the
\trl{nnz} array will be quite small compared with the overhead of the
inherently expensive mallocs and moves of data that are needed for
dynamic allocation during matrix assembly.

Thus, when assembling a sparse matrix with very different
numbers of nonzeros in various rows, one could proceed 
as follows for finite difference methods:
\begin{tabbing}
12\=12\= \kill
    \> - Allocate integer array \trl{nnz}.\\
    \> - Loop over grid, counting the expected number of nonzeros for the row(s)\\
    \>\>  associated with the various grid points.\\
    \> - Create the sparse matrix via \trl{MatCreateSeqAIJ()} or alternative.\\
    \> - Loop over the grid, generating matrix entries and inserting 
      in matrix via \trl{MatSetValues()}.\\
  \end{tabbing}
\vspace{-0.2in}
For (vertex-based) finite element type calculations, an analogous procedure is as follows:
  \begin{tabbing}
12\=12\= \kill
    \> - Allocate integer array \trl{nnz}.\\
    \>- Loop over vertices, computing the number of neighbor vertices, which determines the\\
    \>\> number of nonzeros for the corresponding matrix row(s).\\
    \> - Create the sparse matrix via \trl{MatCreateSeqAIJ()} or alternative.\\
    \> - Loop over elements, generating matrix entries and inserting
      in matrix via \trl{MatSetValues()}.\\
  \end{tabbing}

The \trl{-log_info} \findex{-log_info} option causes the routines
\trl{MatAssemblyBegin()} and \trl{MatAssemblyEnd()} to print
information about the success of the preallocation.  Consider the
following example for the \trl{MATSEQAIJ} matrix format:
\begin{verse}
   MatAssemblyEnd\_SeqAIJ:Matrix size 10 X 10; storage space:20 unneeded, 100 used\\
   MatAssemblyEnd\_SeqAIJ:Number of mallocs during MatSetValues is 0
\end{verse}
The first line indicates that the user preallocated 3000 spaces but only
1000 were used. The second line indicates that the user preallocated
enough space so that PETSc did not have to internally allocate additional
space (an expensive operation).  In the next example the user did not
preallocate sufficient space, as indicated by the fact that the number
of mallocs is very large (bad for efficiency):
\begin{verse}
   MatAssemblyEnd\_SeqAIJ:Matrix size 10 X 10; storage space:47 unneeded, 1000 used\\
   MatAssemblyEnd\_SeqAIJ:Number of mallocs during MatSetValues is 40000
\end{verse}
 
Although at first glance such procedures for determining the matrix
structure in advance may seem unusual, they are actually very
efficient because they alleviate the need for dynamic
construction of the matrix data structure, which can be very
expensive.


\subsubsection{Parallel AIJ Sparse Matrices}

Parallel sparse matrices with the AIJ 
format can be created with the command \findex{MatCreateMPIAIJ()}
\begin{verse}
  MatCreateMPIAIJ(MPI\_Comm comm,int m,int n,int M,int N,int d\_nz,\\
                          int *d\_nnz, int o\_nz,int *o\_nnz,Mat *A);
\end{verse}
\trl{A} is the newly created matrix, while the arguments \trl{m}, \trl{n}, 
\trl{M}, and \trl{N}, indicate the number of local rows and columns and
the number of global rows and columns, respectively. Either the local or
global parameters can be replaced with \trl{PETSC_DECIDE}, so that 
PETSc will determine \findex{PETSC_DECIDE} them.
The matrix is stored with a fixed number of rows on 
each processor, given by \trl{m}, or determined by PETSc if \trl{m} is
\trl{PETSC_DECIDE}.

If \trl{PETSC_DECIDE} is not used for the arguments
\trl{m} and \trl{n}, then the user must ensure that they are chosen to be
compatible with the vectors. To do this, one first considers the matrix-vector product 
$y = A x$. The \trl{m} that is used in the matrix creation routine \trl{MatCreateMPIAIJ()}
must match the local size used in the vector creation routine \trl{VecCreateMPI()} for \trl{y}.
Likewise, the \trl{n} used must match that used as the local size in 
\trl{VecCreateMPI()} for \trl{x}. 

The user must set \trl{d_nz=0}, \trl{o_nz=0}, \trl{d_nnz=PETSC_NULL}, and 
\trl{o_nnz=PETSC_NULL} for PETSc to control dynamic allocation of matrix
memory space.  Analogous to \trl{nz} and \trl{nnz} for the routine 
\trl{MatCreateSeqAIJ()}, these arguments optionally specify 
nonzero information for the diagonal (\trl{d_nz} and \trl{d_nnz}) and 
off-diagonal (\trl{o_nz} and \trl{o_nnz}) parts of the matrix. 
For a square global matrix, we define each processor's diagonal portion 
to be its local rows and the corresponding columns (a square submatrix);  
each processor's off-diagonal portion encompasses the remainder of the
local matrix (a rectangular submatrix).  
The rank in the MPI communicator determines the absolute ordering of the
blocks.  That is, the process with rank 0 in the communicator given to \trl{
  MatCreateMPIAIJ} contains the top rows of the matrix; the i$^{th}$ process
in that communicator contains the i$^{th}$ block of the matrix.

\subsubsection{Preallocation of Memory for Parallel AIJ Sparse Matrices}

As discussed above, preallocation of memory is critical for achieving good
performance during matrix assembly, as this reduces the number of
allocations and copies required.  We present an example for
three processors to indicate how this may be done for the \trl{MATMPIAIJ}
matrix format.  Consider the 8 by
8 matrix, which is partitioned by default with three rows on the first
processor, three on the second and two on the third.  {\small
\[
\left( \begin{array}{cccccccccc} 
1  & 2  & 0  & | & 0  & 3  & 0  & |  & 0  & 4  \\
0  & 5  & 6  & | & 7  & 0  & 0  & |  & 8  & 0 \\
9  & 0  & 10 & | & 11 & 0  & 0  & |  & 12 & 0  \\
\hline \\
13 & 0  & 14 & | & 15 & 16 & 17 & |  & 0  & 0  \\
0  & 18 & 0  & | & 19 & 20 & 21 & |  & 0  & 0 \\
0  & 0  & 0  & | & 22 & 23 & 0  & |  & 24 & 0 \\
\hline \\
25 & 26 & 27 & | & 0  & 0  & 28 & |  & 29 & 0 \\
30 & 0  & 0  & | & 31 & 32 & 33 & |  & 0  &34 
\end{array} \right)
\]
}

The ``diagonal'' submatrix, \trl{d}, on the first processor is given by 
{\small
\[
\left( \begin{array}{ccc} 
1  & 2  & 0  \\
0  & 5  & 6  \\
9  & 0  & 10 
\end{array} \right),
\]
}
while the ``off-diagonal'' submatrix, \trl{o}, matrix is given by 
{\small
\[
\left( \begin{array}{ccccc} 
 0  & 3  & 0   & 0  & 4  \\
 7  & 0  & 0   & 8  & 0  \\
 11 & 0  & 0   & 12 & 0  \\
\end{array} \right).
\]
}
For the first processor one could set \trl{d_nz} to 2 (since each
row has 2 nonzeros) or, alternatively, set \trl{d_nnz} to \{2,2,2\}.
The \trl{o_nz} could be set to 2 since each row of the \trl{o} matrix
has 2 nonzeros, or \trl{o_nnz} could be set to \{2,2,2\}.

For the second processor the \trl{d} submatrix is given by 
{\small
\[
\left( \begin{array}{cccccccccc} 
 15 & 16 & 17 \\
 19 & 20 & 21 \\
 22 & 23 & 0  
\end{array} \right) .
\]
}
Thus, one could set \trl{d_nz} to 3, since the maximum number of
nonzeros in each row is 3, or alternatively one could set \trl{d_nnz} to
\{3,3,2\}, thereby indicating that the first two rows will have 3
nonzeros while the third has 2. The corresponding \trl{o} submatrix for the
second processor is
{\small
\[
\left( \begin{array}{cccccccccc} 
13 & 0  & 14 &  0  & 0  \\
0  & 18 & 0  &  0  & 0 \\
0  & 0  & 0  &  24 & 0 \\
\end{array} \right)
\]
}
so that one could set \trl{o_nz} to 2 or \trl{o_nnz} to \{2,1,1\}.

Note that the user never directly works with the \trl{d} and \trl{o}
submatrices, except when preallocating storage space as indicated above.
Also, the user need not preallocate exactly the correct amount of
space; as long as a sufficiently close estimate is given, the high
efficiency for matrix assembly will remain.  

As described above, the option \trl{-log_info} \findex{-log_info}
will print information about the success of preallocation during
matrix assembly.  For the \trl{MATMPIAIJ} format, PETSc will also list
the number of elements owned by on each processor that were generated
on a different processor.  For example, the statements
\begin{verse}
   \[0\]MatAssemblyBegin\_MPIAIJ:Number of off processor values 10\\
   \[1\]MatAssemblyBegin\_MPIAIJ:Number of off processor values 7\\
   \[2\]MatAssemblyBegin\_MPIAIJ:Number of off processor values 5
\end{verse}
indicate that very few values have been generated on different processors.
On the other hand, the statements
\begin{verse}
   \[0\]MatAssemblyBegin\_MPIAIJ:Number of off processor values 100000\\
   \[1\]MatAssemblyBegin\_MPIAIJ:Number of off processor values 77777
\end{verse}
indicate that many values have been generated on the ``wrong'' processors.
This situation can be very inefficient, since the transfer of values
to the ``correct'' processor is generally expensive.  By using the command
\trl{MatGetOwnershipRange()} in application codes, the user should be able
to generate most entries on the owning processor.

{\em Note}: It is fine to generate some entries on the ``wrong'' processor. Often
this can lead to cleaner, simpler, less buggy codes.  One should never
make code overly complicated in order to generate all values locally. Rather,
one should organize the code in such a way that {\em most} values are generated locally.

\subsection{Dense Matrices}
\label{sec:matdense}

PETSc provides both sequential and parallel dense matrix formats,
where each processor stores its entries in a column-major array in the
usual Fortran style.  To create a sequential, dense PETSc matrix,
\trl{A} of dimensions \trl{m} by \trl{n}, the user should
call \findex{MatCreateSeqDense()}
\begin{verse}
  MatCreateSeqDense(PETSC\_COMM\_SELF,int m,int n,PetscScalar *data,Mat *A);
\end{verse}
The variable \trl{data} enables the user to optionally provide the
location of the data for matrix storage (intended for Fortran users who
wish to allocate their own storage space).  Most users should merely
set \trl{data} to \trl{PETSC_NULL} for PETSc to control matrix memory allocation.
To create a parallel, dense matrix, \trl{A}, the user should call
\begin{verse}
  MatCreateMPIDense(MPI\_Comm comm,int m,int n,int M,int N,PetscScalar *data,Mat *A)
\end{verse}
The arguments \trl{m}, \trl{n}, 
\trl{M}, and \trl{N}, indicate the number of local rows and columns and
the number of global rows and columns, respectively. Either the local or
global parameters can be replaced with \trl{PETSC_DECIDE}, so that 
PETSc will determine \findex{PETSC_DECIDE} them.
The matrix is stored with a fixed number of rows on 
each processor, given by \trl{m}, or determined by PETSc if \trl{m} is
\trl{PETSC_DECIDE}. 

PETSc does not currently provide parallel dense direct solvers. Our focus is on 
sparse iterative solvers.

\section{Basic Matrix Operations}
\label{sec:matoptions}

Table \ref{fig:matrixops} summarizes basic PETSc matrix operations.
We briefly discuss a few of these routines in more detail below.

The parallel matrix can multiply a vector with \trl{n} 
local entries, returning a vector with \trl{m} local entries. That is, 
to form the product \findex{MatMult()}
\begin{verse}
  MatMult(Mat A,Vec x,Vec y);
\end{verse}
the vectors \trl{x} and \trl{y} should be generated with 
\begin{verse}
  VecCreateMPI(MPI\_Comm comm,n,N,\&x);\\
  VecCreateMPI(MPI\_Comm comm,m,M,\&y);
\end{verse}
By default, if the user lets PETSc decide the number of components to
be stored locally (by passing in \trl{PETSC_DECIDE} as the second
argument to \trl{VecCreateMPI()} or using \trl{VecCreate()}), vectors
and matrices of the same dimension are automatically compatible for
parallel matrix-vector operations.

Along with the matrix-vector multiplication routine, there is 
a version for the transpose of the matrix, \findex{MatMultTranspose()}
\begin{verse}
  MatMultTranspose(Mat A,Vec x,Vec y);
\end{verse}
There are also versions that add the result
to another vector: \findex{MatMultAdd()} \findex{MatMultTransposeAdd()}
\begin{verse}
  MatMultAdd(Mat A,Vec x,Vec y,Vec w);\\
  MatMultTransposeAdd(Mat A,Vec x,Vec y,Vec w);
\end{verse}
These routines, respectively, produce $ w = A*x + y $ and $ w = A^{T}*x + y$ . 
In C it is legal for the vectors \trl{y} and \trl{w} to be identical.
In Fortran, this situation is forbidden by the language standard, 
but we allow it anyway.

One can print a matrix (sequential or parallel) to the screen with the 
command \findex{MatView()}
\begin{verse}
  MatView(Mat mat,PETSC\_VIEWER\_STDOUT\_WORLD);
\end{verse}
Other viewers can be used as well. For instance, one can draw the
nonzero stucture of the matrix into the default X-window with the 
command 
\begin{verse}
  MatView(Mat mat,PETSC\_VIEWER\_DRAW\_WORLD);
\end{verse}
Use \findex{PETSC_VIEWER_DRAW_WORLD}
\begin{verse}
  MatView(Mat mat,PetscViewer viewer);
\end{verse}
where \trl{viewer} was obtained with \trl{PetscViewerDrawOpenX()}.
\findex{PetscViewerDrawOpenX} 
Additional viewers \findex{MatCopy()} and options are given in the \trl{MatView()} man
page and Section~\ref{sec:viewers}.

\begin{table}[H]
\begin{center}
\begin{tabular}{ll}
{\bf Function Name} & {\bf Operation} \\
\hline
MatAXPY(PetscScalar *a,Mat X, Mat Y,MatStructure); & $ Y = Y + a*X $ \\
MatMult(Mat A,Vec x, Vec y); & $ y = A*x $ \\
MatMultAdd(Mat A,Vec x, Vec y,Vec z); & $ z = y + A*x $ \\
MatMultTranspose(Mat A,Vec x, Vec y); & $ y = A^{T}*x $ \\
MatMultTransposeAdd(Mat A,Vec x, Vec y,Vec z); & $ z = y + A^{T}*x $ \\
MatNorm(Mat A,NormType type,  double *r); & $ r = ||A||_{type}$ \\
MatDiagonalScale(Mat A,Vec l,Vec r); & $ A = \hbox{diag}(l)*A*\hbox{diag}(r) $ \\
MatScale(PetscScalar *a,Mat A); & $ A = a*A $ \\
MatConvert(Mat A,MatType type,Mat *B); & $ B = A $ \\
MatCopy(Mat A,Mat B,MatStructure); &  $ B = A $ \\
MatGetDiagonal(Mat A,Vec x); & $ x = \hbox{diag}(A)$ \\
MatTranspose(Mat A,Mat* B); & $ B = A^{T} $ \\
MatZeroEntries(Mat A); & $ A = 0 $ \\
MatShift(PetscScalar *a,Mat Y); & $ Y =  Y + a*I $ \\
\hline 
\end{tabular}
\end{center}
\caption{\hbox{PETSc Matrix Operations}}
\label{fig:matrixops}
\end{table}
The \trl{NormType} argument to \trl{MatNorm()} is one of \findex{MatNorm()}
\findex{NormType} \trl{NORM_1} or \trl{NORM_INFINITY}.
\findex{NORM_1}  \findex{NORM_INFINITY}
\sindex{1-norm} \sindex{infinity norm}

\section{Matrix-Free Matrices} \sindex{matrix-free methods}

\label{sec:matrixfree}
Some people like to use matrix-free methods, which do not require
explicit storage of the matrix, for the numerical solution of partial
differential equations.  To support matrix-free methods in PETSc, one
can use the following command to create a \trl{Mat} structure without
ever actually generating the matrix:
\findex{MatCreateShell()}
\begin{verse}
  MatCreateShell(MPI\_Comm comm,int m,int n,int M,int N,void *ctx,Mat *mat);
\end{verse}
Here \trl{M} and \trl{N} are the global matrix dimensions (rows and
columns), \trl{m} and \trl{n} are the local matrix dimensions, and
\trl{ctx} is a pointer to data needed by any user-defined shell matrix
operations; the manual page has additional details about these
parameters.  Most matrix-free algorithms require only the application
of the linear operator to a vector. To provide this action, the user
must write a routine with the calling sequence
\begin{verse}
  UserMult(Mat mat,Vec x,Vec y);
\end{verse}
and then associate it with the matrix, \trl{mat}, by using the 
command \findex{MatShellSetOperation()}
\begin{verse}
  MatShellSetOperation(Mat mat,MatOperation MATOP\_MULT,\\
                               (void(*)(void)) int (*UserMult)(Mat,Vec,Vec));
\end{verse}
Here \trl{MATOP_MULT} is the name of the operation for matrix-vector
multiplication. Within each user-defined routine (such as
\trl{UserMult()}), the user should call \findex{MatShellGetContext()}
\trl{MatShellGetContext()} to obtain the user-defined context, \trl{ctx},
that was set by \trl{MatCreateShell()}.
This shell matrix can be used with the iterative linear
equation solvers discussed in the following chapters.

The routine \trl{MatShellSetOperation()} can be used to set any other
matrix operations as well.  The file 
\trl{${PETSC_DIR}/include/petscmat.h} provides a complete list of matrix
operations, which have the form \trl{MATOP_<OPERATION>}, where \trl{
<OPERATION>} is the name (in all capital letters) of the user
interface routine (for example, \trl{MatMult()} $ \rightarrow $ \trl{MATOP_MULT}).  All
user-provided functions have the same calling sequence as the
usual matrix interface routines, since the user-defined functions are
intended to be accessed through interface, e.g., 
\trl{MatMult(Mat,Vec,Vec)} $ \rightarrow$ \trl{UserMult(Mat,Vec,Vec)}.
The final argument for \trl{MatShellSetOperation()} needs to be cast
to a \trl{void *}, since the final argument could (depending on the 
\trl{MatOperation} be a variety of different functions.

Note that \trl{MatShellSetOperation()} can also be used as a
``backdoor'' means of introducing user-defined changes in matrix
operations for other storage formats (for example, to override the
default LU factorization routine supplied within PETSc for the
\trl{MATSEQAIJ} format).  However, we urge anyone who introduces such
changes to use caution, since it would be very easy to
accidentally create a bug in the new routine that could affect
other routines as well.

See also Section~\ref{sec:nlmatrixfree} for details on one set of
helpful utilities for using the matrix-free approach for nonlinear
solvers.

\section{Other Matrix Operations}
\label{sec:othermat}

In many iterative calculations (for instance, in a nonlinear equations
solver), it is important for efficiency purposes to reuse the nonzero 
structure of a matrix, rather than determining it anew every time 
the matrix is generated.  To retain a given matrix but reinitialize
its contents, one can employ \findex{MatZeroEntries()}
\begin{verse}
  MatZeroEntries(Mat A);
\end{verse}
This routine will zero the matrix entries in the 
data structure but keep all the data that indicates where the nonzeros
are located.  In this way a new matrix assembly will be much less 
expensive, since no memory allocations or copies will be needed. 
Of course, one can also explicitly set selected matrix elements to zero
by calling \trl{MatSetValues()}.

% CANNOT BE USED SINCE NOT GOOD FOR BLOCKSOLVE
%By default, if new entries are made in locations where no nonzeros 
%previously existed, space will be allocated for the new entries. 
%To prevent the allocation of additional memory and simply discard those 
%new entries, one can use the option \findex{MatSetOption()}
%\begin{verse}
%  MatSetOption(Mat A,MAT_NO_NEW_NONZERO_LOCATIONS);
%\end{verse}
%Once the matrix has been assembled, one can factor it numerically
%without repeating the ordering or the symbolic factorization. 
%This option can save some computational time, although it
%does require that the factorization is not done in-place.

In the numerical solution of elliptic partial differential equations,
it can be cumbersome to deal with Dirichlet boundary 
\sindex{boundary conditions} conditions. In
particular, one would like to assemble the matrix without regard to 
boundary conditions and then at the end apply the Dirichlet boundary 
conditions. 
In numerical analysis classes this process is usually presented as moving the 
known boundary conditions to the right-hand side and then solving a smaller
linear system for the interior unknowns. Unfortunately, implementing this
requires extracting a large submatrix from the original matrix and 
creating its corresponding data structures. This process can be expensive 
in terms of both time and memory. 

One simple way to deal with this difficulty is to replace those rows in the 
matrix associated with known boundary conditions, by rows of the 
identity matrix (or some scaling of it). This action can be done with 
the command \findex{MatZeroRows()}
\begin{verse}
  MatZeroRows(Mat A,IS rows,PetscScalar *diag\_value);
\end{verse}
For sparse matrices this removes the data structures for certain rows 
of the matrix. If the pointer \trl{diag_value} is \trl{PETSC_NULL}, it 
even removes the diagonal entry. If the pointer is not null, it uses that 
given value at the pointer location 
in the diagonal entry of the eliminated rows. 

% CANNOT BE USED SINCE NOT GOOD FOR BLOCKSOLVE
%One nice feature of this approach is that when solving a nonlinear problem 
%such that at each iteration the Dirichlet boundary conditions are in the 
%same positions and the matrix retains the same nonzero structure, the user 
%can call \trl{MatZeroRows()} in the first iteration. Then, before generating 
%the matrix in the second iteration the user should call
%\begin{verse}
%  MatSetOption(Mat A,MAT_NO_NEW_NONZERO_LOCATIONS);
%\end{verse}
%From that point, \findex{MatSetOption()}
%no new values will be inserted into those (boundary) rows of 
%the matrix. \findex{MAT_NO_NEW_NONZERO_LOCATIONS} 

Another matrix routine of interest is \findex{MatConvert()}
\begin{verse}
  MatConvert(Mat mat,MatType newtype,Mat *M)
\end{verse}
which converts the matrix \trl{mat} to new matrix, \trl{M}, that has
either the same or different format.  Set \trl{newtype} to \trl{MATSAME}
to copy the matrix, keeping the same matrix format.  See 
\trl{${PETSC_DIR}/include/petscmat.h} for other available matrix types;
standard ones are \trl{MATSEQDENSE}, \trl{MATSEQAIJ},   \trl{MATMPIAIJ},  
               \trl{MATMPIROWBS}, \trl{MATSEQBDIAG}, \trl{MATMPIBDIAG},  \trl{MATSEQBAIJ}, and
               \trl{MATMPIBAIJ}.

In certain applications it may be necessary for application codes
to directly access elements of a matrix. This may be done by using the 
the command 
\begin{verse}
  MatGetRow(Mat A,int row, int *ncols,int **cols,PetscScalar **vals);
\end{verse}
The argument \trl{ncols} returns the number of nonzeros in that row, 
while \trl{cols} and \trl{vals} returns the column indices (with indices
starting at zero) and values in the row. If only the column 
indices are needed (and not the corresponding matrix elements), one
can use \trl{PETSC_NULL} for the \trl{vals} argument. Similarly,
one can use \trl{PETSC_NULL} for the \trl{cols} argument.
The user can only examine the values extracted with \trl{MatGetRow()};
the values {\em cannot} be altered. 
\findex{MatGetRow()} \findex{MatRestoreRow()}
To change the matrix entries, one must use \trl{MatSetValues()}.

Once the user has finished using a row, he or she {\em must} call 
\begin{verse}
  MatRestoreRow(Mat A,int row,int *ncols,int **cols,PetscScalar **vals);
\end{verse}
to free any space that was allocated during the call to \trl{MatGetRow()}.

% ------------------------------------------------------------------
\section{Partitioning}
\label{sec:partitioning} \sindex{partitioning} \sindex{grid partitioning}

For almost all unstructured grid computation, the distribution of portions of 
the grid across the processor's work load and memory can have a very large
impact on performance. In most PDE calculations the grid partitioning and 
distribution across the processors can (and should) be done in a ``pre-processing'' step
before the numerical computations. However, this does not mean it need be done
in a separate, sequential program, rather it should be done before one sets up the 
parallel grid data structures in the actual program. PETSc provides an interface to
the ParMETIS (developed by George Karypis; see the docs/installation/index.htm file
for directions on installing PETSc to use ParMETIS), to allow the partitioning to be done in
parallel. PETSc does not currently provide directly support for dynamic
repartitioning, load balancing by migrating matrix entries between processors, etc.
For problems that require mesh refinement, PETSc uses the ``rebuild the data structure''
approach, as opposed to the ``maintain dynamic data structures that support the 
insertion/deletion of additional vector and matrix rows and columns entries'' approach.

Partitioning in PETSc is organized around the \trl{MatPartitioning} object. \findex{MatPartitioning}
One first creates a parallel matrix that contains the connectivity information about the 
grid (or other graph-type object) that is to be partitioned. This is done with the 
command
\begin{verse}
 MatCreateMPIAdj(MPI\_Comm comm,int mlocal,int n,int *ia,int *ja,\\
                         int *weights,Mat *Adj);
\end{verse}
The argument \trl{mlocal} indicates the number of rows of the graph being provided
by the given processor, \trl{n} is the total number of columns; equal to the 
sum of all the \trl{mlocal}. The arguments \trl{ia} and \trl{ja} are the row pointers
and column pointers for the given rows, these are the usual format for parallel 
compressed sparse row storage, using indices starting at 0, {\bf not} 1.

\begin{figure}[H]
\centerline{ \includegraphics{usg.pdf}}
%\centerline{\psfig{file=usg.ps,angle=0}}
\caption{Numbering on Simple Unstructured Grid}
\label{fig:usg}
\end{figure}

This, of course, assumes that one has already distributed the grid (graph) information
among the processors. The details of this initial distribution is not important; it 
could be simply determined by assigning to the first processor the first $n_0$ nodes
from a file, the second processor the next $ n_1$ nodes, etc.

For example, we demonstrate the form of the \trl{ia} and \trl{ja} for a triangular
grid where we 

(1) partition by element (triangle) 
\begin{itemize}
  \item Processor 0, $ mlocal = 2, n = 4, ja = \{2, 3,| 3\}, ia = \{0, 2, 3\} $
  \item Processor 1, $ mlocal = 2, n = 4, ja = \{0, | 0, 1\}, ia = \{0, 1, 3\} $
\end{itemize}
Note that elements are not connected to themselves and we only indicate edge connections
(in some contexts single vertex connections between elements may also be included). We use a
$ | $ above to denote the transition between rows in the matrix.

and (2) partition by vertex.
\begin{itemize}
  \item Processor 0, $ mlocal = 3, n = 6, ja = \{3, 4, | 4, 5, | 3, 4, 5\}, ia = \{0, 2, 4, 7\} $
  \item Processor 1, $ mlocal = 3, n = 6, ja = \{0, 2, 4, |  0, 1, 2, 3, 5,|  1, 2, 4\}, ia = \{0, 3, 8, 11\}. $
\end{itemize}



Once the connectivity matrix has been created the following code will generate the 
renumbering required for the new partition
\begin{verse}
 MatPartitioningCreate(MPI\_Comm comm,MatPartitioning *part);\\
 MatPartitioningSetAdjacency(MatPartitioning part,Mat Adj);\\
 MatPartitioningSetFromOptions(MatPartitioning part);\\
 MatPartitioningApply(MatPartitioning part,IS *is);\\
 MatPartitioningDestroy(MatPartitioning part); \\
 MatDestroy(Mat Adj);\\
 ISPartitioningToNumbering(IS is,IS *isg);
\end{verse}
The \findex{MatPartitioningCreate()} \findex{MatPartitioningSetAdjacency()} 
\findex{MatPartitioningSetFromOptions()} \findex{MatPartitioningApply()} 
\findex{MatPartitioningDestroy()}  resulting \trl{isg} contains for each local node the new global
number of that node. The resulting \trl{is} contains the new processor number
that each local node has been assigned to.

Now that a new numbering of the nodes has been determined one must 
renumber all the nodes and migrate the grid information to the correct processor.
The command
\begin{verse}
 AOCreateBasicIS(isg,PETSC\_NULL,\&ao);
\end{verse}
generates, see Section \ref{sec:ao}, an AO object that can be used in conjunction with the
\trl{is} and \trl{gis} to move the relevant grid information to the correct processor
and renumber the nodes etc. 

PETSc does not currently provide tools that completely manage the migration and 
node renumbering, since it will be dependent on the particular data structure you 
use to store the grid information and the type of grid information that you need
for your application. We do plan to include more support for this in the future,
but designing the appropriate user interface and providing a scalable implementation
that can be used for a wide variety of different grids requires a great deal of time.
Thus we demonstrate how this may be managed for the model
grid depicted above using (1) element based partitioning and (2) a vertex based 
partitioning. 



 


% ------------------------------------------------------------------
\chapter{SLES: Linear Equations Solvers} \sindex{linear system solvers}
\label{ch:sles}

SLES is the heart of PETSc, because it provides uniform and efficient access 
to all of the package's linear system solvers, including parallel and sequential,
direct and iterative.
SLES is intended for solving nonsingular systems of the form
\begin{equation}
   A x = b,
\label{eq:Ax=b}
\end{equation}
where $ A$ denotes the matrix representation of a linear operator, $b$
is the right-hand-side vector, and $ x $ is the solution vector.  SLES
uses the same calling sequence for both direct and iterative solution
of a linear system.  In addition, particular solution techniques and
their associated options can be selected at runtime.

The combination of a Krylov subspace method and a preconditioner is at
the center of most modern numerical codes for the iterative solution of
linear systems.  See, for example, \cite{fgn} for an overview of the theory
of such methods.  SLES creates a simplified interface to the
lower-level KSP and PC modules within the PETSc package.  The KSP component, 
discussed in
Section~\ref{sec:ksp}, provides many popular Krylov
\sindex{Krylov subspace methods} subspace iterative methods;
the PC module, described in Section~\ref{sec:pc}, includes a
variety of preconditioners.  Although both  KSP and PC can be used
directly, users should employ the interface of SLES.

\section{Using SLES} 
\label{sec:usingsles}

To solve a linear system with SLES, one must first create a solver context 
with the command \findex{SLESCreate()}
\begin{verse}
  SLESCreate(MPI\_Comm comm,SLES *sles); 
\end{verse}
Here \trl{comm} is the MPI communicator, and \trl{sles} is the newly
formed solver context.
Before actually solving a linear system with SLES, the user must call 
the following routine to set the matrices associated with the linear
system: \findex{SLESSetOperators()}
\begin{verse}
  SLESSetOperators(SLES sles,Mat Amat,Mat Pmat,MatStructure flag);
\end{verse}
The argument \trl{Amat}, representing the matrix that defines the
linear system, is a symbolic place holder for any kind of matrix.  
In particular, SLES {\em does} support matrix-free methods. 
\sindex{matrix-free methods}
The routine \trl{MatCreateShell()} \findex{MatCreateShell()}
in Section~\ref{sec:matrixfree} provides further information regarding
matrix-free methods. 
Typically the {\em preconditioning matrix} (i.e., the matrix from which the
preconditioner is to be constructed), \trl{Pmat}, is the same as
the matrix that defines the linear system, \trl{Amat}; however,
occasionally these matrices differ (for instance, \sindex{preconditioning}
when a preconditioning matrix obtained from a high order method with 
that from a low order method).
The argument \trl{flag} can be used to eliminate unnecessary work when
repeatedly solving linear systems of the same size with the same 
preconditioning method; when solving just one linear system, this flag is
ignored.  The user can set \trl{flag} as follows:
\begin{itemize}
\item \trl{SAME_NONZERO_PATTERN} - the preconditioning matrix has the
    same \findex{SAME_NONZERO_PATTERN} nonzero structure during successive
    linear solves,
\item \trl{DIFFERENT_NONZERO_PATTERN} - the preconditioning matrix does
     not have the same nonzero structure during successive linear solves,
   \findex{DIFFERENT_NONZERO_PATTERN}
\item \trl{SAME_PRECONDITIONER} - the preconditioner matrix is identical
   to that of the previous linear solve. \findex{SAME_PRECONDITIONER}
\end{itemize}
If in doubt about the structure of a matrix, one should use
the flag \trl{DIFFERENT_NONZERO_PATTERN}.
 
Much of the power of SLES can be accessed through the single routine
\begin{verse}
  SLESSetFromOptions(SLES sles);
\end{verse}
This \findex{SLESSetFromOptions()}
routine accepts the options \trl{-h} and \trl{-help} as well as 
any of the KSP and PC options discussed below. 
To solve a linear system, one merely executes the command \findex{SLESSolve()}
\begin{verse}
  SLESSolve(SLES sles,Vec b,Vec x,int *its);
\end{verse}
where \trl{b} and \trl{x} respectively denote the right-hand-side and
solution vectors.  On return, the parameter \trl{its} contains
either the iteration number at which convergence was successfully
reached, or the {\em negative} of the iteration at which divergence or
breakdown was detected.  Section~\ref{section:convergencetests} gives
more details regarding convergence testing.
Note that multiple linear solves can be performed by the same SLES context.
Once the SLES context is no longer needed, it should be destroyed with the 
command \findex{SLESDestroy()}
\begin{verse}
  SLESDestroy(SLES sles);
\end{verse}

The above procedure is sufficient for general use of the SLES package.
One additional step is required for users who wish to customize certain 
preconditioners (e.g., see Section~\ref{sec:bjacobi}) or to log certain 
performance data using the PETSc profiling facilities (as discussed in 
Chapter~\ref{ch:profiling}).
In this case, the user can optionally explicitly call \findex{SLESSetUp()}
\begin{verse}
  SLESSetUp(SLES sles,Vec b,Vec x);
\end{verse}
before calling \trl{SLESSolve()} to perform any setup required for 
the linear solvers.  The explicit call of this routine enables the
separate monitoring of any computations performed during the set up
phase, such as incomplete factorization for the ILU preconditioner.

The default solver within SLES is restarted GMRES, preconditioned for
the uniprocessor case with ILU(0), and for the multiprocessor case
with the block Jacobi method (with one block per processor, each of
which is solved with ILU(0)). A variety of other solvers
and options are also available.
To allow application programmers to set any of the preconditioner or 
Krylov subspace options directly within the code, we provide routines
that extract the PC and KSP contexts, \findex{SLESGetPC()}
\begin{verse}
  SLESGetPC(SLES sles,PC *pc);\\
  SLESGetKSP(SLES sles,KSP *ksp);
\end{verse}
\findex{SLESGetKSP()}
The application programmer can then directly call any of the PC or KSP 
routines to modify the corresponding default options.   

To solve a linear system with a direct solver (currently supported 
only for sequential matrices) one may use the options
\trl{-pc_type} \trl{lu} \trl{-ksp_type} \trl{preonly} (see below).

By default, if a direct solver is used, the factorization is {\em not} done 
in-place. This approach is to prevent the user from the unexpected surprise
of having a corrupted matrix after a linear solve. The routine 
\trl{PCLUSetUseInPlace()}, discussed below, causes factorization to 
be done in-place. \findex{PCLUSetUseInPlace()} 

\section{Solving Successive Linear Systems}

When solving multiple linear systems of the same size with the same
method, several options are available.  To solve successive linear
systems having the {\em same} preconditioner matrix (i.e., the same
data structure with exactly the same matrix elements) but different
right-hand-side vectors, the user should simply call \trl{SLESSolve()}
multiple times.  The preconditioner setup operations (e.g.,
factorization for ILU) will be done during the first call to \trl{
SLESSolve()} only; such operations will {\em not} be repeated for
successive solves.

To solve successive linear systems that have {\em different}
preconditioner matrices (i.e., the matrix elements and/or the matrix
data structure change), the user {\em must} call 
\trl{SLESSetOperators()} and \trl{SLESSolve()} for each solve.  See
Section~\ref{sec:usingsles} for a description of various flags for
\trl{SLESSetOperators()} that can save work for such cases.

\section{Krylov Methods}\sindex{Krylov subspace methods}
\label{sec:ksp}

The Krylov subspace methods accept a number of options, many of which 
are discussed below.  First, to set the Krylov subspace method that is to 
be used, one calls the command \findex{KSPSetType()}
\begin{verse}
  KSPSetType(KSP ksp,KSPType method);
\end{verse}
The type can be one of \trl{KSPRICHARDSON}, \trl{KSPCHEBYCHEV}, \trl{KSPCG}, \trl{KSPGMRES}, 
\trl{KSPTCQMR}, \trl{KSPBCGS}, \trl{KSPCGS}, \trl{KSPTFQMR}, \trl{KSPCR}, \trl{KSPLSQR}, \trl{KSPBICG}, or \trl{KSPPREONLY.}
\findex{KSPRICHARDSON} \findex{KSPCHEBYCHEV} \findex{KSPCG}
\findex{KSPGMRES} \findex{KSPTCQMR} \findex{KSPBCGS} \findex{KSPTFQMR}
\findex{KSPCR} \findex{KSPPREONLY} \findex{KSPBICG}
The KSP method can also be set with the options database command 
\trl{-ksp_type},
followed by one of the options \trl{richardson}, \trl{chebychev}, \trl{cg}, \trl{gmres}, \trl{tcqmr}, 
\trl{bcgs}, \trl{cgs}, \trl{tfqmr}, \trl{cr}, \trl{lsqr}, \trl{bicg}, or \trl{preonly.} \findex{-ksp_type}
There are method-specific options for the Richardson, Chebychev,
and GMRES \findex{KSPChebychevSetEigenvalues()} \findex{KSPGMRESSetRestart()}
methods. \findex{KSPRichardsonSetScale()}  \sindex{GMRES} \sindex{CG}
\begin{verse}
  KSPRichardsonSetScale(KSP ksp,double damping\_factor);\\
  KSPChebychevSetEigenvalues(KSP ksp,double emax,double emin);\\
  KSPGMRESSetRestart(KSP ksp,int max\_steps);
\end{verse}
The default parameter values are \trl{damping_factor=1.0, 
emax=0.01, emin=100.0}, and \trl{max_steps=30}. The GMRES 
\sindex{restart} restart and Richardson damping factor
can also be set with the options \trl{-ksp_gmres_restart <n>}
and \trl{-ksp_richardson_scale <factor>}. \findex{-ksp_gmres_restart} 
\findex{-ksp_richardson_scale}

The default technique for orthogonalization of the Hessenberg
matrix in GMRES is the iterative refinement Gram-Schmidt method.
\findex{KSPGMRESIROrthogonalization} This can be set  by using the command line option 
\trl{-ksp_gmres_irorthog}. \findex{-ksp_gmres_irorthog} Or
via 
\begin{verse}
  KSPGMRESSetOrthogonalization(KSP ksp,KSPGMRESModifiedGramSchmidtOrthogonalization);
\end{verse}
A slightly
faster approachis to use the 
unmodified (classical) Gram-Schmidt method, which can be set \sindex{Gram-Schmidt}
with \findex{KSPGMRESSetOrthogonalization()} 
\findex{KSPGMRESUnmodifiedGramSchmidtOrthogonalization}
\begin{verse}
  KSPGMRESSetOrthogonalization(KSP ksp,KSPGMRESUnmodifiedGramSchmidtOrthogonalization);
\end{verse}
or the options database \findex{-ksp_gmres_unmodifiedgramschmidt}
command \trl{-ksp_gmres_unmodifiedgramschmidt}.
Note that this algorithm is numerically unstable, but may deliver 
slightly better speed performance. One can also use 
modifed Gram-Schmidt, 
\findex{KSPGMRESModifiedGramSchmidtOrthogonalization} by setting the orthogonalization routine,
\trl{KSPGMRESModifiedGramSchmidtOrthogonalization()}, by using the command line option 
\trl{-ksp_gmres_modifiedgramschmidt}. \findex{-ksp_gmres_modifiedgramschmidt}

For the conjugate gradient method with complex numbers, there are two
slightly different algorithms depending on whether the matrix is 
Hermitian symmetric or truly symmetric (the default is to assume that
it is Hermitian symmetric). To indicate that it is symmetric, one uses the command
\findex{KSPCGSetType()} \findex{KSP_CG_SYMMETRIC} \findex{KSPCGType}
\sindex{Hermitian matrix} 
\begin{verse}
  KSPCGSetType(KSP ksp,KSPCGType KSP\_CG\_SYMMETRIC);
\end{verse}
Note that this option is not valid for all matrices.

The LSQR algorithm does not involve a preconditioner, any preconditioner
set to work with the KSP object is ignored if LSQR was selected.

By default, KSP assumes an initial guess of zero by zeroing the initial 
value for the solution vector that is given; this zeroing is done at the
call to \trl{SLESSolve()} (or \trl{KSPSolve()}). To use a nonzero 
initial guess, the user {\em must} call \findex{KSPSetInitialGuessNonzero()}
\begin{verse}
  KSPSetInitialGuessNonzero(KSP ksp,PetscTruth flg);
\end{verse}

\subsection{Preconditioning within KSP} 
\label{sec:ksppc}
\sindex{preconditioning}

Since the rate of convergence of Krylov projection methods for a
particular linear system is strongly dependent on its spectrum,
preconditioning is typically used to alter the spectrum and hence
accelerate the convergence rate of iterative techniques.
Preconditioning can be applied to the system (\ref{eq:Ax=b}) by
\begin{equation}
   (M_L^{-1} A M_R^{-1}) \, (M_R x) = M_L^{-1} b,
\label{eq:prec}
\end{equation}
where $ M_L$ and $ M_R $ indicate preconditioning matrices (or, matrices
from which the preconditioner is to be constructed).  If $ M_L = I $
in (\ref{eq:prec}), right preconditioning results, and the
residual of (\ref{eq:Ax=b}),
  \[ r \equiv b - Ax = b - A M_R^{-1} \, M_R x, \]
is preserved.  In contrast, the residual is altered for left 
($ M_R = I $) and symmetric preconditioning, as given by
  \[ r_L \equiv M_L^{-1} b - M_L^{-1} A x = M_L^{-1} r. \]
By default, all KSP implementations use left preconditioning.  
% Not really, KSPQCG uses symmetric preconditioning.
Right preconditioning can be activated for some methods by
using the options database command \trl{-ksp_right_pc} or
calling the routine \findex{-ksp_right_pc} \findex{PCSide}
\begin{verse}
  KSPSetPreconditionerSide(KSP ksp,PCSide PC\_RIGHT);
\end{verse}
Attempting to use right preconditioning for a method that
does not currently support it results in an error message of the form
\begin{verse}
   KSPSetUp\_Richardson:No right preconditioning for KSPRICHARDSON
\end{verse}

We summarize the defaults for the residuals used in KSP convergence
monitoring within Table~\ref{tab:kspdefaults}.  Details regarding
specific convergence tests and monitoring routines are presented in
the following sections.  The preconditioned residual is used by
default for convergence testing of all left-preconditioned KSP
methods. For the conjugate gradient, Richardson, and
Chebyshev methods the true residual can be used by
the options database command \trl{ksp_norm_type unpreconditioned } or by calling the routine
\begin{verse}
  KSPSetNormType(KSP ksp,KSP\_UNPRECONDITIONED\_NORM);
\end{verse}

\begin{table}
\begin{center}
\begin{tabular}{llll}
& & {\bf Options}       & {\bf Default}\\
& & {\bf Database}      & {\bf Convergence}\\
{\bf Method}    &{\bf KSPType}  & {\bf Name}    & {\bf Monitor $ \dagger $}\\
\hline
Richardson                                & KSPRICHARDSON & richardson  & true\\
Chebychev                                 & KSPCHEBYCHEV  & chebychev   & true\\
Conjugate Gradient \cite{hs:52}           & KSPCG         & cg          & true\\
BiConjugate Gradient                      & KSPBICG       & bicg        & true\\
Generalized Minimal Residual \cite{ss:86} & KSPGMRES      & gmres       & precond\\
BiCGSTAB \cite{v:92}                      & KSPBCGS       & bcgs        & precond \\
Conjugate Gradient Squared \cite{so:89}   & KSPCGS        & cgs         & precond \\
Transpose-Free Quasi-Minimal Residual (1) \cite{f:93} & KSPTFQMR & tfqmr & precond \\
Transpose-Free Quasi-Minimal Residual (2) & KSPTCQMR & tcqmr & precond \\
Conjugate Residual                        & KSPCR         & cr          & precond \\
Least Squares Method                      & KSPLSQR       & lsqr        & precond \\
Shell for no KSP method                   & KSPPREONLY    & preonly     & precond \\
\hline
\end{tabular}
\medskip \medskip
$ \dagger $ true - denotes true residual norm, precond - denotes preconditioned residual norm
\end{center}
\caption{KSP Defaults.  All methods use left preconditioning by default.}
\label{tab:kspdefaults}
\end{table}

Note: the bi-conjugate gradient method requires application of both the matrix and 
its transpose plus the preconditioner and its transpose. Currently no all matrices
and preconditioners provide this support and thus the \trl{KSPBICG} cannot always 
be used. \findex{KSPBICG} \sindex{Bi-conjugate gradient}

\subsection{Convergence Tests}
\label{section:convergencetests}

The default convergence test, \trl{KSPDefaultConverged()}, is 
based on the $l_2$-norm of the residual. Convergence 
(or divergence) is decided by three quantities:
the relative decrease of the residual norm, \trl{rtol}, the absolute 
size of the residual norm, \trl{atol}, and the relative increase in the 
residual, \trl{dtol}.  Convergence is detected at iteration $ k $ if
\[  \| r_k \|_2 < {\rm max} ( rtol * \| r_0 \|_2, atol), \]
where $r_k = b - A x_k$.  Divergence is detected if
\[  \| r_k \|_2 > dtol * \| r_0 \|_2. \]
These parameters, as well as the maximum number of allowable iterations, 
can be set with the routine \findex{KSPSetTolerances()}
\begin{verse}
  KSPSetTolerances(KSP ksp,double rtol,double atol,double dtol,int maxits);
\end{verse}
The user can retain the default value of any of these parameters by
specifying \trl{PETSC_DEFAULT} \findex{PETSC_DEFAULT} as the 
corresponding tolerance; the
defaults are \trl{rtol}=$10^{-5}$, \trl{atol}=$10^{-50}$,
\trl{dtol}=$10^{5}$, and \trl{maxits}=$10^5$.
These parameters can also be set from the options database with the 
commands \trl{-ksp_rtol} \trl{<rtol>}, \trl{-ksp_atol} \trl{<atol>}, \trl{-ksp_divtol} \trl{<dtol>},
\findex{-ksp_rtol} \findex{-ksp_atol} \findex{-ksp_divtol}
and \trl{-ksp_max_it} \trl{<its>}. \findex{-ksp_max_it}

In addition to providing an interface to a simple convergence test,
KSP allows the application programmer the flexibility to provide 
customized convergence-testing routines.  \sindex{convergence tests}
The user can specify a customized 
routine with the command \findex{KSPSetConvergenceTest()}
\begin{verse}
  KSPSetConvergenceTest(KSP ksp,int (*test)(KSP ksp,int it,double rnorm,\\
                               KSPConvergedReason *reason,void *ctx),void *ctx);
\end{verse}
The final routine argument, \trl{ctx}, is an optional context for private
data for the user-defined convergence routine, \trl{test}.  Other
\trl{test} routine arguments are the iteration
number, \trl{it}, and the residual's $ l_2 $ norm, \trl{rnorm}.
The routine for detecting convergence, \trl{test}, should set reason to 
positive for convergence, 0 for no convergence, and negative for 
failure to converge.  A list of possible \trl{KSPConvergedReason} is given
\findex{KSPConvergedReason} in \trl{include/petscksp.h}.
 
\subsection{Convergence Monitoring}
\label{sec:kspmonitor}

By default, the Krylov solvers run silently without displaying information 
about the iterations. The user can indicate that the norms of the residuals 
should be displayed by using \findex{-ksp_monitor}
\trl{-ksp_monitor} within the options database.  
To display the residual norms in a graphical window (running under X Windows),
one should use \trl{-ksp_xmonitor} \trl{[x,y,w,h]}, where either all or none of 
the options must be specified. \findex{-ksp_xmonitor}
Application programmers can also provide their own routines to perform 
the monitoring by using the command \findex{KSPSetMonitor()}
\begin{verse}
  KSPSetMonitor(KSP ksp,int (*mon)(KSP ksp,int it,double rnorm,void *ctx),\\
                        void *ctx,int (*mondestroy)(void *));
\end{verse}
The final routine argument, \trl{ctx}, is an optional context for private
data for the user-defined monitoring routine, \trl{mon}.  Other
\trl{mon} routine arguments are the iteration
number (\trl{it}) and the residual's $ l_2 $ norm (\trl{rnorm}).
A helpful routine within user-defined monitors is 
\trl{PetscObjectGetComm((PetscObject)ksp,MPI_Comm *comm)}, which returns
in \trl{comm} \findex{PetscObjectGetComm()} \sindex{communicator} the
MPI communicator for the \trl{KSP} context.  See section ~\ref{sec:writing}
for more discussion of the use of MPI communicators within PETSc.

Several monitoring routines are supplied with PETSc, 
including \findex{KSPDefaultMonitor()} \findex{KSPSingularValueMonitor()}
\begin{verse}
  KSPDefaultMonitor(KSP,int,double, void *);\\
  KSPSingularValueMonitor(KSP,int,double, void *);\\
  KSPTrueMonitor(KSP,int,double, void *);
\end{verse}
The default monitor simply prints an estimate of the $l_2$-norm of the 
residual at each iteration. The routine
\trl{KSPSingularValueMonitor()} is appropriate only for use with the conjugate 
gradient method or GMRES, since it prints estimates of the extreme singular 
values of the preconditioned operator at each iteration. Since
\trl{KSPTrueMonitor()} prints \findex{KSPTrueMonitor()}
the true residual at each iteration by 
actually computing the residual using the formula $ r = b - Ax $, the routine
is slow and should be used only for testing or convergence studies,
not for timing. These monitors may be accessed with the command line options
\trl{-ksp_monitor}, \trl{-ksp_singmonitor}, and \trl{-ksp_truemonitor}.
\findex{-ksp_monitor} \findex{-ksp_singmonitor} \findex{-ksp_truemonitor}.

To employ the default graphical monitor, one should use the 
commands \findex{KSPLGMonitorCreate()} \findex{KSPLGMonitorDestroy()}
\begin{verse}
   PetscDrawLG lg;\\
  KSPLGMonitorCreate(char *display,char *title,int x,int y,int w,int h,PetscDrawLG *lg);\\
  KSPSetMonitor(KSP ksp,KSPLGMonitor,lg,0);
\end{verse}
When no longer needed, the line graph should be destroyed 
with the command
\begin{verse}
  KSPLGMonitorDestroy(PetscDrawLG lg);
\end{verse}
The user can change aspects of the graphs with the \trl{DrawLG*()} and 
\trl{DrawAxis*()} routines. \findex{DrawAxis*()} \findex{DrawLG*()}
One can also access this functionality from the options database 
with the command \trl{-ksp_xmonitor} \trl{[x,y,w,h]}. \findex{-ksp_xmonitor} 
Where \trl{x, y, w, h} are the optional location and size of the window.

Once can cancel hardwired monitoring routines for KSP at runtime with 
\trl{-ksp_cancelmonitors}. \findex{-ksp_cancelmonitors}

As the Krylov method converges so that the residual norm is small,
say $ 10^{-10} $ many of the final digits printed with the \trl{-ksp_monitor}
option are meaningless. Worse, they are different on different 
machines; due to different round-off rules used by, say, the IBM RS6000
and the Sun Sparc. This makes testing between different machines
difficult. The option \trl{-ksp_smonitor} \findex{-ksp_smonitor}
causes PETSc to print fewer of the digits of the residual norm 
as it gets smaller; thus on most of the machines it will always
print the same numbers making cross processor testing easier.


\subsection{Understanding the Operator's Spectrum}

Since the convergence of Krylov subspace methods depends strongly on 
the spectrum (eigenvalues) of the preconditioned operator, PETSc has specific
routines for eigenvalue approximation via the Arnoldi or Lanczos iteration.
First, before the linear solve one must call 
\begin{verse}
 KSPSetComputeEigenvalues(KSP ksp,PETSC\_TRUE);
\end{verse}
Then after the SLES solve one calls \findex{KSPSetComputeEigenvalues()}
\begin{verse}
 KSPComputeEigenvalues(KSP ksp, int n,double *realpart,double *complexpart,int *neig);
\end{verse}
Here, \trl{n} is the size of the two arrays and the eigenvalues are 
inserted into those two arrays. \trl{Neig} is the number of eigenvalues computed;
this number depend depends on the size of the Krylov space generated during the 
linear system solution, for GMRES it is never larger than the restart parameter.
 \findex{KSPComputeEigenvalues()}
There is an additional routine 
\begin{verse}
 KSPComputeEigenvaluesExplicitly(KSP ksp, int n,double *realpart,double *complexpart);
\end{verse}
that is useful only for very small problems. It explicitly computes the 
full representation of the preconditioned operator and calles LAPACK to 
compute its eigenvalues. It should be only used for matrices of size up to 
a couple hundred.  The \trl{DrawSP*()} routines are very useful for 
drawing scatter plots of the eigenvalues. \findex{DrawSP*()}

The eigenvalues may also be computed and displayed graphically with the options 
data base commands \trl{-ksp_plot_eigenvalues} and \trl{-ksp_plot_eigenvalues_explicitly}.
\findex{-ksp_plot_eigenvalues} \findex{-ksp_plot_eigenvalues_explicitly}
Or they can be dumped to the screen in ASCII text via
\trl{-ksp_compute_eigenvalues} and \trl{-ksp_compute_eigenvalues_explicitly}.
\findex{-ksp_compute_eigenvalues} \findex{-ksp_compute_eigenvalues_explicitly}
\sindex{eigenvalues} \sindex{spectrum} \sindex{Arnoldi} \sindex{Lanczo}

\subsection{Other KSP Options}

To obtain the solution vector and right hand side from a KSP 
context, one uses \findex{KSPGetSolution()} \findex{KSPGetRhs()}
\begin{verse}
  KSPGetSolution(KSP ksp,Vec *x);\\
  KSPGetRhs(KSP ksp,Vec *rhs);
\end{verse}
During \findex{KSPSetRhs()} \findex{KSPSetSolution()} the iterative process
the solution may not yet have been calculated or it may be stored in 
a different location. To access the approximate solution during the 
iterative process, one uses the command \findex{KSPBuildSolution()}
\begin{verse}
  KSPBuildSolution(KSP ksp,Vec w,Vec *v);
\end{verse}
where the solution is returned in \trl{v}. The user can optionally provide
a vector in \trl{w} as the location to store the vector; however, if 
\trl{w} is \trl{PETSC_NULL}, space allocated by PETSc in the KSP context is 
used. One should not destroy this vector. For certain KSP methods, 
(e.g., GMRES), the construction of the solution is expensive, while for many 
others it requires not even a vector copy. 

Access to the residual is done in a similar way with the 
command \findex{KSPBuildResidual()}
\begin{verse}
  KSPBuildResidual(KSP ksp,Vec t,Vec w,Vec *v);
\end{verse}
Again, for GMRES and certain other methods this is an expensive 
operation.

\section{Preconditioners} \sindex{preconditioners}
\label{sec:pc}

As discussed in Section~\ref{sec:ksppc}, the Krylov space methods are
typically used in conjunction with a preconditioner.
To employ a particular preconditioning method, the user can either select 
it from the options database using input of the form 
\trl{-pc_type <methodname>} or set the method with the 
command \findex{PCSetType()} \findex{-pc_type}
\begin{verse}
  PCSetType(PC pc,PCType method);
\end{verse}
In Table~\ref{tab:pcdefaults} we summarize the basic
preconditioning methods supported in PETSc. \findex{PCNONE}
\findex{PCJACOBI} \findex{PCSOR} \findex{PCICC} \findex{PCLU}
\findex{PCILU} \findex{PCSHELL} \findex{PCASM}
\findex{PCBJACOBI}
The \trl{PCSHELL} preconditioner uses a specific,
application-provided preconditioner.  The direct preconditioner, \trl{PCLU},
is, in fact, a direct solver for the linear system that uses LU
factorization. \trl{PCLU} is included as a preconditioner so that PETSc has a
consistent interface among direct and iterative linear solvers.

\begin{table}
\begin{center}
\begin{tabular}{lll}
{\bf Method}    &{\bf PCType}   & {\bf Options Database Name}\\
\hline
Jacobi                               & PCJACOBI      & jacobi\\
Block Jacobi                         & PCBJACOBI     & bjacobi\\
SOR (and SSOR)                       & PCSOR         & sor\\
SOR with Eisenstat trick             & PCEISENSTAT   & eisenstat\\
Incomplete Cholesky                  & PCICC         & icc\\
Incomplete LU                        & PCILU         & ilu\\
Additive Schwarz                     & PCASM         & asm\\
Linear solver                        & PCSLES        & sles \\
Combination of preconditioners       & PCCOMPOSITE   & composite \\
\hline
LU                                   & PCLU          & lu\\
Cholesky                             & PCCholesky    & cholesky\\
No preconditioning                   & PCNONE        & none\\
Shell for user-defined PC            & PCSHELL       & shell\\
\hline
\end{tabular}
\end{center}
\caption{PETSc Preconditioners}
\label{tab:pcdefaults}
\end{table}

Each preconditioner may have associated with it a set of options,
which can be set with routines and options database commands provided
for this purpose.  Such routine names and commands are all of the form
\trl{PC<TYPE>Option} and \trl{-pc_<type>_option [value]}.  A
complete list can be found by consulting the manual pages; we discuss
just a few in the sections below.

\subsection{ILU and ICC Preconditioners}
\label{sec:ilu_icc}

Some of the options for ILU preconditioner are 
\begin{verse}
  PCILUSetLevels(PC pc,int levels);\\
  PCILCCSetLevels(PC pc,int levels);\\
  PCILUSetReuseOrdering(PC pc,PetscTruth flag);\\
  PCILUSetUseDropTolerance(PC pc,double dt,int dtcount);\\
  PCILUDTSetReuseFill(PC pc,PetscTruth flag);\\
  PCILUSetUseInPlace(PC pc); \\
  PCILUSetAllowDiagonalFill(PC pc);
\end{verse}
\findex{PCILUSetLevels()} \findex{PCILUSetReuseOrdering}
\findex{PCILUSetUseDropTolerance()} \findex{PCILUDTSetReuseFill()}
\findex{PCILUSetUseInPlace()} \findex{PCILUSetAllowDiagonalFill()}

When repeatedly solving linear systems with the same SLES
context, one can reuse some information  computed
during the first linear solve.
In particular, \trl{PCILUSetReuseOrdering()} causes the ordering  (for example, set with 
\trl{-pc_ilu_ordering_type order}) computed in the first factorization to be reused
for later factorizations. \sindex{orderings} 
The \trl{ PCILUDTSetReuseFill()} causes the 
fill computed during the first drop tolerance factorization to be reused
in later factorizations. \trl{PCILUSetUseInPlace()} is often used with 
\trl{PCASM} or \trl{PCBJACOBI} when zero fill is used, since it reuses the 
matrix space to store the incomplete factorization it saves memory and 
copying time. Note that in-place factorization is not appropriate with 
any ordering besides natural and cannot be used with the drop tolerance
factorization. These options may be set in the database with 
\begin{verse}
   -pc\_ilu\_levels <levels>\\
   -pc\_ilu\_reuse\_ordering\\
   -pc\_ilu\_use\_drop\_tolerance <dt>,<dtcount>\\
   -pc\_ilu\_reuse\_fill\\
   -pc\_ilu\_in\_place\\
   -pc\_ilu\_nonzeros\_along\_diagonal\\
   -pc\_ilu\_diagonal\_fill
\end{verse}
\findex{-pc_ilu_levels} \findex{-pc_ilu_reuse_ordering}
\findex{-pc_ilu_use_drop_tolerance} \findex{-pc_ilu_reuse_fill}
\findex{-pc_ilu_in_place} \findex{-pc_ilu_nonzeros_along_diagonal}
\findex{-pc_ilu_diagonal_fill}

See Section~\ref{sec:symbolfactor} for information on preallocation
of memory for anticipated fill during factorization.
By alleviating the considerable overhead for dynamic memory allocation,
such tuning can significantly enhance performance.

% --------------------------------------------------------------------
PETSc supports incomplete factorization preconditioners for several matrix
types for the uniprocessor case.  \sindex{ILU, parallel}
\sindex{ICC, parallel} In addition, for the parallel case
we provide an interface to the ILU and ICC preconditioners
of BlockSolve95 \cite{bs-user-ref}.   PETSc enables users
to employ the preconditioners within
BlockSolve95 by using the BlockSolve95 matrix format \trl{MATMPIROWBS}
and invoking either the \trl{PCILU} or \trl{PCICC} method within the linear
solvers.  Since PETSc automatically handles matrix assembly,
preconditioner setup, profiling, etc., users who employ BlockSolve95
through the PETSc interface need not concern themselves with many
details provided within the BlockSolve95 users manual. Consult the file
\trl{docs/installation/index.htm} for details on installing PETSc to allow 
the use of BlockSolve95.

One can create a matrix that is compatible with BlockSolve95 by using
\trl{MatCreate()} with the option \trl{-mat_mpirowbs}, or by directly
calling \findex{MatCreateMPIRowbs()}
\begin{verse}
  MatCreateMPIRowbs(MPI\_Comm comm,int m,int M,int nz,int *nnz,Mat *A)
\end{verse}
\trl{A} is the newly created matrix, while the arguments \trl{m} and
\trl{M} indicate the number of local and global rows,
respectively. Either the local or global parameter can be replaced
with \trl{PETSC_DECIDE}, so that PETSc will determine it.  The matrix
is stored with a fixed number of rows on each processor, given by \trl{
m}, or determined by PETSc if \trl{m} is \trl{PETSC_DECIDE}.  The
arguments \trl{nz} and \trl{nnz} can be used to preallocate storage
space, as discussed in Section~\ref{sec:matcreate} for increasing
the efficiency of matrix assembly; one sets \trl{nz=0} and
\trl{nnz=PETSC_NULL} for PETSc to control all matrix memory
allocation.  The argument \trl{proci} is an optional BlockSolve95
\trl{BSprocinfo} context; most users should set this parameter to 
\trl{PETSC_NULL}, so that PETSc will create and initialize this context.

If the matrix is symmetric, one {\em may} call
\begin{verse}
  MatSetOption(Mat mat,MAT\_SYMMETRIC);
\end{verse}
to improve efficiency, but in this case one cannot use the ILU 
preconditioner, only ICC.

Internally, PETSc inserts zero elements into matrices of the \trl{
MATMPIROWBS} format if necessary, so that nonsymmetric matrices are
considered to be symmetric in terms of their sparsity structure; this
format is required for use of the parallel communication routines
within BlockSolve95. In particular, if the matrix element $A[i,j]$
exists, then PETSc will internally allocate a 0 value for the element
$ A[j,i] $ during \trl{MatAssemblyEnd()} if the user has not already set
a value for the matrix element $ A[j,i] $ .

When manipulating a preconditioning matrix, $ A $, BlockSolve95
internally works with a scaled and permuted matrix, $ \hat{A} = P
D^{-1/2} A D^{-1/2},$ where $ D $ is the diagonal of $ A $, and $ P $ is a
permutation matrix determined by a graph coloring for efficient
parallel computation.  Thus, when solving a linear system, $ Ax=b $,
using ILU/ICC preconditioning and the matrix format \trl{MATMPIROWBS}
for {\em both} the linear system matrix and the preconditioning
matrix, one actually solves the scaled and permuted system $ \hat{A}
\hat{x} = \hat{b} $, where $ \hat{x} = P D^{1/2} x $ and $\hat{b} = P
D^{-1/2} b$ .  PETSc handles the internal scaling and permutation of
$ x $ and $ b $, so the user does {\em not} deal with these conversions, 
but instead always works with the original linear system.  In
this case, by default the scaled residual norm is monitored; one must use the
option \trl{-ksp_truemonitor} \findex{-ksp_truemonitor} to print both the
scaled and unscaled residual norms. {\em Note}: If one is using ILU/ICC via
BlockSolve95 and the \trl{MATMPIROWBS} matrix format for the 
preconditioner matrix, but using a different format for a different
linear system matrix, then this scaling and permuting is done only
internally during the application of the preconditioner.


% --------------------------------------------------------------------

\subsection{SOR and SSOR Preconditioners}

PETSc does not provide a parallel SOR, it can only be used on sequential 
matrices or as the subblock preconditioner when using block Jacobi or 
ASM preconditioning, see below.

The options for SOR \sindex{SSOR} \sindex{SOR} \sindex{relaxation}
preconditioning are \findex{PCSORSetOmega()}
\begin{verse}
  PCSORSetOmega(PC pc,double omega);\\
  PCSORSetIterations(PC pc,int its);\\
  PCSORSetSymmetric(PC pc,MatSORType type);
\end{verse}
The \findex{PCSORSetIterations()} \findex{PCSORSetSymmetric()}
first of these commands sets the relaxation factor for successive
over (under) relaxation.  The second command sets the number of inner
iterations of SOR, given by \trl{its}, to use between steps of the
Krylov space method.  The third command sets the kind of SOR sweep,
where the argument \trl{type} can be one of \trl{SOR_FORWARD_SWEEP,
SOR_BACKWARD_SWEEP} or \trl{SOR_SYMMETRIC_SWEEP}, the default
being \trl{SOR_FORWARD_SWEEP}. Setting the type to be \trl{
SOR_SYMMETRIC_SWEEP} produces the SSOR method.  In addition, 
each processor can locally and independently perform the specified 
variant of SOR with the types \trl{SOR_LOCAL_FORWARD_SWEEP, 
SOR_LOCAL_BACKWARD_SWEEP}, and \trl{SOR_LOCAL_SYMMETRIC_SWEEP}.
These \findex{SOR_FORWARD_SWEEP} \findex{SOR_BACKWARD_SWEEP}
variants \findex{SOR_SYMMETRIC_SWEEP} \findex{SOR_LOCAL_FORWARD_SWEEP}
can \findex{SOR_LOCAL_BACKWARD_SWEEP} \findex{SOR_LOCAL_SYMMETRIC_SWEEP}
also be set with the options \trl{-pc_sor_omega <omega>}, 
\trl{-pc_sor_its <its>}, \trl{-pc_sor_backward}, \trl{-pc_sor_symmetric}, 
\break \trl{-pc_sor_local_forward}, \trl{-pc_sor_local_backward}, and 
\trl{-pc_sor_local_symmetric}.
\findex{-pc_sor_omega} \findex{-pc_sor_its}
\findex{-pc_sor_backward} \findex{-pc_sor_symmetric}
\findex{-pc_sor_local_forward} \findex{-pc_sor_local_backward}
\findex{-pc_sor_local_symmetric} 

The Eisenstat trick \cite{eisenstat81} \sindex{Eisenstat trick} for SSOR preconditioning 
can be employed with the method PCEISENSTAT \findex{PCEISENSTAT} 
(\trl{-pc_type eisenstat}).  
By using both left and right preconditioning of the linear system,
this variant of SSOR requires about half of the floating-point operations 
for conventional SSOR. \findex{-pc_eisenstat_no_diagonal_scaling} 
\findex{PCEisenstatNoDiagonalScaling()} The option 
\break \trl{-pc_eisenstat_no_diagonal_scaling}) 
(or the routine \trl{PCEisenstatNoDiagonalScaling()})
turns off diagonal scaling in conjunction with Eisenstat SSOR method, while
the option \trl{-pc_eisenstat_omega <omega>} (or the routine
\trl{PCEisenstatSetOmega(PC pc,double omega)})
sets the SSOR relaxation coefficient, \trl{omega}, as discussed above.
\findex{-pc_eisenstat_omega} \findex{PCEisenstatSetOmega()} 


%------------------------------------------------------------------
\subsection{LU Factorization}

The LU preconditioner provides several options.  The first, given by
the \sindex{direct solver}
command \findex{PCLUSetUseInPlace()} \sindex{in-place solvers}
\begin{verse}
  PCLUSetUseInPlace(PC pc);
\end{verse}
causes the factorization to be performed in-place and hence
destroys the original matrix.  The options database variant of
this command is \trl{-pc_lu_in_place}. \findex{-pc_lu_in_place}
Another direct preconditioner option is selecting the ordering
of equations with the command \findex{-pc_lu_ordering_type} \sindex{orderings}
\begin{verse}
   -pc\_lu\_ordering\_type <ordering>
\end{verse}
The possible orderings are
\begin{itemize}
\item \trl{MATORDERING_NATURAL} - Natural
\item \trl{MATORDERING_ND} - Nested Dissection
\item \trl{MATORDERING_1WD} - One-way Dissection
\item \trl{MATORDERING_RCM} - Reverse Cuthill-McKee
\item \trl{MATORDERING_QMD} - Quotient Minimum Degree
\end{itemize}
\findex{MATORDERING_NATURAL} \findex{MATORDERING_ND} \findex{MATORDERING_1WD}
\findex{MATORDERING_RCM} \findex{MATORDERING_QMD} \sindex{nested dissection}
\sindex{one-way dissection} \sindex{reverse Cuthill-McKee} 
\sindex{quotient minimum degree} \findex{-pc_lu_ordering_type}
These orderings can also be set through the options database by specifying 
one of the following:  \trl{-pc_lu_ordering_type} \trl{natural}, \trl{-pc_lu_ordering_type}
\trl{nd}, \trl{-pc_lu_ordering_type} \trl{1wd}, \trl{-pc_lu_ordering_type} \trl{rcm},
\trl{-pc_lu_ordering_type} \trl{qmd}.
In addition, see 
\break \trl{MatGetOrdering()}, discussed in Section~\ref{sec:matfactor}.

The sparse LU factorization provided in PETSc does not perform pivoting for 
numerical stability (since they are designed to preserve nonzero 
structure), thus occasionally a LU factorization will fail with a zero 
pivot when, in fact, the matrix is non-singular. The option
\trl{-pc_lu_nonzeros_along_diagonal <tol>} \findex{-pc_lu_nonzeros_along_diagonal}
will often help eliminate the zero pivot, by preprocessing the the 
column ordering to remove small values from the diagonal. Here, \trl{tol}
is an optional tolerance to decide if a value is nonzero; by default it
is $ 1.e-10.$ 


In addition, Section~\ref{sec:symbolfactor} provides information on
preallocation of memory for anticipated fill during factorization.  
Such tuning can significantly enhance performance, since it
eliminates the considerable overhead for dynamic memory allocation.

%----------------------------------------------------------------------
\subsection{Block Jacobi and 
            Overlapping Additive Schwarz Preconditioners}
\label{sec:bjacobi}

\sindex{Jacobi} \sindex{block Jacobi}
\sindex{block Gauss-Seidel} \sindex{ASM} \sindex{overlapping Schwarz}
The block Jacobi and overlapping additive Schwarz methods in PETSc are
supported in parallel; however, only the uniprocessor
version of the block Gauss-Seidel method is currently in place.
By default, the PETSc implentations of these methods
employ ILU(0) factorization on each individual block ( that is, the default solver on each 
subblock is \trl{PCType=PCILU,
KSPType=KSPPREONLY}); the user can set alternative linear solvers via the options 
\findex{-sub_ksp_type} \findex{-sub_pc_type}
\trl{-sub_ksp_type} and \trl{-sub_pc_type}. In fact, all of the KSP
and PC options can be applied to the subproblems by inserting the prefix
\trl{-sub_} at the beginning of the option name. \sindex{local linear solves}
These options database commands set the particular options for {\em all} 
of the blocks within the global problem.  In addition, the routines
\begin{verse}
  PCBJacobiGetSubSLES(PC pc,int *n\_local,int *first\_local,SLES **subsles);\\
  PCASMGetSubSLES(PC pc,int *n\_local,int *first\_local,SLES **subsles);
\end{verse}
extract the \findex{PCBJacobiGetSubSLES()} SLES context for each local 
block.  The argument \trl{n_local} is the number of blocks on the 
calling processor, and \trl{first_local} indicates the global number 
of the first block on the processor. The blocks are numbered 
successively by processors from zero through $ gb-1$, 
where $ gb $ is the number of global blocks.  
The array of SLES contexts for the local blocks is given by \trl{subsles}. 
This mechanism enables the user to set different solvers for the 
various blocks.  To set the appropriate data structures, the 
user {\em must} explicitly call \trl{SLESSetUp()} \findex{SLESSetUp()} 
before calling \trl{PCBJacobiGetSubSLES()} or
\trl{PCASMGetSubSLES()}.
For further details, see the 
example \trl{${PETSC_DIR}/src/sles/examples/tutorials/ex7.c}.

The block Jacobi, block Gauss-Seidel, and additive Schwarz 
preconditioners allow the user
to set the number of blocks into which the problem is divided.  The
options database commands to set this value are \trl{-pc_bjacobi_blocks n}
and \trl{-pc_bgs_blocks n}, and, within a program, the corresponding routines
are \sindex{block Jacobi} \sindex{block Gauss-Seidel}
\findex{-pc_bjacobi_blocks} \findex{-pc_bgs_blocks} 
\begin{verse}
  PCBJacobiSetTotalBlocks(PC pc,int blocks,int *size);\\
  PCASMSetTotalSubdomains(PC pc,int n,IS *is);\\
  PCASMSetType(PC pc,PCASMType type);
\end{verse}
\findex{PCBJacobiSetTotalBlocks()}
The \findex{PCASMSetTotalSubdomains()}
optional argument \trl{size}, is an array indicating the size of
each block. Currently, for certain parallel matrix formats, only a
single block per processor is supported. However, the \trl{MATMPIAIJ} and 
\trl{MATMPIBAIJ} formats
support the use of general blocks as long as no blocks are shared
among processors. The \trl{is} argument contains the index sets that
define the subdomains. 

\trl{PCASMType} is one of \trl{PC_ASM_BASIC},
\trl{PC_ASM_INTERPOLATE}, \trl{PC_ASM_RESTRICT}, \trl{PC_ASM_NONE}
and may also be set with the options database \trl{-pc_asm_type [basic,interpolate,restrict,none]}.
\findex{-pc_asm_type} \findex{PCASMSetType()} \findex{PC_ASM_BASIC} 
\findex{PC_ASM_RESTRICT} \findex{PC_ASM_INTERPOLATE} \findex{PC_ASM_NONE}
The type \trl{PC_ASM_BASIC} (or \trl{-pc_asm_type basic}) corresponds to the
standard additive Schwarz method that uses the full restriction and
interpolation operators.
The type \trl{PC_ASM_RESTRICT} (or \trl{-pc_asm_type restrict}) uses a full
restriction operator, but during the interpolation process ignores the off-processor
values.
Similarly, \trl{PC_ASM_INTERPOLATE} (or \trl{-pc_asm_type interpolate}) uses a limited
restriction process in conjunction with a full interpolation, while
\trl{PC_ASM_NONE} (or \trl{-pc_asm_type none}) ignores off-processor valies
for both restriction and interpolation.
The ASM types with limited restriction or interpolation were suggested by 
Xiao-Chuan Cai and Marcus Sarkis \cite{cs97a}.  \sindex{Cai, Xiao-Chuan}  \sindex{Sarkis, Marcus}
\trl{PC_ASM_RESTRICT} is the PETSc default, as it saves substantial communication
and for many problems has the added benefit of requiring fewer iterations for convergence
than the standard additive Schwarz method.

The user can also set the number of blocks and sizes on a per-processor
basis with the commands
\begin{verse}
  PCBJacobiSetLocalBlocks(PC pc,int blocks,int *size);\\
  PCASMSetLocalSubdomains(PC pc,int N,IS *is);
\end{verse}

For the ASM preconditioner one can use the following command to set
the overlap to compute in constructing the subdomains.
\begin{verse}
  PCASMSetOverlap(PC pc,int overlap);
\end{verse}
\findex{PCASMSetOverlap()}
The overlap defaults to 1, so if one desires that no additional
overlap be computed beyond what may have been set with a call to \trl{
PCASMSetTotalSubdomains()} or \trl{PCASMSetLocalSubdomains()}, then
\trl{overlap} must be set to be 0.  In particular, if one does {\em
not} explicitly set the subdomains in an application code, then all
overlap would be computed internally by PETSc, and using an overlap of
0 would result in an ASM variant that is equivalent to the block
Jacobi preconditioner.  Note that one can define initial index sets
\trl{is} with {\em any} overlap via \trl{PCASMSetTotalSubdomains()} or
\trl{PCASMSetLocalSubdomains()}; the routine \trl{PCASMSetOverlap()}
merely allows PETSc to extend that overlap further if desired.

%----------------------------------------------------------------------
\subsection{Shell Preconditioners}

The shell preconditioner simply uses an application-provided routine to 
implement the preconditioner. To set this routine, one uses the 
command \findex{PCShellSetApply()}
\begin{verse}
  PCShellSetApply(PC pc,int (*apply)(void *ctx,Vec,Vec),void *ctx);
\end{verse}
The final argument \trl{ctx} is a pointer to the application-provided 
data structure needed by the preconditioner routine.
The three routine arguments of \trl{apply()} are this context, the
input vector, and the output vector, respectively.

For a preconditioner that requires some sort of ``setup'' before being used,
that requires a new setup everytime the operator is changed, one can 
provide a ``setup'' routine that is called everytime the operator is 
changed (usually via \trl{SLESSetOperators()}).
\findex{PCShellSetSetUp()}
\begin{verse}
  PCShellSetSetUp(PC pc,int (*setup)(void *ctx));
\end{verse}
The argument to the ``setup'' routine is the same application-provided 
data structure passed in with the \trl{PCShellSetApply()} routine.

%----------------------------------------------------------------------
\subsection{Combining Preconditioners} \sindex{combining preconditioners}

The PC type \trl{PCCOMPOSITE} \findex{PCCOMPOSITE} allows one to form 
new preconditioners by combining already defined preconditioners and 
solvers. Combining preconditioners usually requires some experimentation
to find a combination of preconditioners that works better than any
single method. It is a tricky business and is not recommended until 
your application code is complete and running and you are trying to 
improve performance. In many cases using a single preconditioner is better
than a combination; an exception is the multigrid/multilevel preconditioners
(solvers) that are always combinations of some sort, see Section \ref{sec:mg}.

Let $B_1$ and $B_2$ represent the application of two 
preconditioners of type \trl{type1} and \trl{type2}. The preconditioner
$ B = B_1 + B_2 $ can be obtained with
\begin{verse}
  PCSetType(pc,PCCOMPOSITE);\\
  PCCompositeAddPC(pc,type1);\\
  PCCompositeAddPC(pc,type2);
\end{verse}
Any number of preconditioners may added in this way. 

This way of combining preconditioners is called additive, since 
the actions of the preconditioners are added together. This is the 
default behavior. An alternative can be set with the option
\begin{verse}
  PCCompositeSetType(PC pc,PCCompositeType PC\_COMPOSITE\_MULTIPLICATIVE);
\end{verse}
\findex{PCCompositeSetType()} \findex{PC_COMPOSITE_MULTIPLICATIVE}
\findex{PC_COMPOSITE_ADDITIVE} \findex{PCCompositeAddPC} \sindex{additive preconditioners}
\sindex{multiplicative preconditioners}
In this form the new residual is updated after the application of 
each preconditioner and the next preconditioner applied to the next 
residual. For example, with two composed preconditioners: $B_1$ and 
$ B_2$; $ y = B x $ is obtained from
\begin{eqnarray*}
  y    = B_1 x \\
  w_1  = x - A y \\
  y    = y + B_2 w_1 
\end{eqnarray*}
Loosely, this corresponds to a Gauss-Siedel iteration, while
additive corresponds to a Jacobi like.

Under most circumstances the multiplicative form requires one-half the number of
iterations as the additive form; but the multiplicative form does require 
the application of $ A $ inside the preconditioner. 

In the multiplicative version, the calculation of the residual inside the 
preconditioner can be done in two ways: using the original linear system matrix
or using the matrix used to build the preconditioners \trl{B1}, \trl{B2}, etc.
By default it uses the ``preconditioner matrix'', to use the true matrix use the 
option \findex{PCCompositeSetUseTrue()}
\begin{verse}
  PCCompositeSetUseTrue(PC pc);
\end{verse}

The individual 
preconditioners can be accessed (in order to set options) via
\begin{verse}
  PCCompositeGetPC(PC pc,int count,PC *subpc);
\end{verse}
For example, to set the first sub preconditioners to use ILU(1)
\begin{verse}
   PC subpc;\\
  PCCompositeGetPC(pc,0,\&subpc);\\
  PCILUSetFill(subpc,1);
\end{verse}
\findex{PCCompositeGetPC}

These various options can also be set via the options database. For example,
\trl{-pc_type composite} \findex{composite} \trl{-pc_composite_pcs jacobi,ilu}
\findex{-pc_composite_pcs} causes the composite preconditioner to be used with 
two preconditioners: Jacobi and ILU. The option \trl{-pc_composite_type multiplicative}
\findex{-pc_composite_type} initiates the multiplicative version of the algorithm,
while \trl{-pc_composite_type additive} the additive version. Using the true
preconditioner is obtained with the option \trl{-pc_composite_true}. 
\findex{-pc_composite_true} One sets options for the subpreconditioners with the 
extra prefix \trl{-sub_N_} where \trl{N} is the number of the subpreconditioner.
For example, \trl{-sub_0_pc_ilu_fill 0}.


PETSc also allows a preconditioner to be a complete linear solver. This is 
achieved with the \trl{PCSLES} type. \findex{PCSLES}
\begin{verse}
  PCSetType(PC pc,PCSLES PCSLES);\\
  PCSLESGetSLES(pc,\&sles);\\
   /* set any SLES/KSP/PC options */
\end{verse}
\findex{PCSLESGetSLES()} From the command line one can use 5 iterations of 
bi-CG-stab with ILU(0) preconditioning as the preconditioner with 
\trl{-pc_type sles -sles_pc_type ilu -sles_ksp_max_it 5 -sles_ksp_type bcgs}. 

By default the inner SLES preconditioner uses the outter ``preconditioner matrix'', 
as the matrix to be solved in the linear system, to use the true matrix use the 
option \findex{PCSLESSetUseTrue()}
\begin{verse}
  PCSLESSetUseTrue(PC pc);
\end{verse}
at the command line with \trl{-pc_sles_true}. \findex{-pc_sles_true}

Naturally one can use a SLES preconditioner inside a composite preconditioner. For example,
\trl{-pc_type composite -pc_composite_pcs ilu,sles -sub_1_pc_type jacobi -sub_1_ksp_max_it 10}
uses two preconditioners: ILU(0) and 10 iterations of GMRES with Jacobi preconditioning. Though
it is not clear whether one would ever wish to do such a thing.

%----------------------------------------------------------------------
\subsection{Multigrid Preconditioners} \sindex{multigrid} \label{sec:mg}

See also Chapter \ref{chapter:dmmg} for a higher level
interface to the multigrid solvers for linear and nonlinear problems using the \trl{DMMG} object.

A large suite of routines is available for using multigrid as a
preconditioner. In the \trl{PC} framework the user is required to provide 
the coarse grid solver, smoothers, restriction, and interpolation, 
as well as the code to calculate residuals. The \trl{PC} component 
allows all of that to be wrapped up into a PETSc compliant preconditioner. 
We fully support both matrix-free and matrix-based multigrid solvers.

A multigrid preconditioner is created with the four commands 
\begin{verse}
  SLESCreate(MPI\_Comm comm,SLES *sles);\\
  SLESGetPC(SLES sles,PC *pc);\\
  PCSetType(PC pc,PCMG);\\
  MGSetLevels(pc,int levels,MPI\_Comm *comms);
\end{verse}
A \findex{MGSetLevels()} 
large number of parameters affect the multigrid behavior. The command
\begin{verse}
  MGSetType(PC pc,MGType mode); 
\end{verse}
indicates which form of multigrid to apply \cite{1sbg}. 
\findex{MGSetType()}
\sindex{multigrid, multiplicative} 
\sindex{multigrid, additive} \sindex{multigrid, full} \sindex{multigrid, Kaskade}

For standard V or W-cycle multigrids, one sets the \trl{
mode} to be \findex{MGMULTIPLICATIVE} \trl{MGMULTIPLICATIVE}; for the
additive form (which in certain cases reduces to the BPX method, or additive 
multilevel Schwarz, or multilevel diagonal scaling), one uses
\findex{MGADDITIVE} \trl{MGADDITIVE} as the \trl{mode}.  For a variant
of full multigrid, one can
 use \findex{MGFULL} \trl{MGFULL}, and for the Kaskade 
algorithm \findex{MGKASKADE} \trl{MGKASKADE}.
For the multiplicative and full multigrid options, one can use a
W-cycle by \sindex{W-cycle} \sindex{V-cycle} calling
\findex{MGSetCycles()} \findex{MG_W_CYCLE}
\begin{verse}
  MGSetCycles(PC pc,int cycles);
\end{verse}
with a value of \trl{MG_W_CYCLE} for \trl{cycles}. 
The commands above can also be set from the options database. The option 
names are \trl{-pc_mg_type [multiplicative, additive, full, kaskade]},
and \trl{-pc_mg_cycles} \trl{<cycles>}. \findex{-pc_mg_type} \findex{-pc_mg_cycles}

The user can control the amount of pre- and postsmoothing 
\sindex{smoothing} \sindex{relaxation} by using
either the options \findex{-pc_mg_smoothup} \findex{-pc_mg_smoothdown}
\trl{-pc_mg_smoothup} \trl{m} and \trl{-pc_mg_smoothdown} \trl{n} or
the routines \findex{MGSetNumberSmoothUp()} \findex{MGSetNumberSmoothDown()}
\begin{verse}
  MGSetNumberSmoothUp(PC pc,int m);\\
  MGSetNumberSmoothDown(PC pc,int n);
\end{verse}
Note that if the command \trl{MGSetSmoother()} (discussed below) has
\findex{MGSetSmoother()} been employed, the same amounts of pre-
and postsmoothing will be used.

The remainder of the multigrid routines, which determine
the solvers and interpolation/restriction operators that are used,
are mandatory.
To set the coarse grid solver, one must \sindex{coarse grid solve}
call \findex{MGGetCoarseSolve()}
\begin{verse}
  MGGetCoarseSolve(PC pc,SLES *sles);
\end{verse}
and set the appropriate options in \trl{sles}. Similarly, the 
smoothers are set by calling \findex{MGGetSmoother()}
\begin{verse}
  MGGetSmoother(PC pc,int level,SLES *sles);
\end{verse}
and setting the various options in \trl{sles.} 
To use a different pre- and postsmoother, one should call the following
routines instead.
\begin{verse}
  MGGetSmootherUp(PC pc,int level,SLES *upsles);
\end{verse}
and 
\begin{verse}
  MGGetSmootherDown(PC pc,int level,SLES *downsles);
\end{verse}

Use
\begin{verse}
  MGSetInterpolate(PC pc,int level,Mat P);
\end{verse}
and
\begin{verse}
  MGSetRestriction(PC pc,int level,Mat R);
\end{verse}
to define the intergrid transfer operations.

It is possible for these interpolation operations to be matrix free
(see Section \ref{sec:matrixfree}),
he or she should make sure that these operations are defined for the (matrix-free) matrices
passed in. 
Note that this system is arranged so that if the interpolation is 
the transpose of the restriction, the same \trl{mat} argument can be 
passed to both \trl{MGSetRestriction()} and \trl{MGSetInterpolation()}.

On each level except the coarsest, one must also set the routine to 
compute the residual.  The following command suffices: \findex{MGSetResidual()}
\begin{verse}
   MGSetResidual(PC pc,int level,int (*residual)(Mat,Vec,Vec,Vec),Mat mat);
\end{verse}
The \trl{residual()} function can be set to be \trl{MGDefaultResidual()}
if \findex{MGDefaultResidual()}
one's operator is stored in a \trl{Mat} format.  In certain circumstances, 
where it is much cheaper to calculate the residual directly, rather 
than through the usual formula $b - Ax$,  the user may wish to provide 
an alternative. 

Finally, the user must provide three work vectors for each level 
(except on the finest, where only the residual work vector is required).
The work vectors are set with the 
commands \findex{MGSetRhs()} \findex{MGSetX()} \findex{MGSetR()} 
\begin{verse}
  MGSetRhs(PC pc,int level,Vec b);\\
  MGSetX(PC pc,int level,Vec x);\\
  MGSetR(PC pc,int level,Vec r);
\end{verse}
The user is responsible for freeing these vectors once the iteration 
is complete.

One can control the KSP and PC options used on the various levels
(as well as the coarse grid) using the prefix \trl{mg_levels_} (
\trl{mg_coarse_} for the coarse grid).
\findex{-mg_levels} For example,
\begin{verse}
  -mg\_levels\_ksp\_type cg
\end{verse}
will cause the CG method to be used as the Krylov method for each level.
Or
\begin{verse}
  -mg\_levels\_pc\_type ilu -mg\_levels\_pc\_ilu\_levels 2
\end{verse}
will cause the the ILU preconditioner to be used on each level with 
two levels of fill in the incomplete factorization.


% ---------------------------------------------------------------
\chapter{SNES: Nonlinear Solvers}
\sindex{nonlinear equation solvers}
\label{chapter:snes}

The solution of large-scale nonlinear problems pervades many facets of
computational science and demands robust and flexible solution
strategies. The SNES component of PETSc provides a powerful suite of
data-structure-neutral numerical routines for such problems.  Built on
top of the linear solvers and data structures discussed in preceding
chapters, SNES enables the user to easily customize the nonlinear
solvers according to the application at hand.  Also, the SNES
interface is {\em identical} for the uniprocessor and parallel cases;
the only difference in the parallel version is that each processor
typically forms only its local contribution to various matrices and
vectors.

SNES includes methods for solving systems of nonlinear equations of the form 
\begin{equation}
\F(\x) = 0,
\label{eq:F=0}
\end{equation}
where $\F: \, \Re^n \rightarrow \Re^n$.
Newton-like methods provide the core of the package, including
\sindex{Newton-like methods} both line search \sindex{line search} 
and trust region \sindex{trust region} techniques, which are discussed
further in Section~\ref{sec:nlsolvers}. Following the
PETSc design philosophy, the interfaces to the various solvers are all
virtually identical. In addition, the SNES software is completely
flexible, so that the user can at runtime change any facet of the
solution process.

The general form of the $n$-dimensional Newton's method for solving
(\ref{eq:F=0}) is
\begin{equation}
     \x_{k+1} = \x_k - [ \F'(\x_k)]^{-1} \F(\x_k), \;\; k=0,1, \ldots, 
\label{eq:n1}
\end{equation}
where $ \x_0 $ is an initial approximation to the solution and   
$ \F'(\x_k) $ is nonsingular.  
In practice, the Newton iteration (\ref{eq:n1}) is implemented by
the following two steps:
\begin{eqnarray}
  1. & {\rm (Approximately) \;solve\;\;\;} \F'(\x_k) \Delta \x_k = -\F(\x_k).\\
  2. & {\rm Update\;\;\;} \x_{k+1} = \x_k + \Delta \x_k. \hspace{.225in}
\end{eqnarray}


\section{Basic Usage}
\label{sec:snesusage}

In the simplest usage of the nonlinear solvers, the user must merely 
provide a C, C++, or Fortran routine to evaluate the nonlinear function 
of Equation~(\ref{eq:F=0}) or (\ref{eq:minF}).
The corresponding Jacobian \sindex{Jacobian} matrix 
can be approximated with finite differences.
For codes that are typically more efficient and accurate, the
user can provide a routine to compute the Jacobian; details regarding these application-provided 
routines are discussed below.  
To provide an overview of the use of the nonlinear solvers,
we first introduce a complete and simple example in
Figure~\ref{fig:snesexample}, corresponding to 
\trl{${PETSC_DIR}/src/snes/examples/tutorials/ex1.c}.  

\begin{figure}[H]
{\small
\fileinclude{../../../src/snes/examples/tutorials/ex1.c}
}
\caption{Example of Uniprocessor SNES Code}
\label{fig:snesexample}
\end{figure}

To create a SNES solver, one must first call \trl{SNESCreate()} and indicate
the class of problem being solved, using one of the following:
\begin{verse}
  SNESCreate(MPI\_Comm comm,SNES\_NONLINEAR\_EQUATIONS,SNES *snes);\\
  SNESCreate(MPI\_Comm comm,SNES\_UNCONSTRAINED\_MINIMIZATION,SNES *snes);
\end{verse}
When solving a system of nonlinear equations, the user must then set
routines for evaluating the function of equation~(\ref{eq:F=0}) and its
associated Jacobian matrix. 

To choose a nonlinear solution method, the user can either
call \findex{SNESSetType()}
\begin{verse}
  SNESSetType(SNES snes,SNESType method);
\end{verse}
or use the the option \trl{-snes_type <method>}, \findex{-snes_type} 
where details regarding the available methods are presented in
Section~\ref{sec:nlsolvers}.
The application code can take complete control of the linear and
nonlinear techniques used in the Newton-like method by calling
\findex{SNESetFromOptions()}
\begin{verse}
  SNESSetFromOptions(snes);
\end{verse}
This routine provides an interface to the PETSc options database, so
that at runtime the user can select a particular nonlinear solver, set
various parameters and customized routines (e.g., specialized line
search variants), prescribe the convergence tolerance, and set
monitoring routines.  With this routine the user can also control all
linear solver options in the SLES, KSP, and PC modules, as discussed
in Chapter~\ref{ch:sles}.

After having set these routines and options, the user
solves the problem by calling \findex{SNESSolve}
\begin{verse}
  SNESSolve(SNES snes,Vec x,int *iters);
\end{verse}
where \trl{iters} is the number of nonlinear iterations required for
convergence and \trl{x} indicates the solution vector. The user should
initialize this vector to the initial guess for the nonlinear solver
prior to calling \trl{SNESSolve()}.  In particular, to employ an
initial guess of zero, the user should explicitly set this vector to
zero by calling \trl{VecSet()}.  Finally, after solving the nonlinear
system (or several systems), the user should destroy the SNES context
with
\begin{verse}
  SNESDestroy(SNES snes);
\end{verse}

%----------------------------------------------------------------------
\subsection{Solving Systems of Nonlinear Equations}
\label{sec:sneseq}

When solving a system of nonlinear equations, the user must provide
a vector, \trl{f}, for storing the function of
Equation~(\ref{eq:F=0}), as well as a routine that evaluates this
function at the vector \trl{x}.  This information should be set with
the command \findex{SNESSetFunction()}
\begin{verse}
  SNESSetFunction(SNES snes,Vec f,\\
          int (*FormFunction)(SNES snes,Vec x,Vec f,void *ctx),void *ctx);
\end{verse}
The argument \trl{ctx} is an optional user-defined context, which can
store any private, application-specific data required by the
function evaluation routine; \trl{PETSC_NULL} should be used if such information
is not needed.  In C and C++, a user-defined context is merely a
structure in which various objects can be stashed; in Fortran a user
context can be an integer array that contains both parameters and
pointers to PETSc objects. \trl{${PETSC_DIR}/src/snes/examples/tutorials/ex5.c} and
\trl{${PETSC_DIR}/src/snes/examples/tutorials/ex5f.F} give examples of user-defined
application contexts in C and Fortran, respectively.

The user must also specify a routine to form some approximation of the
Jacobian matrix, \trl{A}, at the current iterate, \trl{x},
as is typically done with
\begin{verse}
  SNESSetJacobian(SNES snes,Mat A,Mat B,int (*FormJacobian)(SNES snes,\\
          Vec x,Mat *A,Mat *B,MatStructure *flag,void *ctx),void *ctx);
\end{verse}
The \findex{SNESSetJacobian()} arguments of the routine \trl{
FormJacobian()} are the current iterate, \trl{x}; the Jacobian matrix,
\trl{A}; the preconditioner matrix, \trl{B} (which is usually the same
as \trl{A}); a \trl{flag} indicating information about the
preconditioner matrix structure; and an optional user-defined Jacobian
context, \trl{ctx}, for application-specific data.  The options for
\trl{flag} are identical to those for the flag of \trl{
SLESSetOperators()}, discussed in Section~\ref{sec:usingsles}.  
Note that the SNES solvers are all data-structure neutral, so the full
range of PETSc matrix formats (including ``matrix-free''
methods) can be used.  Chapter~\ref{chapter:matrices} discusses
information regarding available matrix formats and options, while
Section~\ref{sec:nlmatrixfree} focuses on matrix-free
methods in SNES. We briefly touch on a few details of matrix usage that are
particularly important for efficient use of the nonlinear solvers.

During successive calls to \trl{FormJacobian()}, the user can either
insert new matrix contexts or reuse old ones, depending on the
application requirements. For many sparse matrix formats, reusing the
old space (and merely changing the matrix elements) is more efficient;
however, if the matrix structure completely changes, creating an
entirely new matrix context may be preferable.  
Upon subsequent calls to the 
\trl{FormJacobian()} routine, the user may wish to reinitialize the matrix
entries to zero by calling \trl{MatZeroEntries()}.  See
Section~\ref{sec:othermat} for details on the reuse of the matrix
context.

If the preconditioning matrix retains identical nonzero structure
during successive nonlinear iterations, setting the parameter, \trl{flag},
in the \trl{FormJacobian()} routine to be \trl{SAME_NONZERO_PATTERN} 
\findex{SAME_NONZERO_PATTERN} and reusing the matrix context can save
considerable overhead.  For example, when one is using a parallel
preconditioner such as incomplete factorization in solving the
linearized Newton systems for such problems, matrix colorings and
communication patterns can be determined a single time and then reused
repeatedly throughout the solution process.  In addition, if using
different matrices for the actual Jacobian and the preconditioner, the
user can hold the preconditioner matrix fixed for multiple iterations
by setting \trl{flag} to \trl{SAME_PRECONDITIONER}.  See the
discussion of \trl{SLESSetOperators()} in Section~\ref{sec:usingsles} for
details.

The directory \trl{${PETSC_DIR}/src/snes/examples/tutorials} provides
a variety of examples.


\section{The Nonlinear Solvers}
\label{sec:nlsolvers}

As summarized in Table~\ref{tab:snesdefaults}, SNES includes several
Newton-like nonlinear solvers based on line search techniques and
trust region methods.  The methods for solving systems of nonlinear
equations employ the prefixes
\trl{SNES_EQ}.

Each solver may have associated with it a set of options, which can be
set with routines and options database commands provided for this
purpose.  A complete list can be found by consulting the manual pages
or by running a program with the \trl{-help} option; we discuss just a
few in the sections below.

\begin{table}
\begin{center}
\begin{tabular}{llll}
{\bf Method}    &{\bf SNES Type}& {\bf Options Name}    &{\bf Default Convergence Test}\\
\hline
Line search     & SNES\_EQ\_LS   & ls   & SNESConverged\_EQ\_LS()\\
Trust region    & SNES\_EQ\_TR   & tr   & SNESConverged\_EQ\_TR()\\
Test Jacobian   & SNES\_EQ\_TEST        & test  & \\
\hline
\end{tabular}
\end{center}
\label{tab:snesdefaults}
\caption{PETSc Nonlinear Solvers}
\end{table}

\subsection{Line Search Techniques} \sindex{line search}

The method \trl{SNES_EQ_NLS} (\trl{-snes_type ls}) provides a line
search Newton method for solving systems of nonlinear equations.  By
default, this technique employs cubic backtracking \cite{dennis:83}.
An alternative line search routine can be set with the command
\findex{SNESSetLineSearch()}
\begin{verse}
  SNESSetLineSearch(SNES snes,\\
          int (*ls)(SNES,Vec,Vec,Vec,Vec,double,double*,double*),void *lsctx);
\end{verse}
Other line search methods provided by PETSc are 
\trl{SNESQuadraticLineSearch()}, \trl{SNESNoLineSearch()}, and \trl{SNESNoLineSearchNoNorms()},
\findex{SNESNoLineSearch()} \findex{SNESNoLineSearchNoNorms()}
\findex{SNESQuadraticLineSearch()}
which can be set with the option
\trl{-snes_eq_ls [cubic, quadratic, basic, basicnonorms]}. \findex{-snes_eq_ls}
The line search routines involve several parameters, which are set
to defaults that are reasonable for many applications.  The user
can override the defaults by using the options
\trl{-snes_eq_ls_alpha <alpha>}, \findex{-snes_eq_ls_alpha}
\trl{-snes_eq_ls_maxstep <max>}, and \findex{-snes_eq_ls_maxstep}
\trl{-snes_eq_ls_steptol <tol>}. \findex{-snes_eq_ls_steptol}


\subsection{Trust Region Methods}\sindex{trust region}

The most basic trust region method in SNES for solving systems of nonlinear
equations, \trl{SNES_EQ_NTR} (\trl{-snes_type tr}), is taken from
the MINPACK project \cite{more84}. Several parameters can be set to
control the variation of the trust region size during the solution
process.  In particular, the user can control the initial trust region
radius, computed by
\[
  \Delta = \Delta_0 \| F_0 \|_2,
\]
by setting $ \Delta_0 $ via the option 
\trl{-snes_eq_tr_delta0 <delta0>}.

The default trust region method for unconstrained minimization, 
\trl{SNES_UM_NTR} (\trl{-snes_type umtr}), is based on the work of
Steihaug \cite{steihaug:83}.  This method uses the preconditioned
conjugate gradient method via the KSP solver \trl{KSPQCG} to determine
the approximate minimizer of the resulting quadratic at each nonlinear
iteration.  This formulation requires the use of a symmetric
preconditioner, where the currently available options are Jacobi,
incomplete Cholesky, and the null preconditioners, which can be set
with the options \trl{-pc_type jacobi}, \trl{-pc_type icc}, and \trl{
-pc_type none}, respectively.

\section{General Options}

This section discusses options and routines that apply to all SNES
solvers and problem classes.  In particular, we focus on convergence
tests, monitoring routines, and tools for checking derivative
computations.

\subsection{Convergence Tests}
\label{sec:snesconvergence}

Convergence of the nonlinear solvers can be detected in a variety of
ways; the user can even specify a customized test, as discussed
below. \sindex{convergence tests} The default convergence routines for
the various nonlinear solvers within SNES are listed in
Table~\ref{tab:snesdefaults}; see the corresponding manual pages for
detailed descriptions.  Each of these convergence tests involves
several parameters, which are set by default to values that should be
reasonable for a wide range of problems.  The user can customize the
parameters to the problem at hand by using some of the following
routines and options.

One method of convergence testing is
to declare convergence when the norm of the change in the solution
between successive iterations is less than some tolerance, \trl{stol}.
Convergence can also be determined based on the norm of the function
(or gradient for a minimization problem).  
Such a test can use either the absolute size of the
norm, \trl{atol}, or its relative decrease, \trl{rtol}, from an initial
guess.  The following routine sets these parameters, which are used
in many of the default SNES convergence tests: \findex{SNESSetTolerances()}
\begin{verse}
  SNESSetTolerances(SNES snes,double atol,double rtol,double stol,\\
          int its,int fcts);
\end{verse}
This routine also sets the maximum numbers of allowable
nonlinear iterations, \trl{its}, and function evaluations, \trl{fcts}.
The corresponding options database commands for setting these parameters
are \trl{-snes_atol <atol>}, \trl{-snes_rtol <rtol>}, \trl{-snes_stol <stol>},
\findex{-snes_atol} \findex{-snes_rtol}  \findex{-snes_stol}
\findex{-snes_max_it} \findex{-snes_max_funcs}
\trl{-snes_max_it <its>}, and \trl{-snes_max_funcs <fcts>}.
A related routine is \trl{SNESGetTolerances()}. \findex{SNESGetTolerances()}

Convergence tests for trust regions methods often use an additional
parameter that indicates the minimium allowable trust region radius.
The user can set this parameter with the option \trl{-snes_trtol <trtol>}
\findex{-snes_trtol} or with the routine
\begin{verse}
  SNESSetTrustRegionTolerance(SNES snes,double trtol);
\end{verse}
An additional parameter is sometimes used for unconstrained minimization
problems, namely the minimum function tolerance, \trl{ftol}, which can
be set with the option \trl{-snes_fmin <ftol>} \findex{-snes_fmin} or
with the routine
\begin{verse}
  SNESSetMinimizationFunctionTolerance(SNES snes,double ftol);
\end{verse}

Users can set their own customized convergence tests in SNES by using
the command \findex{SNESSetConvergenceTest()}
\begin{verse}
  SNESSetConvergenceTest(SNES snes,int (*test)(SNES snes,double xnorm,\\
                         double gnorm,double f,SNESConvergedReason reason,\\
                         void *cctx),void *cctx);
\end{verse}
The final argument of the convergence test routine, \trl{cctx},
denotes an optional user-defined context for private data.  When
solving systems of nonlinear equations, the arguments \trl{xnorm},
\trl{gnorm}, and \trl{f} are the current iterate norm, current step
norm, and function norm, respectively.  Likewise, when solving
unconstrained minimization problems, the arguments \trl{xnorm}, \trl{
gnorm}, and \trl{f} are the current iterate norm, current gradient
norm, and the function value. \trl{SNESConvergedReason} should be set positive
for convergence and negative for divergence. See \trl{include/petscsnes.h}
for a list of \trl{SNESConvergedReason}. \findex{SNESConvergedReason}

\subsection{Convergence Monitoring}
\label{sec:snesmonitor}

By default the SNES solvers run silently without displaying information
about the iterations. The user can initiate monitoring with the
command \findex{SNESSetMonitor()} 
\begin{verse}
  SNESSetMonitor(SNES snes,int (*mon)(SNES,int its,double norm,void* mctx),\\
                         void *mctx,int (*monitordestroy)(void *));
\end{verse}
The routine, \trl{mon}, indicates a user-defined monitoring routine,
where \trl{its} and \trl{mctx} respectively denote the iteration
number and an optional user-defined context for private data for the
monitor routine.  The argument \trl{norm} is the function norm (or
gradient norm for unconstrained minimization problems).

The routine set by \trl{SNESSetMonitor()} is called once after every
successful step computation within the nonlinear solver.  Hence, the
user can employ this routine for any application-specific computations
that should be done after the solution update. The option
\trl{-snes_monitor} \findex{-snes_monitor} activates the default
SNES monitor routine, \trl{SNESDefaultMonitor()}, \findex{SNESDefaultMonitor()}
while \trl{-snes_xmonitor} \findex{-snes_xmonitor} draws
a simple line graph of the residual norm's convergence.

Once can cancel hardwired monitoring routines for SNES at runtime with 
\trl{-snes_cancelmonitors}. \findex{-snes_cancelmonitors}

As the Newton method converges so that the residual norm is small,
say $ 10^{-10} $, many of the final digits printed with the \trl{-snes_monitor}
option are meaningless. Worse, they are different on different 
machines; due to different round-off rules used by, say, the IBM RS6000
and the Sun Sparc. This makes testing between different machines
difficult. The option \trl{-snes_smonitor} \findex{-snes_smonitor}
causes PETSc to print fewer of the digits of the residual norm 
as it gets smaller; thus on most of the machines it will always
print the same numbers making cross processor testing easier.

The routines \findex{SNESGetSolution()} \findex{SNESGetFunction}
\begin{verse}
  SNESGetSolution(SNES snes,Vec *x);\\
  SNESGetFunction(SNES snes,Vec *r,void *ctx,\\
                          int(**func)(SNES,Vec,Vec,void*));
\end{verse}
return the solution vector and function vector from a SNES context. 
These routines are useful, for instance, if the convergence test requires 
some property of the solution or function other than those passed with
routine arguments.

\subsection{Checking Accuracy of Derivatives}
\label{sec:snesderivs}

Since hand-coding routines for Jacobian and Hessian matrix evaluation
can be error prone, SNES provides easy-to-use support for checking
these matrices against finite difference versions.  In the simplest
form of comparison, users can employ the option \trl{-snes_type test}
to compare the matrices at several points.  Although not exhaustive,
this test will generally catch obvious problems.  One can compare the
elements of the two matrices by using the option \trl{
-snes_test_display} \findex{-snes_test_display}, which causes the two 
matrices to be printed to the screen.  \sindex{Jacobian, testing}

Another means for verifying the correctness of a code for Jacobian or
Hessian computation is running the problem with either the finite
difference or matrix-free variant, \trl{-snes_fd} or \trl{-snes_mf}.
see Section \ref{sec:fdmatrix} or Section \ref{sec:nlmatrixfree}). 
If a problem converges well
with these matrix approximations but not with a user-provided routine,
the problem probably lies with the hand-coded
matrix. \sindex{Jacobian, debugging} \sindex{Hessian, debugging}

\section{Inexact Newton-like Methods}

\sindex{inexact Newton methods}
Since exact solution of the linear Newton systems within (\ref{eq:n1}) 
and (\ref{eq:n2}) at each iteration can be costly, modifications 
are often introduced that significantly reduce these expenses and 
yet retain the rapid convergence of Newton's method.  Inexact or 
truncated Newton techniques approximately solve the linear systems 
using an iterative scheme.  In comparison with using direct methods 
for solving the Newton systems, iterative methods have the virtue 
of requiring little space for matrix storage and potentially saving 
significant computational work.  Within the class of inexact Newton 
methods, of particular interest are Newton-Krylov methods, where the 
subsidiary iterative technique for solving the Newton system is 
chosen from the class of Krylov subspace projection methods. 
Note that at runtime the user can set any of the linear solver
options discussed in Chapter~\ref{ch:sles}, such as 
\trl{-ksp_type <ksp_method>} and \trl{-pc_type <pc_method>},
to set the Krylov subspace and preconditioner methods.

Two levels of iterations occur for the inexact techniques, where 
during each global or outer Newton iteration a sequence of 
subsidiary inner iterations of a linear solver is performed.
Appropriate control of the accuracy to which the subsidiary 
iterative method solves the Newton system
at each global iteration is critical, since these 
inner iterations determine the asymptotic convergence rate for 
inexact Newton techniques.
While the Newton systems must be solved well enough to retain
fast local convergence of the Newton's iterates, use of excessive
inner iterations, particularly when $ \| \x_k - \x_* \| $ is large,
is neither necessary nor economical.
Thus, the number of required inner iterations typically increases
as the Newton process progresses, so that the truncated iterates
approach the true Newton iterates.

A sequence of nonnegative numbers $ \{\eta_k\} $ can be used to 
indicate the variable convergence criterion.
In this case, when solving a system of nonlinear equations, the 
update step of the Newton process remains unchanged, and direct 
solution of the linear system is replaced by iteration on the 
system until the residuals
\[  \rr_k^{(i)} =  \F'(\x_k) \Delta \x_k + \F(\x_k) \]
satisfy
\[  \frac{ \| \rr_k^{(i)} \| }{ \| \F(\x_k) \| } \leq \eta_k \leq \eta < 1. \]
Here $ \x_0 $ is an initial approximation of the solution, and
$ \| \cdot \| $ denotes an arbitrary norm in $ \Re^n $ .  

By default a constant relative convergence tolerance is used for
solving the subsidiary linear systems within the Newton-like methods
of SNES.  When solving a system of nonlinear equations, one can
instead employ the techniques of Eisenstat and Walker \cite{ew94}
to compute $ \eta_k $ at each step of the nonlinear solver by using the
option \trl{-snes_ksp_ew_conv} \findex{-snes_ksp_ew_conv}. In addition,
by adding one's own KSP convergence test (see Section 
\ref{section:convergencetests}), one can easily create one's own,
problem-dependent, inner convergence tests. 

% ---------------------------------------------------------------
\section{Matrix-Free Methods}
\label{sec:nlmatrixfree}

SNES fully supports matrix-free methods. The matrices specified in the
Jacobian evaluation routine need not be conventional
matrices; instead, they can point to the data required to implement a
particular matrix-free method.  The matrix-free variant is allowed
{\em only} when the linear systems are solved by an iterative method
in combination with no preconditioning (\trl{PCNONE} or \trl{-pc_type none}),
a user-provided preconditioner matrix, or a user-provided preconditioner
shell (\trl{PCSHELL}, discussed in Section~\ref{sec:pc}); that is,
obviously matrix-free methods cannot be used if a direct solver is to 
be employed. \sindex{matrix-free Jacobians} \findex{PCSHELL}

The user can create a matrix-free context for use within SNES with 
the routine
\begin{verse}
  MatCreateSNESMF(SNES snes,Vec x, Mat *mat);
\end{verse}
\findex{MatCreateSNESMF()}
This routine creates the data structures needed for the matrix-vector 
products that arise within Krylov space iterative methods~\cite{brownsaad:90}
by employing the matrix type \trl{MATSHELL}, \findex{MATSHELL}
discussed in Section~\ref{sec:matrixfree}.  The default SNES matrix-free
approximations can also be invoked with the command \trl{-snes_mf}. \findex{-snes_mf}
Or, one can retain the user-provided Jacobian preconditioner, but replace the 
user-provided Jacobian matrix with the default matrix free variant with the
option \trl{-snes_mf_operator}. \findex{-snes_mf_operator}

See also
\begin{verse}
  MatCreateMF(Vec x, Mat *mat);
\end{verse}
for users who need a matrix-free matrix but are not using SNES.

The user can set one parameter to control the Jacobian-vector
product approximation with the command
\begin{verse}
  MatSNESMFSetFunctionError(Mat mat,double rerror);
\end{verse}
\findex{MatSNESMFSetFunctionError()}
The parameter \trl{rerror} should be set to the square root of the 
relative error in the function evaluations, $e_{rel}$; the default is $ 10^{-8} $, 
which assumes that the functions are evaluated to full double precision accuracy. 
This parameter can also be set from the options database with 
\begin{verse}
   -snes\_mf\_err <err>
\end{verse}
\findex{-snes_mf_err}

SNES provides a way to register new routines to compute the h-differencing parameter;
see the manual page for MatSNESMFSetType() and MatSNESMFRegisterDynamic). \findex{MatSNESMFSetType()}
\findex{MatSNESMFRegisterDynamic)}. We currently provide two default routines accessible via
\begin{verse}
  -snes\_mf\_type <default or wp>
\end{verse}
For the default approach there is one ``tuning'' parameter, set with 
\begin{verse}
  MatSNESMFDefaultSetUmin(Mat mat,PetscReal umin);
\end{verse}
\findex{MatSNESMFDefaultSetUmin()}
This parameter, \trl{umin} (or $u_{min}$), is a bit involved; its default is 
$ 10^{-6} $ . The Jacobian-vector product is approximated via the formula
\[
    F'(u) a \approx \frac{F(u + h*a) - F(u)}{h}
\]
where $ h $ is computed via 
\begin{eqnarray*}
        h = e_{rel}*u^{T}a/||a||^2_2                       &    \hbox{if}  |u'a| > u_{min}*||a||_{1} \\
          = e_{rel}*u_{min}*sign(u^{T}a)*||a||_{1}/||a||^2_2  &    \hbox{otherwise}.
\end{eqnarray*}
This approach is taken from Brown and Saad \cite{brownsaad:90}.
The parameter can also be set from the options database with 
\begin{verse}
   -snes\_mf\_umin <umin>
\end{verse}
\findex{-snes_mf_umin}

The second approach, taken from Walker and Pernice, \cite{pw98}, computes $ h $ via
\begin{eqnarray*}
        h = \frac{\sqrt{1 + ||u||}e_{rel}}{||a||}
\end{eqnarray*}
This has no tunable parameters, but note that (a) for GMRES with left preconditioning
$ ||a|| = 1 $ and (b) for the entire {\bf linear} iterative process $ u $ does not change hence
$\sqrt{1 + ||u||} $ need be computed only once. This information may be set with the 
options
\begin{verse}
  MatSNESMFWPSetComputeNormA(Mat mat,PetscTruth);\\
  MatSNESMFWPSetComputeNormU(Mat mat,PetscTruth);
\end{verse}
or 
\begin{verse}
   -snes\_mf\_compute\_norma <true or false>\\
   -snes\_mf\_compute\_normu <true or false>
\end{verse}
This information is used to eliminate the redundant computation of these parameters,
therefor reducing the number of collective operations and improving the efficiency of the 
application code.

It is also possible to monitor the differencing parameters h that are computed
via the routines
\begin{verse}
   MatSNESMFSetHHistory(Mat,PetscScalar *,int);\\
   MatSNESMFResetHHistory(Mat,PetscScalar *,int);\\
   MatSNESMFGetH(Mat,PetscScalar *);\\
   MatSNESMFKSPMonitor(KSP,int,double,void *);
\end{verse}
and the runtime option  \trl{-snes_mf_ksp_monitor} \findex{snes_mf_ksp_monitor}

We include an example in Figure~\ref{fig:snesexample2} that explicitly
uses a matrix-free approach.  Note that by using the option 
\trl{-snes_mf} one can easily convert any SNES code to use a matrix-free
Newton-Krylov method without a preconditioner.  As shown in this
example, \trl{SNESSetFromOptions()} must be called {\em after}
\trl{SNESSetJacobian()} to enable runtime switching between the
user-specified Jacobian and the default SNES matrix-free form.

Table~\ref{tab:jacobians} summarizes the various matrix situations
that SNES supports.  In particular, different linear system matrices
and preconditioning matrices are allowed, as well as both matrix-free
and application-provided preconditioners.  All combinations are
possible, as demonstrated by the example, 
\trl{${PETSC_DIR}/src/snes/examples/tutorials/ex5.c},
 in Figure~\ref{fig:snesexample2}.

\begin{center}
\begin{table}[H]
\begin{tabular}{|c|l|l|} \hline
& & \\
{\bf Matrix Use}      & {\bf Conventional Matrix Formats}          & {\bf Matrix-Free Versions}\\ 
& & \\ \hline
& & \\
Jacobian        & Create matrix with MatCreate(). $ ^* $ & Create matrix with MatCreateShell().\\
(or Hessian)    & Assemble matrix with user-defined     & Use MatShellSetOperation() to set\\
Matrix          & routine. $ ^\dagger $                         & various matrix actions.\\
                &                                    & Or use MatCreateSNESMF().\\
& & \\ \hline
& & \\
Preconditioning  & Create matrix with MatCreate(). $ ^* $ & Use SNESGetSLES() and SLESGetPC() \\
Matrix           & Assemble matrix with user-defined & to access the PC, then use\\
                & routine. $ ^\dagger $         & PCSetType(pc,PCSHELL);\\ 
                &             & followed by PCSetApply(). \\

& & \\ \hline
\end{tabular}

\medskip
$ ^* $ Use either the generic \trl{MatCreate()} or a format-specific variant
   such as \trl{MatCreateMPIAIJ()}.\\
$ ^\dagger $ Set user-defined matrix formation routine with \trl{SNESSetJacobian()} or
   \trl{SNESSetHessian()}.
\medskip
\caption{Jacobian Options}
\label{tab:jacobians}
\end{table}
\end{center}

\begin{figure}[H]
{\small
\fileinclude{../../../src/snes/examples/tutorials/ex6.c}
}
\caption{Example of Uniprocessor SNES Code - Both Conventional and Matrix-Free Jacobians}
\label{fig:snesexample2}
\end{figure} 

% ---------------------------------------------------------------
\section{Finite Difference Jacobian Approximations}
\label{sec:fdmatrix}

PETSc provides some tools to help approximate the Jacobian matrices efficiently via 
finite differences.  These tools are intended for use in certain situations where
one is unable to compute Jacobian matrices analytically, and matrix-free methods
do not work well without a preconditioner, due to very poor conditioning.  
The approximation requires several steps:\sindex{coloring with SNES}
\begin{itemize}
\item First, one colors the columns of the (not yet built) Jacobian matrix, so that 
      columns of the same color do not share any common rows.
\item Next, one creates a \trl{MatFDColoring} data structure that will be used later in 
      actually computing the Jacobian. \findex{MatFDColoring}
\item Finally, one tells the nonlinear solvers of SNES to use the
      \trl{SNESDefaultComputeJacobianColor()}
      routine to compute the Jacobians. \findex{SNESDefaultComputeJacobianColor()}
\end{itemize}
A code fragment that demonstrates this process is given below.
\begin{verbatim}
   ISColoring    iscoloring;
   MatFDColoring fdcoloring;
   MatStructure  str;

   /* 
      This initializes the nonzero structure of the Jacobian. This is artificial
      because clearly if we had a routine to compute the Jacobian we wouldn't
      need to use finite differences.
   */
   FormJacobian(snes,x,&J,&J,&str,&user);

   /*
       Color the matrix, i.e. determine groups of columns that share no common 
      rows. These columns in the Jacobian can all be computed simulataneously.
   */
   MatGetColoring(J,MATCOLORING_SL,&iscoloring);

   /*
       Create the data structure that SNESDefaultComputeJacobianColor() uses
       to compute the actual Jacobians via finite differences.
   */
   MatFDColoringCreate(J,iscoloring,&fdcoloring);
   ISColoringDestroy(iscoloring);
   MatFDColoringSetFromOptions(fdcoloring);

   /*
      Tell SNES to use the routine SNESDefaultComputeJacobianColor()
      to compute Jacobians.
   */
   SNESSetJacobian(snes,J,J,SNESDefaultComputeJacobianColor,fdcoloring);

\end{verbatim}

\findex{MatGetColoring()} \findex{MatFDColoringCreate()} 
\findex{MatFDColoringSetFromOptions()} \findex{ISColoringDestroy()}

Of course, we are cheating a bit. If we do not have an analytic
formula for computing the Jacobian, then how do we know what its
nonzero structure is so that it may be colored?  Determining the
structure is problem dependent, but fortunately, for most grid-based
problems (the class of problems for which PETSc is designed) if one
knows the stencil used for the nonlinear function one can usually
fairly easily obtain an estimate of the location of nonzeros in
the matrix.

One need not necessarily use the routine \trl{MatGetColoring()} to
determine a coloring.  For example, if a grid can be colored directly
(without using the associated matrix), then that coloring can be provided
to \trl{MatFDColoringCreate()}.  Note that the user must always
preset the nonzero structure in the matrix regardless of which
coloring routine is used.

For sequential matrices PETSc provides three matrix coloring routines from the 
MINPACK package \cite{more84}: smallest-last (\trl{sl}), largest-first (\trl{lf}),
and incidence-degree (\trl{id}).  These colorings, as well as the ``natural'' coloring 
for which each column has its own unique color, may be accessed with the command line options
\begin{verse}
   -mat\_coloring [sl,id,lf,natural]
\end{verse}
Alternatively, one can set a coloring type of \trl{COLORING_SL}, \trl{
COLORING_ID}, \trl{COLORING_LF}, or \trl{COLORING_NATURAL} 
when calling \trl{MatGetColoring()}. \findex{-mat_coloring} \findex{MATCOLORING_SL} \findex{MATCOLORING_ID}
\findex{MATCOLORING_LF} \findex{MATCOLORING_NATURAL}

As for the matrix-free computation of Jacobians (see Section
\ref{sec:nlmatrixfree}), two parameters affect the accuracy of the
finite difference Jacobian approximation.  These are set with the command
\begin{verse}
  MatFDColoringSetParameters(MatFDColoring fdcoloring,double rerror,double umin);
\end{verse}
\findex{MatFDColoringSetParameters()}
The parameter \trl{rerror} is the square root of 
the relative error in the function evaluations, $e_{rel}$; the default is $ 10^{-8} $, which assumes
that the functions are evaluated to full double-precision accuracy. The 
second parameter, \trl{umin}, is a bit more involved; its default is 
$ 10e^{-8} $ .  Column $i$ of the Jacobian matrix (denoted by $F_{:i}$) is 
approximated by the formula
\[
    F'_{:i} \approx \frac{F(u + h*dx_{i}) - F(u)}{h}
\]
where $ h $ is computed via 
\begin{eqnarray*}
        h = e_{rel}*u_{i}             &    \hbox{if }  |u_{i}| > u_{min} \\
        h = e_{rel}*u_{min}*sign(u_{i})  &    \hbox{otherwise}.
\end{eqnarray*}
These parameters may be set from the options database with 
\begin{verse}
   -mat\_fd\_coloring\_err <err>\\
   -mat\_fd\_coloring\_umin <umin>
\end{verse}
\findex{-mat_fd_coloring_err} \findex{-mat_fd_coloring_umin}

Note that the \trl{MatGetColoring()} routine currently 
works only on sequential routines.  Extensions may be forthcoming. However,
if one can compute the coloring \trl{iscoloring} some other way, the routine
\trl{MatFDColoringCreate()} does work in parallel. An example of this for 
2D distributed arrays is given below that uses the utility routine
\trl{DAGetColoring()}. \findex{DAGetColoring()}

\begin{verse}
   DAGetColoring(da,IS\_COLORING\_GLOBAL,MATMPIAIJ,\&iscoloring,\&J);\\
   MatFDColoringCreate(J,iscoloring,\&fdcoloring); \\
   MatFDColoringSetFromOptions(fdcoloring);\\
   ISColoringDestroy(iscoloring);
\end{verse}

Note that the routine \trl{MatFDColoringCreate()} currently is only 
supported for the AIJ matrix format.

% ---------------------------------------------------------------
\chapter{TS: Scalable ODE Solvers}
\label{chapter:ts}
\sindex{ODE solvers}

\findex{TS} 
The TS component provides a framework for the scalable solution of ODEs
arising from the discretization of time-dependent PDEs, and of
steady-state problems using pseudo-timestepping.

\vspace{.2cm}

\noindent
{\bf Time-Dependent Problems:} Consider the ODE
\[
              u_t = F(u,t),
\]
where $ u $ is a finite-dimensional vector, usually obtained from
discretizing a PDE with finite differences, finite elements, etc.
For example, discretizing the heat equation 
\[
          u_t = u_{xx} 
\]
with centered finite differences results in 
\[
          (u_i)_t = \frac{u_{i+1} - 2 u_{i} + u_{i-1}}{h^2}.
\]
The TS component provides code to solve these equations (currently 
using the forward or backward Euler method) as well as an interface to 
other sophisticated ODE solvers, in a clean and easy manner,
where the user need only provide code for the evaluation of $ F(u,t) $ and 
(optionally) its associated Jacobian matrix.

\vspace{.2cm}

\noindent
{\bf Steady-State Problems:} 
In addition, TS provides a general code for performing pseudo timestepping
with a variable timestep at each physical node point. For example, instead of
directly attacking the steady-state problem
\[
           F(u) = 0,
\]
we can use pseudo-transient continuation by solving
\[
           u_t = F(u).
\]
By using time differencing with the backward Euler method, we obtain
\[
           \frac{u^{n+1} - u^{n}}{dt^{n}} = F(u^{n+1}).
\]
More generally we can consider a diagonal matrix $ Dt^{n} $ that has a
pseudo-timestep for each node point to obtain the series of nonlinear equations
\[
        Dt^{n^{-1}}(u^{n+1} - u^{n}) =  F(u^{n+1}).
\]
For this problem the user must provide $ F(u) $ and the diagonal 
matrix $ Dt^{n} $, (or optionally, if the timestep is position independent,
a scalar timestep) as well as optionally the Jacobian of $ F(u).$ 

\section{Basic Usage}

The user first creates a TS object with the command
\begin{verse}
  int TSCreate(MPI\_Comm comm,TSProblemType problemtype,TS *ts);
\end{verse}
\findex{TSCreate()} The \trl{TSProblemType} \findex{TSProblemType}
is one of \trl{TS_LINEAR} or \trl{TS_NONLINEAR},
to indicate whether $ F(u,t) $ is given by a matrix $ A $, or $ A(t) $, or a
function $ F(u,t). $

One can set the solution method with the routine
\begin{verse}
 TSSetType(TS ts,TSType type);
\end{verse}
\findex{TSSetType()} Currently supported types are \trl{TS_EULER},
\trl{TS_BEULER}, and \trl{TS_PSEUDO}
\findex{TS_EULER} \findex{TS_BEULER} \findex{TS_PSEUDO}
or the command line option
\trl{-ts_type euler, beuler, pseudo}. \findex{-ts_type} \sindex{Euler}
\sindex{backward Euler}


Set the initial time and timestep with the command
\begin{verse}
 TSSetInitialTimeStep(TS ts,double time,double dt);
\end{verse}
One \findex{TSSetInitialTimeStep} can change the timestep with the command
\begin{verse}
 TSSetTimeStep(TS ts,double dt);
\end{verse}
One \findex{TSSetTimeStep()} 
can \findex{TSGetTimeStep()} determine the current timestep with the routine
\begin{verse}
 TSGetTimeStep(TS ts,double* dt);
\end{verse}
Here, ``current'' refers to the timestep being used to attempt to
promote the solution form $ u^n $ to $ u^{n+1}. $

One sets the total number of timesteps to run or the total time to run 
(whatever is first) with the command \findex{TSSetDuration()}
\begin{verse}
 TSSetDuration(TS ts,int maxsteps,double maxtime);
\end{verse}
One sets up the timestep context with \findex{TSSetUp()}
\begin{verse}
 TSSetUp(TS ts);
\end{verse}
destroys it with \findex{TSDestroy()}
\begin{verse}
 TSDestroy(TS ts);
\end{verse}
and views it with \findex{TSView()}
\begin{verse}
 TSView(TS ts,PetscViewer viewer);
\end{verse}

\subsection{Solving Time-dependent Problems}
To set up TS for solving an ODE, one must set the following:

\begin{itemize}
\item Solution:\\
\begin{verse}
 TSSetSolution(TS ts, Vec initialsolution);
\end{verse}
The vector \trl{initialsolution} should contain the ``initial conditions''
for the ODE. \findex{TSSetSolution()}

\item Function: \\
\subitem $ \bullet $ For linear functions (solved with implicit timestepping),
 the user must call 
\begin{verse}
 TSSetRHSMatrix(TS ts,Mat A, Mat B,int (*f)(TS,double,Mat*,Mat*,MatStructure*,void*),void *fP);
\end{verse}
The matrix \trl{B} (although usually the same as \trl{A}) allows one to 
provide \findex{TSSetRHSMatrix()}
a different matrix to be used in the construction of the preconditioner.
The function \trl{f} is used to form the matrices \trl{A} and \trl{B} 
at each timestep if the matrices are time dependent.
If the matrix does not depend on time, the user should 
pass in \trl{PETSC_NULL} for \trl{f}.  The variable \trl{fP} allows 
users to pass in an application context that is passed to the \trl{f()} function 
whenever it is called, as the final argument. The user must provide the matrices
\trl{A} and \trl{B}; if they have the right-hand side only as a linear
function, they must construct a \trl{MatShell} matrix. Note that this is 
the same interface as that for \trl{SNESSetJacobian()}. \findex{SNESSetJacobian()}

\subitem $ \bullet $  For nonlinear problems (or linear problems solved using
explicit timestepping methods) the user  passes the function with 
the routine

\begin{verse}
 TSSetRHSFunction(TS ts,int (*f)(TS,double,Vec,Vec,void*),void *fP);
\end{verse}
The \findex{TSSetRHSFunction} arguments to the function \trl{f()} are
the timestep context, the current time, the input for the function,
the output for the function, and the (optional) user-provided context
variable \trl{fP}.

\item Jacobian: For nonlinear problems the user must also provide the 
(approximate) Jacobian matrix of \trl{F(u,t)} and a function to
compute it at each Newton iteration. This is done with the command
\begin{verse}
 TSSetRHSJacobian(TS ts,Mat A, Mat B,int (*f)(TS,double,Vec,Mat*,Mat*,\\
                          MatStructure*,void*),void *fP);
\end{verse}
The \findex{TSSetRHSJacobian} arguments for the function \trl{f()} are
the timestep context, the current time, the location where the
Jacobian is to be computed, the Jacobian matrix, an alternative
approximate Jacobian matrix used as a preconditioner, and the optional
user-provided context, passed in as \trl{fP}. The user must provide the 
Jacobian as a matrix; thus, if using a matrix-free approach is used, the
user must create a \trl{MatShell} matrix. Again, note the similarity
to \trl{SNESSetJacobian()}. \findex{SNESSetJacobian()}
\end{itemize}

Similar to \trl{SNESDefaultComputeJacobianColor()} is the
routine \trl{TSDefaultComputeJacobianColor()} and \trl{TSDefaultComputeJacobian()} that 
corresponds to \trl{SNESDefaultComputeJacobian()}. \findex{TSDefaultComputeJacobianColor()}
\sindex{coloring with TS} \findex{TSDefaultComputeJacobian()}

\subsection{Using PVODE from PETSc}

PVODE is a parallel ODE solver developed by Hindmarsh et al. at LLNL.
The TS component provides an interface to use PVODE directly from PETSc.
(To install PETSc to use PVODE, see the installation guide, \trl{docs/installation/index.htm}.) 

To use the PVODE integrators, call 
\begin{verse}
 TSSetType(TS ts,TSType TS\_PVODE);
\end{verse}
or use the command line option \trl{-ts_type pvode}. \findex{TS_PVODE} 
\sindex{PVODE} \sindex{Hindmarsh} \sindex{ODE solvers}

PVODE comes with to main integrator families, Adams and BDF (backward 
differentiation formula). One can select these with 
\begin{verse}
 TSPVodeSetType(TS ts,TSPVodeType [PVODE\_ADAMS,PVODE\_BDF]);
\end{verse}
or the command line option \trl{-ts_pvode_type <adams,bdf>}. BDF is
the default. \findex{-ts_pvode_type} \findex{TSPVodeType}
\findex{TSPVodeSetType()} \sindex{Adams} \sindex{BDF}

PVODE does not use the SNES component of PETSc for its nonlinear
solvers, so one cannot change the nonlinear solver options via
SNES. Rather, PVODE uses the preconditioners within the PC component
of PETSc, which can be accessed via
\begin{verse}
 TSPVodeGetPC(TS ts,PC *pc);
\end{verse}
The user can then directly set preconditioner options; 
alternatively, the usual runtime options can be employed
via \trl{-pc_xxx}. \findex{TSPVodeGetPC()}

Finally, one can set the PVODE tolerances via
\begin{verse}
 TSPVodeSetTolerance(TS ts,double abs,double rel);
\end{verse}
where \trl{abs} denotes the absolute tolerance and \trl{rel}
the relative tolerance. \findex{TSPVodeSetTolerance()}

Other PETSc-PVode options include
\begin{verse}
  TSPVodeSetGramSchmidtType(TS ts,TSPVodeGramSchmidtType type);
\end{verse}
where \trl{type} is either \trl{PVODE_MODIFIED_GS} or 
\trl{PVODE_UNMODIFIED_GS}. \findex{TSPVodeSetGramSchmidtType()}
\findex{TSPVodeGramSchmidtType} \findex{PVODE_MODIFIED_GS}
\findex{PVODE_UNMODIFIED_GS} This may be set via the options data base
with \trl{-ts_pvode_gramschmidt_type <modifed,unmodified>}.
\findex{-ts_pvode_gramschmidt_type}

The routine 
\begin{verse}
  TSPVodeSetGMRESRestart(TS ts,int restart);
\end{verse}
sets the number of vectors in the Krylov subpspace used by GMRES.
\findex{TSPVodeSetGMRESRestart()} This may be set in the options 
database with \trl{-ts_pvode_gmres_restart restart}. \findex{-ts_pvode_gmres_restart}


\subsection{Solving Steady-State Problems with Pseudo-Timestepping}

For solving steady-state problems with pseudo-timestepping one proceeds 
as follows.
\begin{itemize}
\item Provide the function \trl{F(u)} with the routine
\begin{verse}
 TSSetRHSFunction(TS ts,int (*f)(TS,double,Vec,Vec,void*),void *fP);
\end{verse}
The \findex{TSSetRHSFunction} arguments to the function \trl{f()} are
the timestep context, the current time, the input for the function,
the output for the function and the (optional) user-provided context
variable \trl{fP}.

\item Provide the (approximate) Jacobian matrix of \trl{F(u,t)} and a 
function to compute it at each Newton iteration. This is done with the command
\begin{verse}
 TSSetRHSJacobian(TS ts,Mat A, Mat B,int (*f)(TS,double,Vec,Mat*,Mat*,\\
                          MatStructure*,void*),void *fP);
\end{verse}
The \findex{TSSetRHSJacobian} arguments for the function \trl{f()} are
the timestep context, the current time, the location where the
Jacobian is to be computed, the Jacobian matrix, an alternative
approximate Jacobian matrix used as a preconditioner, and the optional
user-provided context, passed in as \trl{fP}. The user must provide the 
Jacobian as a matrix; thus, if using a matrix-free approach, one 
must create a \trl{MatShell} matrix.
\end{itemize}

In addition, the user must provide a routine that computes the 
pseudo-timestep. This is slightly different depending on if 
one is using a constant timestep over the entire grid, or it varies
with location. 
\begin{itemize}
\item For location-independent pseudo-timestepping, one uses the routine 
\begin{verse}
 TSPseudoSetTimeStep(TS ts,int(*dt)(TS,double*,void*),void* dtctx);
\end{verse}
The function \trl{dt} is a user-provided function that computes the next 
pseudo-timestep. As a default one can use
\trl{TSPseudoDefaultTimeStep(TS,double*,void*)} for \trl{dt}. This routine
updates the pseudo-timestep with one of two strategies: the default
\[
   dt^{n} = dt_increment*dt^{n-1}*\frac{|| F(u^{n-1}) ||}{|| F(u^{n})||}
\]
or, the alternative, 
\[
   dt^{n} = dt_increment*dt^{0}*\frac{|| F(u^{0}) ||}{|| F(u^{n})||}
\]
which can be set with the call
\begin{verse}
 TSPseudoIncrementDtFromInitialDt(TS ts);
\end{verse}
or \findex{TSPseudoIncrementDtFromInitialDt()} 
the option \trl{-ts_pseudo_increment_dt_from_initial_dt}. 
\findex{-ts_pseudo_increment_dt_from_initial_dt}
The value $ dt_increment $ is by default $ 1.1$, but can be reset with the 
call 
\begin{verse}
 TSPseudoSetTimeStepIncrement(TS ts,double inc); 
\end{verse}
or \findex{TSPseudoSetTimeStepIncrement()} the option 
\trl{ -ts_pseudo_increment <inc>}. \findex{-ts_pseudo_increment}


\item For location-dependent pseudo-timestepping, the interface function
      has not yet been created.
\end{itemize}

\chapter{High Level Support for Multigrid with DMMG}
\label{chapter:dmmg}

PETSc provides an easy to use high-level interface for multigrid on a
single structured grid using the PETSc DA object (or the VecPack
object) to decompose the grid across the processors.  This DMMG code
\findex{DMMG} is built on top of the lower level PETSc multigrid
interface provided in the \trl{PCType} of MG, see Section
\ref{sec:mg}. Currently we only provide piecewise linear and piecewise
constant interpolation, but can add more if needed. The DMMG routines
only provide linear multigrid but they can be used easily with either
SLES (for linear problems) or SNES (for nonlinear problems).

For linear problems the examples \trl{src/sles/examples/tutorials/ex22.c} and \trl{ex25.c} can be used
to guide your development. We give a short summary here.
\begin{verse}
  DMMG        *dmmg;\\
  DA          da;\\

  /* Create the DA that stores information about the coarsest grid you wish to use */\\
  ierr = DACreate3d(PETSC\_COMM\_WORLD,DA\_NONPERIODIC,DA\_STENCIL\_STAR,
         3,3,3,PETSC\_DECIDE,PETSC\_DECIDE,PETSC\_DECIDE,1,1,0,0,0,\&da);CHKERRQ(ierr);  \\

  /* Create the DMMG data structure \\
       - the second argument indicates the number of levels you wish to use and \\
         can be changed with the option -dmmg\_nlevels \\
  ierr = DMMGCreate(PETSC\_COMM\_WORLD,3,PETSC\_NULL,\&dmmg);CHKERRQ(ierr);\\

  /*  Tell the DMMG object to use the da to define the coarsest grid\\
  ierr = DMMGSetDM(dmmg,(DM)da);\\

  /*  Tell the DMMG we are solving a linear problem (hence SLES) and provide the callback function to 
      compute the right hand side and matrices for each level\\
  ierr = DMMGSetSLES(dmmg,ComputeRHS,ComputeJacobian);CHKERRQ(ierr);\\

  /*  Solve the problem */\\
  ierr = DMMGSolve(dmmg);CHKERRQ(ierr);\\

  /*  One can access the solution with \\
  DMMGGetx(dmmg)\\

  ierr = DMMGDestroy(dmmg);CHKERRQ(ierr);\\
  ierr = DADestroy(da);CHKERRQ(ierr);\\
\end{verse}
\findex{DMMGCreate()} \findex{DMMGSolve()} \findex{DMMGGetx()} \findex{DMMGDestroy()} \findex{DMMGSetDM()} \findex{DMMGSetDA()}
The option \trl{-dmmg_ksp_monitor} \findex{-dmmg_ksp_monitor} 
 causes the DMMG code to print the residual norms for each level of the solver to the screen so that
the coarser the grid the more indented the print out. The option \trl{-dmmg_grid_sequence} \findex{-dmmg_grid_sequence} causes the 
DMMG solve to use grid sequencing to generate the initial guess by solving the same problem on the previous coarser grid; this often
results in a much faster time to solution.

The solver (smoother) used on each level but the coarsest can be controled via the options
database with any PC or KSP option prefixed as \trl{-mg_levels_[pc/ksp]_}. The 
solver options on the finest grid can be set with  \trl{-mg_coarse_[pc/ksp]_}. The 
DMMG has many other options that can view by running the DMMG program with the option \trl{-help}.
You should generally run your code with the option \trl{-sles_view} to see exactly what 
solvers are being used.

For nonlinear problems one replaces the \trl{DMMGSetSLES()} with 
\begin{verse}
  DMMGSetSNES(DMMG *dmmg,int (*function)(SNES,Vec,Vec,void*),\\
  int (*jacobian)(SNES,Vec,Mat*,Mat*,MatStructure*,void*))
\end{verse}
or the prefered approach
\begin{verse}
  DMMGSetSNESLocal(DMMG *dmmg,\\
   int (*localfunction)(DALocalInfo *info,void *x,void *f,void* appctx),int (*localjacobian)(DALocalInfo *,void *x,Mat J,void *appctx),\\
  ad\_function,ad\_mf\_function);
\end{verse}
The \trl{ad_function} and \trl{ad_mf_function} are described in the next chapter. \findex{DMMGSetSNES()} \findex{DMMGSetSNESLocal()}
See examples \trl{src/snes/examples/tutorials/ex18.c} and \trl{ex19.c} for complete details.

The \trl{localfunction} \trl{x} and \trl{f} arguments are, for scalar
problems, (problems with one degree of freedom per node) simply
multi-dimensional arrays of double precision (or complex) numbers
(according to the dimension of the grid) that should be indexed using
{\em global} i, j, k indices on the entire grid. For multi-component
problems you must create a C struct with an entry for each component
and the \trl{x} and \trl{f} arguments are appropriately dimensioned
arrays of that struct. For example, for a 3d scalar problem the function
would be 
\begin{verse}
int localfunction(DALocalInfo *info,double ***x, double ***f,void *ctx)
\end{verse}
For a 2d multicomponent problem with u, v, and p components one would write
\begin{verse}
typedef struct {
  PetscScalar u,v,p;
} Field;
...
int localfunction(DALocalInfo *info,Field **x,Field **f,void *ctx)
\end{verse}

For many nonlinear problems it is too difficult to compute the
Jacobian analytically, thus if \trl{jacobian} or \trl{localjacobian} is not
provided, (indicated by passing in a \trl{PETSC_NULL}) the \trl{DMMG} will compute the
sparse Jacobian reasonably efficiently automatically using finite
differencing. See the next chapter on computing the Jacobian via
automatic differentiation. The option \trl{-dmmg_jacobian_mf_fd}
causes the code to not compute the Jacobian explicitly but rather to
use differences to apply the matrix vector product of the Jacobian.

The option \trl{-dmmg_snes_monitor} \findex{-dmmg_snes_monitor} can
be used to monitor the progress of the nonlinear solver. The usual \trl{-snes_} options
may be used to control the nonlinearr solves. Again we recommend using the 
option \trl{-dmmg_grid_sequence} and \trl{-snes_view} for most runs.


\chapter{Using ADIC and ADIFOR with PETSc}

Automatic differentiation is an incredible technique to generate code
that computes Jacobians and other differentives directly from code
that only evaluates the function. For structured grid problems, via
the DMMG interface, \ref{chapter:dmmg}, PETSc provides a way to use
ADIFOR and ADIC to compute the sparse Jacobians or perform matrix free
vector products with them. See
\trl{src/snes/examples/tutorials/ex18.c} and \trl{ex5f.F} for example
usage. 

First one indicates the functions for which one needs Jacobians by adding
in the comments in the code 
\begin{verse}
       Process adiC: FormFunctionLocal FormFunctionLocali
\end{verse}
where one lists the functions. In Fortran use
\begin{verse}
!     Process adifor: FormFunctionLocal
\end{verse}
Next one uses the call
\begin{verse}
  DMMGSetSNESLocal(DMMG *dmmg,\\
   int (*localfunction)(DALocalInfo *info,void *x,void *f,void* appctx),PETSC\_NULL,
  ad\_localfunction,ad\_mf\_localfunction);
\end{verse}
where the names of the last two functions are obtained 
by prepending a \trl{ad_} and \trl{ad_mf_} in front
of the function name. In Fortran in preppends a \trl{g_} and \trl{m_}.

Two useful options are \trl{-dmmg_jacobian_mf_ad} and \trl{-dmmg_jacobian_mf_ad_operator},
with the former is uses the matrix-free automatic differentiation to apply the operator
and to define the preconditioner operator. The latter form uses the matrix-free for the 
matrix-vector product but still computes the Jacobian (by default with finite differences)
used to construct the precondtioner.

\section{Work arrays inside the local functions} In C you can call \trl{DAGetArray()} to get 
work arrays (this is low overhead). In Fortran you can provide a \trl{FormFunctionLocal()}
that had local arrays that have hardwired sizes that are large enough or somehow allocate
space and pass it into an inner FormFunctionLocal() that is the one you differentiate; this
second approach will require some hand massaging. For example,

\begin{verse}
  subroutine TrueFormFunctionLocal(info,x,f,ctx,ierr)\\
  double precision x(gxs:gxe,gys:gye),f(xs:xe,ys:ye)\\
  DA info(DA\_LOCAL\_INFO\_SIZE)\\
  integer ctx,ierr\\
  double precision work(gxs:gxe,gys:gye)\\
   
  .... do the work ....


  return
   
  subroutine FormFunctionLocal(info,x,f,ctx,ierr)

  double precision x(*),f(*)\\
  DA info(DA\_LOCAL\_INFO\_SIZE)\\
  integer ctx,ierr\\
  double precision work(10000)\\

  call TrueFormFunctionLocal(info,x,f,work,ctx,ierr)\\
  return
\end{verse}

%------------------------------------------------------------------
\chapter{Using Matlab with PETSc}
\label{ch:matlab}\sindex{Matlab}

There are three basic ways to use Matlab with PETSc: (1) dumping files to 
be read into Matlab, (2) automatically sending data from a running PETSc program
to a Matlab process where you may interactively type Matlab commands (or run
scripts) and (3) automatically sending data back and forth between PETSc and 
Matlab where Matlab commands are issued not interactively but from a script or
the PETSc program.

\section{Dumping Data for Matlab}
One can dump PETSc matrices and vectors to the screen (and thus save in a file via
\trl{> filename.m}) in a format that Matlab can read in directly. This is done with the
command line options \trl{-vec_view_matlab} or \trl{-mat_view_matlab}. This causes the PETSc program
to print the vectors and matrices every time a VecAssemblyXXX() and MatAssemblyXXX()
is called. \findex{-vec_view_matlab} \findex{-mat_view_matlab} To provide finer control
over when and what vectors and matrices are dumped one can use the VecView() and 
MatView() functions with a viewer type of ASCII (see \trl{PetscViewerASCIIOpen()}, 
\trl{PETSC_VIEWER_STDOUT_WORLD}, \trl{PETSC_VIEWER_STDOUT_SELF}, or \trl{PETSC_VIEWER_STDOUT_(MPI_Comm)}). Before calling
the viewer set the output type with, for example, 
\begin{verse}
  PetscViewerSetFormat(PETSC\_VIEWER\_STDOUT\_WORLD,PETSC\_VIEWER\_ASCII\_MATLAB);\\
  VecView(A,PETSC\_VIEWER\_STDOUT\_WORLD);
\end{verse}
or 
\begin{verse}
  PetscViewerPushFormat(PETSC\_VIEWER\_STDOUT\_WORLD,PETSC\_VIEWER\_ASCII\_MATLAB);\\
  MatView(B,PETSC\_VIEWER\_STDOUT\_WORLD);
\end{verse}
The name of each PETSc variable printed for Matlab may be set with
\begin{verse}
PetscObjectSetName((PetscObject)A,''name'');
\end{verse}

\section{Sending Data to Interactive Running Matlab Session}

One creates a viewer to Matlab via 
\begin{verse}
PetscViewerSocketOpen(MPI\_Comm,char *machine,int port,PetscViewer *v);
\end{verse}
(port is usally set to \trl{PETSC_DEFAULT}, use \trl{PETSC_NULL} for the machine if the 
Matlab interactive session is running on the same machine as the PETSc program) 
and then sends matrices or vectors via
\begin{verse}
  VecView(Vec A,v);\\
  MatView(Mat B,v);
\end{verse}
One can also send arrays or integer arrays via
\begin{verse}
  PetscViewerSocketPutScalar(v,int m,int n,PetscScalar *array);\\
  PetscViewerSocketPutReal(v,int m,int n,double *array);\\
  PetscViewerSocketPutInt(v,int m,int *array);
\end{verse}
One may start the Matlab program manually or use the PETSc command
\trl{PetscStartMatlab(MPI_Comm,char *machine,char *script,FILE **fp);} where machine and script may be \trl{PETSC_NULL}.

To receive the objects in Matlab you must first make sure that \trl{${PETSC_DIR}/bin/matlab}
is in your Matlab path. Use \trl{p = openport;} (or \trl{p = openport(portnum)} if you provided a port number in
your call to \trl{PetscViewerSocketOpen()}), then \trl{a = receive(p);} returns the object you have passed from PETSc.
\trl{receive()} may be called any number of times. Each call should correspond on the PETSc side with
viewing a single vector or matrix. You many call \trl{closeport()} to close the connection from Matlab.
It is also possible to start your PETSc program from Matlab via \trl{launch()}.

\section{Using the Matlab Compute Engine}

One creates access to the Matlab engine via 
\begin{verse}
  PetscMatlabEngineCreate(MPI\_Comm comm,char *machine,PetscMatlabEngine *e);
\end{verse}
(\trl{machine} may be \trl{PETSC_NULL}).
One can send objects to Matlab via 
\begin{verse}
  PetscMatlabEnginePut(PetscMatlabEngine e,PetscObject obj);
\end{verse}
One can get objects
via 
\begin{verse}
  PetscMatlabEngineGet(PetscMatlabEngine e,PetscObject obj);.
\end{verse}
Similarly one can send arrays via
\begin{verse}
  PetscMatlabEnginePutArray(PetscMatlabEngine e,int m,int n,PetscScalar *array,char *name);
\end{verse}
and get them back via
\begin{verse}
  PetscMatlabEngineGetArray(PetscMatlabEngine e,int m,int n,PetscScalar *array,char *name);
\end{verse}
One cannot use Matlab
interactively in this mode but you can send Matlab commands via
\begin{verse}
  PetscMatlabEngineEvaluate(PetscMatlabEngine,''format'',...);
\end{verse}
where \trl{format} has the usual \trl{printf()} format.
For example,
\begin{verse}
  PetscMatlabEngineEvaluate(PetscMatlabEngine,''x = %g *y + z;'',avalue);
\end{verse}
The name of each PETSc variable passed to Matlab may be set with
\begin{verse}
PetscObjectSetName((PetscObject)A,''name'');
\end{verse}

Text responses can be returned from Matlab via 
\begin{verse}
  PetscMatlabEngineGetOutput(PetscMatlabEngine,char **); 
\end{verse}
or
\begin{verse}
PetscMatlabEnginedPrintOutput(PetscMatlabEngine,FILE*).
\end{verse}
There is a short-cut to starting the Matlab engine
with \trl{PETSC_MATLAB_ENGINE_(MPI_Comm)}.


\chapter{Using ESI with PETSc}
\label{ch:esi} \findex{ESI} \findex{Equation Solver Interface}
The \trllink{http://z.ca.sandia.gov/esi}{Equation Solver Interface} 
(\trllink{http://www.eterascale.com/esi}{location of official headers}) 
is an attempt to 
define a common linear solver interface for a variety of scalable solver
package. It is currently only for C++ and defines a set of abstract
C++ classes (the header files for these classes are in \trl{include/esi}).

PETSc provides code that allows
\begin{itemize}
\item  ESI objects to be wrapped as PETSc objects
      and then be used with ``regular'' PETSc code and
\item PETSc objects to be wrapped as ESI objects and then used with other
      ESI code.
\end{itemize}
The wrapping does not involve data copies and thus is efficient.


%------------------------------------------------------------------
\chapter{PETSc Fortran Users}
\label{ch:fortran}

Most of the functionality of PETSc can be obtained by people who
program purely in Fortran 77 or Fortran 90.  
The PETSc Fortran interface works with both F77 and F90 compilers.

Since Fortran77 does not provide type checking of routine input/output
parameters, we find that many errors encountered within PETSc Fortran
programs result from accidentally using incorrect calling sequences.
Such mistakes are immediately detected during compilation when using
C/C++.  Thus, using a mixture of C/C++ and Fortran often works well
for programmers who wish to employ Fortran for the core numerical
routines within their applications.  In particular, one can
effectively write PETSc driver routines in C/C++, thereby preserving
flexibility within the program, and still use Fortran when desired for
underlying numerical computations.

\section{Differences between PETSc Interfaces for C and Fortran}

Only a few differences exist between the C and Fortran PETSc
interfaces, all of which are due to differences in Fortran syntax.
All Fortran routines have the same names as the corresponding C
versions, and PETSc command line options are fully supported. The
routine arguments follow the usual Fortran conventions; the user need
not worry about passing pointers or values.  The calling sequences
for the Fortran version are in most cases identical to the C version,
except for the error checking variable discussed in 
Section \ref{sec:fortran_errors} and a few routines listed in 
Section \ref{sec:fortran_exceptions}.

\subsection{Include Files}
\label{sec:fortran_includes}

The Fortran include files for PETSc are located in the directory 
\trl{${PETSC_DIR}/include/finclude} and should be used via statements 
such as the following:
\begin{verse}
    \#include "include/finclude/includefile.h"
\end{verse}
Since one must be very careful to include each file no more than once
in a Fortran routine, application programmers must manually include
each file needed for the various PETSc components within their
program.  This approach differs from the PETSc C/C++ interface, where
the user need only include the highest level file, for example, \trl{
petscsnes.h}, which then automatically includes all of the required lower
level files.  As shown in the examples of Section
\ref{sec:fortran-examples}, in Fortran one must explicitly list {\em
each} of the include files. One must employ
the Fortran file suffix \trl{.F}
rather than \trl{.f}.  This convention enables use of the CPP
preprocessor, which allows the use of the {\em \#include} statements
that define PETSc objects and variables. (Familarity with the CPP
preprocessor is not needed for writing PETSc Fortran code; one can simply
begin by copying a PETSc Fortran example and its corresponding
makefile.)  


\subsection{Error Checking}
\label{sec:fortran_errors}

In the Fortran version, each PETSc routine has as its final argument
an integer error variable, in contrast to the C convention of
providing the error variable as the routine's return value.  The error
code is set to be nonzero if an error has been detected; otherwise, it
is zero.  For example, the Fortran and C variants of \trl{SLESSolve()} are
given, respectively, below, where \trl{ierr} denotes the error variable:
\begin{verse}
   call SLESSolve(SLES sles,Vec b,Vec x,int its,int ierr)\\
  SLESSolve(SLES sles,Vec b,Vec x,int *its);
\end{verse}

Fortran programmers
can check these error codes with
\trl{CHKERRQ(ierr)}, which terminates all process when an error is
encountered.  Likewise, one can set error codes within Fortran programs by
using \trl{SETERRQ(ierr,p,' ')}, which again terminates all processes
upon detection of an error.  
Note that complete error tracebacks with
\trl{CHKERRQ()} and \trl{SETERRQ()}, as described in Section
\ref{sec:simple} for C routines, are {\em not} directly supported for
Fortran routines; however, Fortran programmers can easily use the
error codes in writing their own tracebacks.  For example, one could
use code such as the following:
\begin{verse}
   call SLESSolve(sles,x,y,ierr)\\
   if ( ierr .ne. 0) then\\
       print*, 'Error in routine ...'\\
       return\\
   endif
\end{verse}

The most common reason for crashing PETSc Fortran code is forgetting the 
final \trl{ierr} argument.

\subsection{Array Arguments}
\label{sec:fortranarrays}

Since Fortran does not allow arrays to be returned in routine
arguments, all PETSc routines that return arrays, such as 
\trl{VecGetArray()}, \trl{MatGetArray()}, 
\trl{ISGetIndices()}, and \trl{DAGetGlobalIndices()}
are defined slightly differently in Fortran than in C.  
\findex{VecGetArray()} \findex{MatGetArray()} 
\findex{ISGetIndices()} \findex{DAGetGlobalIndices()}
Instead of returning the array itself, these routines
accept as input a user-specified array of dimension one and return an
integer index to the actual array used for data storage within PETSc.
The Fortran interface for several routines is as follows:
\begin{verse}
   double precision xx\_v(1), aa\_v(1)\\
   integer          ss\_v(1), dd\_v(1), ierr, nloc\\
   PetscOffset      ss\_i, xx\_i, aa\_i, dd\_i\\
   Vec x\\
   Mat A\\
   IS  s\\
   DA  d

   call VecGetArray(x,xx\_v,xx\_i,ierr)\\
   call MatGetArray(A,aa\_v,aa\_i,ierr)\\
   call ISGetIndices(s,ss\_v,ss\_i,ierr)\\
   call DAGetGlobalIndices(d,nloc,dd\_v,dd\_i,ierr)
\end{verse}

To access array elements directly, both the user-specified array and
the integer index {\em must} then be used together.  
For example, the following Fortran program fragment illustrates
directly setting the values of a vector array instead of using \trl{
VecSetValues()}.  Note the (optional) use of the preprocessor 
\trl{#define} statement to enable array manipulations in the conventional
Fortran manner.
\begin{verse}
   \#define xx\_a(ib)  xx\_v(xx\_i + (ib))

    double precision xx\_v(1)\\
    PetscOffset      xx\_i\\
    integer          i, ierr, n\\
    Vec              x\\
    call VecGetArray(x,xx\_v,xx\_i,ierr)\\
    call VecGetLocalSize(x,n,ierr)\\
    do 10, i=1,n\\
       xx\_a(i) = 3*i + 1\\
 10 continue\\
    call VecRestoreArray(x,xx\_v,xx\_i,ierr)
\end{verse}
Figure \ref{fig:vec2-Fortran} contains an example of using \trl{VecGetArray()}
within a Fortran routine.

Since in this case the array is accessed directly from Fortran,
indexing begins with 1, not 0 (unless the array is declared as \trl{xx_v(0:1)}).
This is different from the use of \trl{VecSetValues()}
where, indexing always starts with 0.

{\em Note}: If using \trl{VecGetArray()}, \trl{MatGetArray()}, \trl{ISGetIndices()},
or \trl{DAGetGlobalIndices()}
from Fortran, the user {\em must not} compile the Fortran code with options 
to check for ``array entries out of bounds'' (e.g., on the IBM RS/6000 this 
is done with the \trl{-C} compiler option, so never use the \trl{-C} option with this).

\subsection{Calling Fortran Routines from C (and C Routines from Fortran)}


Different machines have
different methods of naming Fortran routines called from C 
(or C routines called from Fortran). Most Fortran compilers change
all the capital letters in Fortran routines to small. On some machines, the 
Fortran compiler appends an underscore to the end of each Fortran 
routine name; for example, the Fortran routine \trl{Dabsc()}
would be called from C with \trl{dabsc_()}.  Other machines
change all the letters in Fortran routine names to capitals. 

PETSc provides two macros (defined in C/C++) to help write 
portable code that mixes C/C++ and Fortran. They are 
\trl{PETSC_HAVE_FORTRAN_UNDERSCORE} and \trl{PETSC_HAVE_FORTRAN_CAPS}
\findex{PETSC_HAVE_FORTRAN_UNDERSCORE} \findex{PETSC_HAVE_FORTRAN_CAPS},
which are defined in the file \trl{${PETSC_DIR}/bmake/${PETSC_ARCH}/petscconf.h}.
The macros are used, for example, as follows:
\begin{verse}
   \#if defined(PETSC\_HAVE\_FORTRAN\_CAPS)\\
   \#define dabsc\_ DABSC\\
   \#elif !defined(PETSC\_HAVE\_FORTRAN\_UNDERSCORE)\\
   \#define dabsc\_ dabsc\\
   \#endif\\
   .....\\
   dabsc\_(\&n,x,y); /* call the Fortran function */
\end{verse}


\subsection{Passing Null Pointers}

In several PETSc C functions, one has the option of passing a 0 (null)
argument (for example, the fifth argument of \trl{MatCreateSeqAIJ()}).
From Fortran, users {\em must} pass \trl{PETSC_NULL_XXX} to indicate a
null argument (where XXX is \trl{INTEGER}, \trl{DOUBLE}, \trl{CHARACTER},
or \trl{SCALAR} depending on the type of argument required); 
\findex{PETSC_NULL_INTEGER} passing \findex{PETSC_NULL_SCALAR} 0 from 
\findex{PETSC_NULL_DOUBLE} Fortran \findex{PETSC_NULL_CHARACTER}  will crash
the code.   Note
that the C convention of passing \trl{PETSC_NULL} (or 0) {\em cannot}
be used.  For example, when no options prefix is desired in the
routine \trl{PetscOptionsGetInt()}, one must use the following command in
Fortran:
\begin{verse}
    call PetscOptionsGetInt(PETSC\_NULL\_CHARACTER,'-name',N,flg,ierr)
\end{verse}

This Fortran requirement is inconsistent with C, where the 
user can employ \trl{PETSC_NULL} for all null arguments. 

\subsection{Duplicating Multiple Vectors}
\label{sec:fortvecd}

\findex{VecDuplicateVecs()}
The Fortran interface to \trl{VecDuplicateVecs()} differs slightly
from the C/C++ variant because Fortran does not allow arrays to be
returned in routine arguments.  To create \trl{n} vectors of the same
format as an existing vector, the user must declare a vector array,
\trl{v_new} of size \trl{n}.  Then, after \trl{VecDuplicateVecs()} has
been called, \trl{v_new} will contain (pointers to) the new PETSc
vector objects.  When finished with the vectors, the user should
destroy them by calling \trl{VecDestroyVectors()}.
\findex{VecDestroyVectors()} For example, the following code fragment
duplicates \trl{v_old} to form two new vectors, \trl{v_new(1)} and \trl{v_new(2)}.
\begin{verse}
   Vec     v\_old, v\_new(2)\\
   integer ierr\\
   PetscScalar  alpha\\
   ....\\
   call VecDuplicateVecs(v\_old,2,v\_new,ierr)\\
   alpha = 4.3\\
   call VecSet(alpha,v\_new(1),ierr)\\
   alpha = 6.0\\
   call VecSet(alpha,v\_new(2),ierr)\\
   ....\\
   call VecDestroyVecs(v\_new,2,ierr)
\end{verse}

\subsection{Matrix and Vector Indices}

All matrices and vectors in PETSc use zero-based indexing, regardless
of whether C or Fortran is being used.  The interface routines, such
as \trl{MatSetValues()} and \trl{VecSetValues()}, always use zero
indexing.  See Section \ref{sec:matoptions} for further details.

\subsection{Setting Routines}

When a routine is set from within a Fortran program by a routine such
as \trl{KSPSetConvergenceTest()}, that routine is assumed to be a
Fortran routine. Likewise, when a routine is set from within a C
program, that routine is assumed to be written in C.

\subsection{Compiling and Linking Fortran Programs}
\label{sec:fortcompile}

Figure \ref{fig:make1} shows a sample makefile that can be used for
PETSc programs.  In this makefile, one can compile and run a debugging version
of the Fortran program \trl{ex3.F} with the actions \trl{make} \trl{BOPT=g} \trl{ex3} and
\trl{make} \trl{runex3}, respectively. The compilation command is restated below:
\begin{verse}
   ex3: ex3.o \\
           -\${FLINKER} -o ex3 ex3.o  \${PETSC\_FORTRAN\_LIB} \${PETSC\_LIB}\\
           \${RM} ex3.o
\end{verse}
Note that the PETSc Fortran interface library, given by 
\trl{${PETSC_FORTRAN_LIB}}, {\em must}  \findex{PETSC_FORTRAN_LIB} precede
the base PETSc libraries, given by \trl{${PETSC_LIB}}, \findex{PETSC_LIB}
on the link line.

\subsection{Routines with Different Fortran Interfaces}
\label{sec:fortran_exceptions}

The following Fortran routines differ slightly from their C counterparts; see the 
manual pages and previous discussion in this chapter for details:
\begin{itemize}
\item \trl{PetscInitialize(char *filename,int} \trl{ierr)}
\item \trl{PetscError(int err,char *message,int} \trl{ierr)}
\item \trl{VecGetArray()}, \trl{MatGetArray()}
\item \trl{ISGetIndices()}, \trl{DAGetGlobalIndices()}
\item \trl{VecDuplicateVecs()}, \trl{VecDestroyVecs()}
\item \trl{PetscOptionsGetString()}
\end{itemize}
The following functions are not supported in Fortran:
\begin{itemize}
\item \trl{PetscFClose()}, \trl{PetscFOpen()}, \trl{PetscFPrintf()}, \trl{PetscPrintf()}
\item \trl{PetscPopErrorHandler()}, \trl{PetscPushErrorHandler()}
\item \trl{PetscLogInfo()}
\item \trl{PetscSetDebugger()}
\item \trl{VecGetArrays()}, \trl{VecRestoreArrays()}
\item \trl{PetscViewerASCIIGetPointer()}, \trl{PetscViewerBinaryGetDescriptor()}
\item \trl{PetscViewerStringOpen()}, \trl{PetscViewerStringSPrintf()}
\item \trl{PetscOptionsGetStringArray()}
\end{itemize}

\subsection{Fortran90}

PETSc includes limited support for direct use of Fortran90 pointers.
Current routines include:
\begin{itemize}
\item \trl{VecGetArrayF90()}, \trl{VecRestoreArrayF90()}
\item \trl{VecDuplicateVecsF90()}, \trl{VecDestroyVecsF90()}
\item \trl{DAGetGlobalIndicesF90()}
\item \trl{MatGetArrayF90()}, \trl{MatRestoreArrayF90()}
\item \trl{ISGetIndicesF90()}, \trl{ISRestoreIndicesF90()}
\end{itemize}
See the manual pages for details and pointers to example programs.  To
use the routines \trl{VecGetArrayF90()}, \trl{VecRestoreArrayF90()}
\trl{VecDuplicateVecsF90()}, and \trl{VecDestroyVecsF90()}, one must
use the Fortran90 vector include file,
\begin{verse}
    \#include "include/finclude/petscvec.h90"
\end{verse}
Analogous include files for other components are \trl{petscda.h90},
\trl{petscmat.h90}, and \trl{petscis.h90}.

Unfortunately, these routines currently work only on certain machines with 
certain compilers. They currently work with the SGI, Solaris, the Cray T3E, the
IBM and the NAG Fortran 90 compiler.

\section{Sample Fortran77 Programs}
\label{sec:fortran-examples}

Sample programs that illustrate the PETSc interface for Fortran
are given in Figures \ref{fig:vec-Fortran} - \ref{fig:SNES-Fortran},
corresponding to
\trl{${PETSC_DIR}/src/vec/examples/tests/ex19.F}, 
\trl{${PETSC_DIR}/src/vec/examples/tutorials/ex4f.F}, 
\break \trl{${PETSC_DIR}/src/draw/examples/tests/ex5.F}, and 
\trl{${PETSC_DIR}/src/snes/examples/ex1f.F}, respectively.  We also
refer Fortran programmers to the C examples listed throughout the manual,
since PETSc usage within the two languages differs only slightly.

\begin{figure}[H]
{\small
\fileinclude{../../../src/vec/examples/tests/ex19.F}
}
\caption{Sample Fortran Program:  Using PETSc Vectors}
\label{fig:vec-Fortran}
\end{figure}

\begin{figure}[H]
{\small
\fileinclude{../../../src/vec/examples/tutorials/ex4f.F}
}
\caption{Sample Fortran Program:  Using VecSetValues() and VecGetArray()}
\label{fig:vec2-Fortran}
\end{figure}

\begin{figure}[H]
{\small
\fileinclude{../../../src/sys/src/draw/examples/tests/ex5.F}
}
\caption{Sample Fortran Program:  Using PETSc PetscDraw Routines}
\label{fig:draw-Fortran}
\end{figure}

\begin{figure}[H]
{\small
\fileinclude{../../../src/snes/examples/tutorials/ex1f.F}
}
\caption{Sample Fortran Program:  Using PETSc Nonlinear Solvers}
\label{fig:SNES-Fortran}
\end{figure}

% --------------------------------------------------------------------
%                            PART 3
% --------------------------------------------------------------------
\part{Additional Information}
\label{part:usefulstuff}

%---------------------------------------------------------------------
\chapter{Profiling} 
\label{ch:profiling} \sindex{profiling}

PETSc includes a consistent, lightweight scheme to allow the profiling
of application programs.  The PETSc routines automatically log
performance data if certain options are specified at runtime.  The
user can also log information about application codes for a complete
picture of performance.  In addition, as described in
Section~\ref{sec:ploginfo}, PETSc provides a mechanism for printing
informative messages about computations.  Section~\ref{sec:profbasic}
introduces the various profiling options in PETSc, while the
remainder of the chapter focuses on details such as monitoring
application codes and tips for accurate profiling.  

\section{Basic Profiling Information}
\label{sec:profbasic}
\sindex{profiling} \sindex{logging} \sindex{timing} 

If an application code and the PETSc libraries have been compiled with
the \trl{-DPETSC_USE_LOG} flag (which is the default for all versions),
then various kinds of profiling of code between calls to \trl{
PetscInitialize()} and \trl{PetscFinalize()} can be \findex{PETSC_USE_LOG}
activated at runtime.  Note that the flag \trl{-DPETSC_USE_LOG} can be
specified for an installation of PETSc in the file \trl{
${PETSC_DIR}/bmake/${PETSC_ARCH}/variables}, as discussed in
Section~\ref{sec:makeflags}.  The profiling options include the following:
\findex{-log_summary} \findex{-log_info}
\findex{-log_trace}
\begin{itemize}
\item \trl{-log_summary} - Prints an ASCII version of performance data
     at program's conclusion. These statistics are comprehensive and concise
     and require little overhead; thus, \trl{-log_summary} is intended as
     the primary means of monitoring the performance of PETSc codes.
\item \trl{-log_info} - Prints verbose information about code to the screen. 
     This option provides details about algorithms, data structures, etc.
     Since the overhead of printing such output slows a code, this
     option should not be used when evaluating a program's performance.
\item \trl{-log_trace [logfile]} - Traces the beginning and ending of all
     PETSc events.  This option, which can be used in conjunction with 
     \trl{-log_info}, is useful to see where a program is hanging
     without running in the debugger.  
\end{itemize}
 As discussed in Section~\ref{sec:mpelogs},
additional profilng can be done with MPE.

\subsection{Interpreting {\tt -log\_summary} Output: The Basics}

As shown in Figure~\ref{fig:exprof} (in Part I), the option \trl{
-log_summary} \findex{-log_summary} activates printing of profile
data to standard output at the conclusion of a program.  Profiling
data can also be printed at any time within a program by calling \trl{
PetscLogPrintSummary()}.

We print performance data for each routine, organized by PETSc
components, followed by any user-defined events (discussed in
Section~\ref{sec:profileuser}).  For each routine, the output data
include the maximum time and floating point operation (flop) rate over
all processors.  Information about parallel performance is also
included, as discussed in the following section.

For the purpose of PETSc floating point operation counting, we define
one {\em flop} as one operation of any of the following types:
multiplication, division, addition, or subtraction.  For example, one
\trl{VecAXPY()} operation, which computes $y = \alpha x + y$ for
vectors of length $N$, requires $2N$ flops (consisting of $N$
additions and $N$ multiplications).  Bear in mind that flop rates
present only a limited view of performance, since memory loads and stores are
the real performance barrier.

For simplicity, the remainder of this discussion focuses on
interpreting profile data for the \trl{SLES} component, 
which provides the linear solvers at the heart of the
PETSc package.  Recall the hierarchical organization of the PETSc
library, as shown in Figure~\ref{fig:1}.  Each \trl{SLES} solver 
is composed of a \trl{PC} (preconditioner) and \trl{KSP} (Krylov
subspace) component, which are in turn built on top of the \trl{Mat} 
(matrix) and \trl{Vec} (vector) modules.  Thus, operations in the
\trl{SLES} module are composed of lower-level operations in these
components.  Note also that the nonlinear solvers component, \trl{SNES}, 
is build on top of the \trl{SLES} module, and the timestepping
component, \trl{TS}, is in turn built on top of \trl{SNES}.

We briefly discuss interpretation of the sample output in
Figure~\ref{fig:exprof}, which was generated by solving a linear
system on one processor using restarted GMRES and ILU
preconditioning.  The linear solvers in \trl{SLES} consist of two
basic phases, \trl{SLESSetUp()} and \trl{SLESSolve()}, each of which
consists of a variety of actions, depending on the particular
solution technique.
For the case of using the \trl{PCILU} preconditioner and \trl{KSPGMRES}
Krylov subspace method, the breakdown of PETSc routines is listed below.
As indicated by the levels of indentation, the
operations in \trl{SLESSetUp()} include all of the operations within
\trl{PCSetUp()}, which in turn include \trl{MatILUFactor()}, and so on. 
\newcommand{\bu}{$\bullet \: $}
\begin{tabbing}
12345\=123\=123\=123\= \kill
\> \bu \trl{SLESSetUp} - Set up linear solver\\
\>\> \bu \trl{PCSetUp} - Set up preconditioner\\
\>\>\> \bu \trl{MatILUFactor} - Factor preconditioning matrix\\
\>\>\>\> \bu \trl{MatILUFactorSymbolic} - Symbolic factorization phase\\
\>\>\>\> \bu \trl{MatLUFactorNumeric} - Numeric factorization phase\\
\> \bu \trl{SLESSolve} - Solve linear system\\
\>\> \bu \trl{PCApply} - Apply preconditioner\\
\>\>\> \bu \trl{MatSolve} - Forward/backward triangular solves\\
\>\> \bu \trl{KSPGMRESOrthog} - Orthogonalization in GMRES\\
\>\>\> \bu \trl{VecDot} or \trl{VecMDot} - Inner products\\
\>\> \bu \trl{MatMult} - Matrix-vector product\\
\>\> \bu \trl{MatMultAdd} - Matrix-vector product + vector addition\\
\>\> \bu  \trl{VecScale}, \trl{VecNorm}, \trl{VecAXPY}, \trl{VecCopy}, ...\\
\end{tabbing}

The summaries printed via \trl{-log_summary} reflect this 
routine hierarchy. For example, the performance summaries for a
particular high-level routine such as \trl{SLESSolve} include all of
the operations accumulated in the lower-level components that
make up the routine.  

Admittedly, we do not currently present the output with \trl{
-log_summary} so that the hierarchy of PETSc operations is completely
clear, primarily because we have not determined a clean and uniform
way to do so throughout the library.  Improvements may follow.
However, for a particular problem, the user should generally have
an idea of the basic operations that are required for its
implementation (e.g., which operations are performed when using GMRES
and ILU, as described above), so that interpreting the \trl{-log_summary}
data should be relatively straightforward.

\subsection{Interpreting {\tt -log\_summary} Output: Parallel Performance}
\label{sec:parperformance}

We next discuss performance summaries for parallel programs,
\findex{-log_summary} as shown within Figures~\ref{fig:exparprof} and
~\ref{fig:exparprof2}, which present the combined output generated by
the \trl{-log_summary} option.  The program that generated this data is
\trl{${PETSC_DIR}/src/sles/examples/ex21.c}.  The code loads a
matrix and right-hand-side vector from a binary file and then solves
the resulting linear system; the program then repeats this process for
a second linear system.  This particular case was run on four
processors of an IBM SP, using restarted GMRES and the block Jacobi
preconditioner, where each block was solved with ILU.

Figure~\ref{fig:exparprof} presents an overall performance summary,
including times, floating-point operations, computational rates, and
message-passing activity (such as the number and size of messages sent
and collective operations).  Summaries for various user-defined stages
of monitoring (as discussed in Section~\ref{sec:profstages}) are also
given. Information about the various phases of computation then follow
(as shown separately here in Figure~\ref{fig:exparprof2}).
Finally, a summary of memory usage and object creation and destruction
is presented.

\begin{figure}[tb]
{\tiny
\begin{verbatim}
mpirun ex21 -f0 medium -f1 arco6 -ksp_gmres_unmodifiedgramschmidt -log_summary -mat_mpibaij \
            -matload_block_size 3 -pc_type bjacobi -optionsleft

Number of iterations =  19
Residual norm = 7.7643e-05
Number of iterations =  55
Residual norm = 6.3633e-01

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

ex21 on a rs6000 named p039 with 4 processors, by mcinnes Wed Jul 24 16:30:22 1996

                         Max         Min        Avg        Total 
Time (sec):           3.289e+01      1.0   3.288e+01
Objects:              1.130e+02      1.0   1.130e+02
Flops:                2.195e+08      1.0   2.187e+08   8.749e+08
Flops/sec:            6.673e+06      1.0               2.660e+07
MPI Messages:         2.205e+02      1.4   1.928e+02   7.710e+02
MPI Message Lengths:  7.862e+06      2.5   5.098e+06   2.039e+07
MPI Reductions:       1.850e+02      1.0

Summary of Stages:  ---- Time ------  ----- Flops -------  -- Messages -- -- Message-lengths -- Reductions -
                      Avg      %Total    Avg     %Total   counts   %Total    avg      %Total   counts  %Total 
 0:  Load System 0: 1.191e+00    3.6% 3.980e+06   0.5%  3.80e+01    4.9%  6.102e+04   0.3%  1.80e+01   9.7% 
 1:    SLESSetup 0: 6.328e-01    2.5% 1.479e+04   0.0%  0.00e+00    0.0%  0.000e+00   0.0%  0.00e+00   0.0% 
 2:    SLESSolve 0: 2.269e-01    0.9% 1.340e+06   0.0%  1.52e+02   19.7%  9.405e+03   0.0%  3.90e+01  21.1% 
 3:  Load System 1: 2.680e+01  107.3% 0.000e+00   0.0%  2.10e+01    2.7%  1.799e+07  88.2%  1.60e+01   8.6% 
 4:    SLESSetup 1: 1.867e-01    0.7% 1.088e+08   2.3%  0.00e+00    0.0%  0.000e+00   0.0%  0.00e+00   0.0% 
 5:    SLESSolve 1: 3.831e+00   15.3% 2.217e+08  97.1%  5.60e+02   72.6%  2.333e+06  11.4%  1.12e+02  60.5% 

------------------------------------------------------------------------------------------------------------------------

.... [Summary of various phases, see part II below] ...

------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type      Creations   Destructions   Memory  Descendants' Mem.
Viewer                5              5          0     0
Index set            10             10     127076     0
Vector               76             76    9152040     0
Vector Scatter        2              2     106220     0
Matrix                8              8    9611488     5.59773e+06
Krylov Solver         4              4      33960     7.5966e+06
Preconditioner        4              4         16     9.49114e+06
SLES                  4              4          0     1.71217e+07
\end{verbatim}
}
%\centerline{ \pdfximage width 7in {perflog1.pdf} \pdfrefximage \pdflastximage}
%\centerline{\psfig{file=perflog1.ps,width=7.5in}}
%\vspace{-2.5in}
\caption{Profiling a PETSc Program: Part I - Overall Summary}
\label{fig:exparprof}
\end{figure}

We next focus on the summaries for the various phases of the
computation, as given in the table within Figure~\ref{fig:exparprof2}.  The summary for
each phase presents the maximum times and flop rates over all
processors, as well as the ratio of maximum to minimum times and flop
rates for all processors.  A ratio of approximately 1 indicates that
computations within a given phase are well balanced among the
processors; as the ratio increases, the balance becomes increasingly
poor.  Also, the total computational rate (in units of MFlops/sec) is
given for each phase in the final column of the phase summary table.
\[
   {\rm Total\: Mflop/sec} \:=\: 10^{-6} * ({\rm sum\; of\; flops\; over\; all\; processors})/({\rm max\; time\; over\; all\; processors})
\]
{\em Note}: Total computational rates $<$ 1 MFlop are listed as 0 in this column
of the phase summary table.
Additional statistics for each phase include the total number of messages sent,
the average message length, and the number of global reductions.

\begin{figure}[tb]
{\tiny
\begin{verbatim}
mpirun ex21 -f0 medium -f1 arco6 -ksp_gmres_unmodifiedgramschmidt -log_summary -mat_mpibaij \
            -matload_block_size 3 -pc_type bjacobi -optionsleft

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------
.... [Overall summary, see part I] ...

Phase summary info:
   Count: number of times phase was executed
   Time and Flops/sec: Max - maximum over all processors
                       Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length
   Reduct: number of global reductions
   Global: entire computation
   Stage: optional user-defined stages of a computation. Set stages with PLogStagePush() and PLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10^6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Phase              Count      Time (sec)       Flops/sec                          --- Global ---  --- Stage ---   Total
                            Max    Ratio      Max    Ratio  Mess  Avg len  Reduct %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------
...
 
--- Event Stage 4: SLESSetUp 1

MatGetReordering       1  3.491e-03   1.0  0.0e+00   0.0  0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   2  0  0  0  0     0
MatILUFctrSymbol       1  6.970e-03   1.2  0.0e+00   0.0  0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   3  0  0  0  0     0
MatLUFactorNumer       1  1.829e-01   1.1  3.2e+07   1.1  0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0  90 99  0  0  0   110
SLESSetUp              2  1.989e-01   1.1  2.9e+07   1.1  0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0  99 99  0  0  0   102
PCSetUp                2  1.952e-01   1.1  2.9e+07   1.1  0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0  97 99  0  0  0   104
PCSetUpOnBlocks        1  1.930e-01   1.1  3.0e+07   1.1  0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0  96 99  0  0  0   105

--- Event Stage 5: SLESSolve 1

MatMult               56  1.199e+00   1.1  5.3e+07   1.0  1.1e+03 4.2e+03 0.0e+00  5 28 99 23  0  30 28 99 99  0   201
MatSolve              57  1.263e+00   1.0  4.7e+07   1.0  0.0e+00 0.0e+00 0.0e+00  5 27  0  0  0  33 28  0  0  0   187
VecNorm               57  1.528e-01   1.3  2.7e+07   1.3  0.0e+00 0.0e+00 2.3e+02  1  1  0  0 31   3  1  0  0 51    81
VecScale              57  3.347e-02   1.0  4.7e+07   1.0  0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   1  1  0  0  0   184
VecCopy                2  1.703e-03   1.1  0.0e+00   0.0  0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 3  2.098e-03   1.0  0.0e+00   0.0  0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                3  3.247e-03   1.1  5.4e+07   1.1  0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   200
VecMDot               55  5.216e-01   1.2  9.8e+07   1.2  0.0e+00 0.0e+00 2.2e+02  2 20  0  0 30  12 20  0  0 49   327
VecMAXPY              57  6.997e-01   1.1  6.9e+07   1.1  0.0e+00 0.0e+00 0.0e+00  3 21  0  0  0  18 21  0  0  0   261
VecScatterBegin       56  4.534e-02   1.8  0.0e+00   0.0  1.1e+03 4.2e+03 0.0e+00  0  0 99 23  0   1  0 99 99  0     0
VecScatterEnd         56  2.095e-01   1.2  0.0e+00   0.0  0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   5  0  0  0  0     0
SLESSolve              1  3.832e+00   1.0  5.6e+07   1.0  1.1e+03 4.2e+03 4.5e+02 15 97 99 23 61  99 99 99 99 99   222
KSPGMRESOrthog        55  1.177e+00   1.1  7.9e+07   1.1  0.0e+00 0.0e+00 2.2e+02  4 39  0  0 30  29 40  0  0 49   290
PCSetUpOnBlocks        1  1.180e-05   1.1  0.0e+00   0.0  0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCApply               57  1.267e+00   1.0  4.7e+07   1.0  0.0e+00 0.0e+00 0.0e+00  5 27  0  0  0  33 28  0  0  0   186
------------------------------------------------------------------------------------------------------------------------
.... [Conclusion of overall summary, see part I] ...
\end{verbatim}
}
%\centerline{ \pdfximage height 7in {perflog2.pdf} \pdfrefximage \pdflastximage}
%\centerline{\psfig{file=perflog2.ps,width=7.5in}}
%\vspace{-2.5in}
\caption{Profiling a PETSc Program: Part II - Phase Summaries}
\label{fig:exparprof2}
\end{figure}

As discussed in the preceding section, the performance summaries for
higher-level PETSc routines include the statistics for the lower
levels of which they are made up.  For example, the communication within
matrix-vector products \trl{MatMult()} consists of vector scatter
operations, as given by the routines \trl{VecScatterBegin()} and \trl{
VecScatterEnd()}. 
%
%  Temp removed
%
%(Details about the implementation of parallel
%matrix-vector products are given in \cite{petsc-mpi-paper}.)

The final data presented are the percentages of the various statistics
(time (\trl{\%T}), flops/sec (\trl{\%F}), messages(\trl{\%M}), average message length (\trl{\%L}),
and reductions (\trl{\%R})) for each event relative to the total computation and to any
user-defined stages (discussed in Section~\ref{sec:profstages}).
These statistics can aid in optimizing performance, since they indicate the sections of code that
could benefit from various kinds of tuning.  Chapter~\ref{ch:performance} gives
suggestions about achieving good performance with PETSc codes.

\subsection{Using {\tt -log\_mpe} with Upshot/Nupshot}
\label{sec:mpelogs}

It is also possible to use the {\em Upshot} (or {\em Nupshot}) package
\cite{upshot} \sindex{Upshot} \sindex{Nupshot}  to visualize PETSc events. 
This package comes with the MPE software, which is part of the MPICH
\cite{mpich-web-page} implementation of MPI.
The option \findex{-log_mpe}
\begin{verse}
   -log\_mpe [logfile]
\end{verse}
creates a logfile of events appropriate for viewing with {\em Upshot}.
The user can either use the default logging file, \trl{mpe.log}, or
specify an optional name via \trl{logfile}.  

To use this logging option, the user may employ any implementation of
MPI (not necessarily MPICH), but must build and link the MPE part of
the MPICH.  The user must compile the PETSc library with the \trl{
-DPETSC_HAVE_MPE} flag, which is {\em not} activated by default. The user
can turn on MPE logging by specifying \trl{-DPETSC_HAVE_MPE} in the \trl{
PCONF} variable within \trl{
${PETSC_DIR}/bmake/${PETSC_ARCH}/packages} and (re)compiling all
of PETSc.

By default, not all PETSc events are logged with MPE. For example,
since \trl{MatSetValues()} may be called thousands of times in a program,
by default its calls are not logged with MPE. To activate MPE logging of
a particular event, one should use the command 
\begin{verse}
    PetscLogEventMPEActivate(int event);
\end{verse}
To deactivate logging of an event for MPE, one should use 
\begin{verse}
    PetscLogEventMPEDeactivate(int event);
\end{verse}
The \trl{event} may be either a predefined PETSc event (as listed in
the file \trl{${PETSC_DIR}/include/petsclog.h}) or one obtained with
\trl{PetscLogEventRegister()} (as described in
Section~\ref{sec:profileuser}).  These routines may be called as many
times as desired in an application program, so that one could restrict
MPE event logging only to certain code segments.

To see what events are logged by default, the user can view the source code; see
the files \trl{src/plot/src/plogmpe.c} and \trl{include/petsclog.h}.  A simple program and
GUI interface to see the events that are predefined and their definition is 
being developed.

The user can also log MPI events.  To do this, simply consider the
PETSc application as any MPI application, and follow the MPI
implementation's instructions for logging MPI calls. For example, when
using MPICH, this merely required adding \trl{-llmpi} to the library
list {\em before} \trl{-lmpi}.

%- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\section{Profiling Application Codes}
\label{sec:profileuser}

PETSc automatically logs object creation, times, and floating-point
counts for the library routines. Users can easily supplement
this information by monitoring their application codes as well.  
The basic steps involved in logging a
user-defined portion of code, called an {\em event}, are shown in the 
code fragment below:
\begin{verse}
    \#include "petsclog.h"\\
    int USER\_EVENT;\\
    PetscLogEventRegister(\&USER\_EVENT,"User event name",0);\\
    PetscLogEventBegin(USER\_EVENT,0,0,0,0);\\
       /* application code segment to monitor */\\
       PetscLogFlops(number of flops for this code segment);\\
    PetscLogEventEnd(USER\_EVENT,0,0,0,0);
\end{verse}

One must register the event by calling \trl{
PetscLogEventRegister()}, which assigns a unique integer to identify the
event for profiling purposes: \findex{PetscLogEventRegister()}
\begin{verse}
  PetscLogEventRegister(int *e,char *string);
\end{verse}
Here \trl{string} is a user-defined event name, and \trl{color} is an
optional user-defined event color (for use with {\em Upshot/Nupshot} logging);
one should see the manual page for details.  The argument returned in \trl{e} should then
be passed to the \trl{PetscLogEventBegin()} and \trl{PetscLogEventEnd()}
routines.

Events are logged by using the pair \findex{PetscLogEventBegin()}
\begin{verse}
   PetscLogEventBegin(int event,PetscObject o1,PetscObject o2,PetscObject o3,PetscObject o4);\\
   PetscLogEventEnd(int event,PetscObject o1,PetscObject o2,PetscObject o3,PetscObject o4);
\end{verse}
The \findex{PetscLogEventEnd()}
four objects are the PETSc objects that are most closely associated 
with the event.  For instance, in a matrix-vector product they 
would be the matrix and the two vectors.  These objects can be omitted
by specifying 0 for \trl{o1} - \trl{o4}.  The code between these 
two routine calls will be automatically timed and logged as part of the
specified event.

The user can log the number of floating-point operations 
for this segment of code by calling \findex{PetscLogFlops()}
\begin{verse}
    PetscLogFlops(number of flops for this code segment);
\end{verse}
between the calls to \trl{PetscLogEventBegin()} and \trl{PetscLogEventEnd()}.
This value will automatically be added to the global flop counter for the
entire program.

%- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\section{Profiling Multiple Sections of Code}
\label{sec:profstages}

By default, the profiling produces a single set of statistics for all
code between the \trl{PetscInitialize()} and \trl{PetscFinalize()}
calls within a program.  One can independently monitor up to ten
stages of code by switching among the various stages with the comands
\findex{PetscLogStagePush()} \findex{PetscLogStagePop()}
\begin{verse}
   PetscLogStagePush(int stage);\\
   PetscLogStagePop();
\end{verse}
where \trl{stage} is an integer (0-9); see the manual pages for details.
The command  \findex{PetscLogStageRegister()}
\begin{verse}
   PetscLogStageRegister(int stage,char *name)
\end{verse}
allows one to associate a name with a stage; these names are printed whenever
summaries are generated with \trl{-log_summary} or \trl{PetscLogPrintSummary()}.
The following code fragment uses three profiling stages within an program.

\begin{verse}
   PetscInitialize(int *argc,char ***args,0,0);\\
   /* stage 0 of code here */\\
   PetscLogStageRegister(0,"Stage 0 of Code");\\
   for (i=0; i<ntimes; i++) \{\\
      PetscLogStagePush(1);\\
      PetscLogStageRegister(1,"Stage 1 of Code");\\
      /* stage 1 of code here */\\
      PetscLogStagePop();\\
      PetscLogStagePush(2);\\
      PetscLogStageRegister(1,"Stage 2 of Code");\\
      /* stage 2 of code here */\\
      PetscLogStagePop();\\
   \}
   PetscFinalize();
\end{verse}

Figures~\ref{fig:exparprof} and \ref{fig:exparprof2} show output
generated by \trl{-log_summary} for a program that employs
several profiling stages.  In particular, this program is
subdivided into six stages: loading a matrix and right-hand-side
vector from a binary file, setting up the preconditioner, and solving
the linear system; this sequence is then repeated for a second linear
system.  For simplicity, Figure~\ref{fig:exparprof2} contains output
only for stages 4 and 5 (linear solve of the second system), which comprise
the part of this computation of most interest to us in terms of
performance monitoring.  This code organization (solving a small
linear system followed by a larger system) enables generation of more
accurate profiling statistics for the second system by overcoming the
often considerable overhead of paging, as discussed in
Section~\ref{sec:profaccuracy}.

%- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\section{Restricting Event Logging}
\label{sec:deactivate}

By default, all PETSc operations are logged.
To enable or disable the PETSc logging of individual events, one uses the commands
\begin{verse}
   PetscLogEventActivate(int event);\\
   PetscLogEventDeactivate(int event);
\end{verse}
The \trl{event} may be either a predefined PETSc event (as listed in
the file \trl{${PETSC_DIR}/include/petsclog.h}) or one obtained with
\trl{PetscLogEventRegister()} (as described in Section~\ref{sec:profileuser}).

PETSc also provides routines that deactivate (or activate)
logging for entire components of the library. Currently, the 
components that support such logging (de)activation are \trl{Mat} (matrices),
\trl{Vec} (vectors), \trl{SLES} (linear solvers, including \trl{KSP} 
and \trl{PC} components), and \trl{SNES} (nonlinear solvers):
\begin{verse}
   PetscLogEventDeactivateClass(MAT\_COOKIE);\\
   PetscLogEventDeactivateClass(SLES\_COOKIE); /* includes PC and KSP */\\
   PetscLogEventDeactivateClass(VEC\_COOKIE);\\
   PetscLogEventDeactivateClass(SNES\_COOKIE);
\end{verse}
and 
\begin{verse}
   PetscLogEventActivateClass(MAT\_COOKIE);\\
   PetscLogEventActivateClass(SLES\_COOKIE);   /* includes PC and KSP */\\
   PetscLogEventActivateClass(VEC\_COOKIE);\\
   PetscLogEventActivateClass(SNES\_COOKIE);
\end{verse}

Recall that the option \trl{-log_all} produces extensive profile
data, which can be a challenge for PETScView to handle due to
the memory limitations of Tcl/Tk.  Thus, one should generally use
\trl{-log_all} when running programs with a relatively small
number of events or when disabling some of the events that occur many
times in a code (e.g., \trl{VecSetValues()}, \trl{MatSetValues()}).

Section~\ref{sec:mpelogs} gives information on the restriction of events
in MPE logging.

%- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

\section{Interpreting {\tt -log\_info} Output: Informative Messages}
\label{sec:PetscLoginfo}

Users can activate the printing of verbose information about
algorithms, data structures, etc. to the screen by using the option \trl{
-log_info} \findex{-log_info} or by calling \trl{
PetscLogInfoAllow(PETSC_TRUE)}. \findex{PetscLogInfoAllow()}
Such logging, which is used throughout the PETSc libraries,
can aid the user in understanding algorithms and 
tuning program performance.  For example, as discussed in
Section~\ref{sec:matsparse}, \trl{-log_info} activates the
printing of information about memory allocation during
matrix assembly.

Application programmers can employ this logging as well, by
using the routine \findex{PetscLogInfo()}
\begin{verse}
   PetscLogInfo(void* obj,char *message,...)
\end{verse}
where \trl{obj} is the PETSc object associated most closely with
the logging statement, \trl{message}.
For example, in the line search Newton methods, we use a statement such as
\begin{verse}
   PetscLogInfo(snes,"Cubically determined step, lambda %g\n",lambda);
\end{verse}

One can selectively turn off informative messages about any of the 
basic PETSc objects (e.g., \trl{Mat}, \trl{SNES}) with the command
\begin{verse}
   PetscLogInfoDeactivateClass(int object\_cookie)
\end{verse}
where \findex{PetscLogInfoDeactivateClass()} \findex{PetscLogInfoActivateClass()}
\trl{object_cookie} is one of \trl{MAT_COOKIE}, \trl{SNES_COOKIE}, etc.
Messages can be reactivated with the command
\begin{verse}
   PetscLogInfoActivateClass(int object\_cookie)
\end{verse}
Such deactivation can be useful when one wishes to view information
about higher level PETSc components (e.g., \trl{TS} and \trl{SNES}) without 
seeing all lower level data as well (e.g., \trl{Mat}).  One can deactivate
events at runtime for matrix and linear solver components via \trl{-log_info [no_mat, no_sles]}.

%- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\section{Time}

PETSc application programmers can access the wall clock time directly 
with the command \sindex{wall clock time}
\begin{verse}
  PetscLogDouble time;\\
PetscGetTime(\&time);CHKERRQ(ierr);
\end{verse}
\findex{PetscGetTime()} \sindex{time}
In addition, as discussed in Section~\ref{sec:profileuser},
PETSc can automatically profile user-defined segments of code.

%- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\section{Saving Output to a File}

All output from PETSc programs (including informative messages, profiling information,
and convergence data) can be saved to a file by using the command line
option \trl{-log_history [filename]}. \findex{-log_history}
If no file name is specified, the output is stored in the file \trl{$HOME/.petschistory}.
\findex{.petschistory} Note that this option only saves output printed with 
the \trl{PetscPrintf()} and \trl{PetscFPrintf()} commands, not the
standard \trl{printf()} and \trl{fprintf()} statements. 
\findex{PetscPrintf()} \findex{PetscFPrintf()}

%- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\section{Accurate Profiling: Overcoming the Overhead of Paging}
\label{sec:profaccuracy}

One factor that often plays a significant role in profiling a code is
paging by the operating system.  Generally, when running a program
only a few pages required to start it are loaded into memory rather
than the entire executable.  When the execution procedes to code
segments that are not in memory, a pagefault occurs, prompting the
required pages to be loaded from the disk (a very slow process).  This
activity distorts the results significantly. (The paging effects are
noticeable in the the log files generated by \trl{-log_mpe}, which is
described in Section~\ref{sec:mpelogs}.)

To eliminate the effects of paging when profiling the performance of a
program, we have found an effective procedure is to run the {\bf exact same
code} on a small dummy problem before running it on the actual problem
of interest. We thus ensure that all code required by a solver is
loaded into memory during solution of the small problem.  When the
code procedes to the actual (larger) problem of interest, all required
pages have already been loaded into main memory, so that the
performance numbers are not distorted.

When this procedure is used in conjunction with the user-defined stages of profiling
described in Section~\ref{sec:profstages}, we can focus easily on the
problem of interest.  For example, we used this technique in the program
\trl{${PETSC_DIR}/src/sles/examples/tutorials/ex10.c} to
generate the timings within Figures~\ref{fig:exparprof} and \ref{fig:exparprof2}.
In this case,
the profiled code of interest (solving the linear system for the larger problem)
occurs within event stages 4 and 5.  Section~\ref{sec:parperformance} provides
details about interpreting such profiling data.

In particular, the macros 
\begin{verse}
  PreLoadBegin(PetscTruth,char* stagename),\\
  PreLoadStage(char *stagename),
\end{verse}
and 
\begin{verse}
  PreLoadEnd()
\end{verse}
can be used to easily 
convert a regular PETSc program to one that uses preloading. The command line options
\trl{-preload} \trl{true} and \trl{-preload} \trl{false} may be used to turn on and off
preloading at run time for PETSc programs that use these macros. \findex{PreLoadBegin()}
\findex{PreLoadEnd()} \findex{PreLoadStage()} \findex{-preload}

%---------------------------------------------------------------------
\chapter{Hints for Performance Tuning} 
\label{ch:performance} \sindex{performance tuning}

This chapter presents some tips on achieving good performance within
PETSc codes.  We urge users to read these hints before
evaluating the performance of PETSc application codes.

\section{Compiler Options}
\sindex{compiler options}

Code compiled with the \trl{BOPT=O} option generally runs two to three times
faster than that compiled with \trl{BOPT=g}, so we recommend using one
of the optimized versions of code (\trl{BOPT=O}, \trl{BOPT=O_c++}, or
\trl{BOPT=O_complex}) when evaluating performance.

The user can specify alternative compiler options instead of
the defaults set in the PETSc distribution.  One can set
the compiler options for a particular architecture
(\trl{PETSC_ARCH}) and \trl{BOPT} by editing the file
\trl{${PETSC_DIR}/bmake/${PETSC_ARCH}/variables}.
Section~\ref{sec:custom} gives details.

\section{Profiling}
\sindex{profiling} \sindex{logging} \sindex{timing}

Users should not spend time optimizing a code until after having determined
where it spends the bulk of its time on realistically sized problems.
As discussed in detail in Chapter~\ref{ch:profiling}, the PETSc
routines automatically log performance data if certain runtime options
are specified.  We briefly highlight usage of these features below.

\begin{itemize}
\item Run the code with the option \trl{-log_summary} to print a performance
   summary for various phases of the code. \findex{-log_summary}

\item Run the code with the option \trl{-log_mpe} \trl{[logfilename]}, which creates a
   logfile of events suitable for viewing with Upshot or Nupshot (part of
   MPICH). \findex{-log_mpe}

\end{itemize}

\section{Aggregation}
\sindex{aggregation}

Performing operations on chunks of data rather than a single element
at a time can significantly enhance performance.  
\begin{itemize}
\item Insert several (many) elements of a matrix or vector at once, rather 
   than looping and inserting a single value at a time.  In order to
   access elements in of vector repeatedly, employ {VecGetArray()} to allow 
   direct manipulation of the vector elements. \findex{VecGetArray()}

\item When using \trl{MatSetValues()}, if the column indices of the values being
   inserted have been sorted in monotonically increasing order, call
   the routine \trl{MatSetOption(mat,MAT_COLUMNS_SORTED)} before setting the values
   to reduce the insertion time significantly.

\item When possible, use \trl{VecMDot()} rather than a series of calls to \trl{VecDot()}.
\end{itemize}

\section{Efficient Memory Allocation}
\label{sec:perf:memory}

\subsection{Sparse Matrix Assembly}

Since the process of dynamic memory allocation for sparse matrices is
inherently very expensive, accurate preallocation of memory is crucial
for efficient sparse matrix assembly.  One should use the matrix creation
routines for particular data structures, such as \trl{
MatCreateSeqAIJ()} and \trl{MatCreateMPIAIJ()} for compressed, sparse
row formats, instead of the generic \trl{MatCreate()} routine.  For
problems with multiple degrees of freedom per node, the block,
compressed, sparse row formats, created by \trl{MatCreateSeqBAIJ()}
and \trl{MatCreateMPIBAIJ()}, can significantly enhance performance.
Section~\ref{sec:matsparse} includes extensive details and
examples regarding preallocation.

\subsection{Sparse Matrix Factorization}
\label{sec:symbolfactor}

When symbolically factoring an AIJ matrix, PETSc has to guess
how much fill there will be.  Careful use of the fill parameter in the 
\trl{MatILUInfo} structure \findex{MatILUInfo} 
when calling \trl{MatLUFactorSymbolic()} or \trl{MatILUFactorSymbolic()}
can reduce greatly the number of mallocs and copies required, and thus
greatly improve the performance of the factorization.  One way to
determine a good value for \trl{f} is to run a program with the option \trl{-log_info}.
The symbolic factorization phase will then print information such as
\begin{verse}
   Info:MatILUFactorSymbolic\_AIJ:Realloc 12 Fill ratio:given 1 needed 2.16423
\end{verse}
This indicates that the user should have used a fill estimate factor of
about 2.17 (instead of 1) to prevent the 12 required mallocs and copies.
The command line option  \findex{-pc_ilu_fill} \findex{-pc_lu_fill}
\begin{verse}
    -pc\_ilu\_fill 2.17
\end{verse}
will cause PETSc to preallocate the correct amount of space for incomplete
(ILU) factorization.  The corresponding option for direct (LU) factorization
is \trl{-pc_lu_fill} \trl{<fill_amount>}.

\subsection{PetscMalloc() Calls}
Users should employ a reasonable number of \trl{PetscMalloc()} calls in their codes.
Hundreds or thousands of memory allocations may be appropriate; however, if tens of 
thousands are being used, then reducing the number of \trl{PetscMalloc()} calls
may be warranted.  For example, reusing space or allocating large chunks 
and dividing it into pieces can produce a significant savings in 
allocation overhead.  Section~\ref{sec:dsreuse} gives details.

\section{Data Structure Reuse}
\label{sec:dsreuse}

Data structures should be reused whenever possible.  For example, if a code often
creates new matrices or vectors, there often may be a way to reuse some
of them.  Very significant performance improvements can be achieved by
reusing matrix data structures with the same nonzero pattern.  If a code
creates thousands of matrix or vector objects, performance will be
degraded.  For example, when solving a nonlinear problem or timestepping,
reusing the matrices and their nonzero structure for many steps when
 appropriate can make the code run significantly faster.  

A simple technique for saving work vectors, matrices, etc. is employing
a user-defined context.  In C and C++ such a context is merely a
structure in which various objects can be stashed; in Fortran a user
context can be an integer array that contains both parameters and pointers
to PETSc objects. See \trl{${PETSC_DIR}/snes/examples/tutorials/ex5.c} and 
\trl{${PETSC_DIR}/snes/examples/tutorials/ex5f.F} for examples of user-defined application
contexts in C and Fortran, respectively.

\section{Numerical Experiments}

PETSc users should run a variety of tests.  For example, there are a large number of options 
for the linear and nonlinear equation solvers in PETSc, and different 
choices can make a {\em very} big difference in convergence rates and execution 
times.  PETSc employs defaults that are generally reasonable for a wide
range of problems, but clearly these defaults cannot be best for all
cases.  Users should experiment with many combinations to determine 
what is best for a given problem and customize the solvers accordingly.
\begin{itemize}
\item Use the options \trl{-snes_view}, \trl{-sles_view}, etc. (or the routines 
     \trl{SLESView()}, \trl{SNESView()}, etc.) to view the options that have been
     used for a particular solver.
\item Run the code with the option \trl{-help} for a list of the available 
     runtime commands.
\item Use the option \trl{-log_info} to print details about the solvers' operation.
\item Use the PETSc monitoring discussed in Chapter~\ref{ch:profiling}
     to evaluate the performance of various numerical methods.
\end{itemize}

\section{Tips for Efficient Use of Linear Solvers}
\label{sec:slestips}

As discussed in Chapter~\ref{ch:sles}, the default linear solvers are
\begin{itemize}
\item uniprocessor: GMRES(30) with ILU(0) preconditioning\\
\item multiprocessor: GMRES(30) with block Jacobi preconditioning, where there
                     is 1 block per processor, and each block is solved with ILU(0)\\
\end{itemize}
One should experiment to determine alternatives that may be better for
various applications.  Recall that one can specify the KSP methods and
preconditioners at runtime via the options:
\begin{verse}
   -ksp\_type <ksp\_name> -pc\_type <pc\_name>
\end{verse}
One can also specify a variety of runtime customizations for the
solvers, as discussed throughout the manual.

In particular, note that the default restart parameter for GMRES is
30, which may be too small for some large-scale problems.  One can alter this
parameter with the option \trl{-ksp_gmres_restart} \trl{<restart>} or by
calling \trl{KSPGMRESSetRestart()}. Section~\ref{sec:ksp} gives
information on setting alternative GMRES orthogonalization routines,
which may provide much better parallel performance.

\section{Detecting Memory Allocation Problems}

PETSc provides a number of tools to aid in detection of problems
with memory allocation, including leaks and use of uninitialized space.
We briefly describe these below.
\sindex{memory leaks} \sindex{memory allocation}

\findex{-trmalloc} \findex{-trmalloc_off}
\begin{itemize}
\item The PETSc memory allocation (which collects statistics and performs
error checking), is employed by default for codes compiled in a
debug mode (\trl{BOPT=g}, \trl{BOPT=g_c++}, \trl{BOPT=g_complex}).
PETSc memory allocation can be activated for other other cases, such
as \trl{BOPT=O}, with the option \trl{-trmalloc}, while \trl{
-trmalloc_off} forces the use of conventional memory allocation for the
\trl{BOPT=g}, \trl{BOPT=g_c++}, and \trl{BOPT=g_complex} versions.
When running timing tests, one should use  
the \trl{BOPT=O} version of the libraries.

\findex{-trdump} \findex{PetscTrDump()}
\item When the PETSc memory allocation routines are used, the option 
\trl{-trdump} will print a list of unfreed memory at the conclusion of a
program.  If all memory has been freed, only a message stating
the maximum allocated space will be printed.  However, if some memory
remains unfreed, this information will be printed.  Note that the
option \trl{-trdump} merely activates a call to \trl{PetscTrDump()} during
\trl{PetscFinalize()}; the user can also call \trl{PetscTrDump()} elsewhere
in a program.

\findex{-trmalloc_log} \findex{PetscGetResidentSetSize()}
\findex{PetscTrLog()} \findex{PetscTrLogDump()}
\item Another useful option for use with PETSc memory allocation
routines is \trl{-trmalloc_log}, which activates logging of all calls
to malloc and reports memory usage, including all Fortran arrays.
This option provides a more complete picture than \trl{-trdump} for
codes that employ Fortran with hardwired arrays.  Note that the option
\trl{-trmalloc_log} activates calls to \trl{PetscTrLog()}, \trl{
PetscTrLogDump()}, and \trl{PetscGetResidentSetSize()} during \trl{
PetscFinalize()}; the user can also call these routines elsewhere in a
program.

\findex{PetscInitializeNans()} \findex{PetscInitializeLargeInts()}
\findex{-trmalloc_nan}
\item The option \trl{-trmalloc_nan} is useful for tracking down the
allocated memory that is used before it has been initialized.  This
option calls \trl{PetscInitializeNans()} which marks an array as being
uninitialized, so that if values are used for computation without
first having been set, a floating point exception is generated.  This
option also calls {PetscInitializeLargeInts()}; see the manual pages for
details.  Note that so far these work only on the certain systems.

\end{itemize}

\section{Machine-Specific Optimizations}

\begin{itemize}
\item On the IBM SP, using the mpirun option \trl{-nopoll} may
      improve the performance of some PETSc programs.
\end{itemize}

\section{System-Related Problems}

The performance of a code can be affected by a variety of factors, 
including the cache behavior, other users on the machine, etc.
Below we briefly describe some common problems and possibilities for
overcoming them.

\begin{itemize}
\item {\bf Problem too large for physical memory size}: When timing a program, one
      should always leave at least a ten percent margin between the total
      memory a process is using and the physical size of 
      the machine's memory. One way to estimate the amount of 
      memory used by given process is with the UNIX \trl{ps} command.
      Also, the PETSc option \trl{-log_summary} prints the amount of 
      memory used by the basic PETSc objects, thus providing a lower
      bound on the memory used.  Another useful option is \trl{-trmalloc_log}
      which reports all memory, including any Fortran arrays in an
      application code.
\item {\bf Effects of other users}:  If other users are running
      jobs on the same physical processor nodes on which a program is being profiled,
      the timing results are essentially meaningless. 
\item {\bf Overhead of timing routines on certain machines}: On certain machines,
      even calling the system clock in order to time routines is 
      slow; this skews all of the flop rates and timing results. The file
      \trl{${PETSC_DIR}/src/benchmarks/PetscTime.c} contains a
      simple test problem that will approximate the ammount of time
      required to get the current time in a running program. On good
      systems it will on the order of 1.e-6 seconds or less.
\item {\bf Problem too large for good cache performance}: Certain machines
      with lower memory bandwidths (slow memory access) attempt to 
      compensate by having a very large cache.  Thus, if a significant
      portion of an application fits within the cache, the program will achieve very 
      good performance; if the code is too large, the performance can degrade markedly.
      To analyze whether this situation affects a particular code, one can
      try plotting the total flop rate as a function of problem
      size.  If the flop rate decreases rapidly at some point, then the
      problem may likely be too large for the cache size. 
\item {\bf Inconsistent timings}:  Inconsistent timings are likely due to other
      users on the machine, thrashing (using more virtual memory than available
      physical memory), or paging in of the initial executable.  
      Section~\ref{sec:profaccuracy} provides information on overcoming paging
      overhead when profiling a code. We have found on all systems that if you 
      follow all the advise above your timings will be consistent within a variation
      of less than five percent.
\end{itemize}

%---------------------------------------------------------------------
\chapter{Other PETSc Features}

\section{Runtime Options} \sindex{runtime options} \sindex{options}
\sindex{command line options}
\label{sec:options}

Allowing the user to modify parameters and options easily at runtime
is very desirable for many applications.  PETSc provides a simple
mechanism to enable such customization.  To print a list of
available options for a given program, simply specify the option 
\trl{-help} (or \trl{-h}) at runtime, e.g., \findex{-help} \findex{-h}
\begin{verse}
    mpirun -np 1 ex1 -help
\end{verse}

Note that all runtime options correspond to particular PETSc routines
that can be explicitly called from within a program to set compile-time
defaults.   For many applications it is natural to use a combination
of compile-time and runtime choices.  For example, when solving a linear
system, one could explicitly specify use of the Krylov subspace
technique BiCGStab by calling
\begin{verse}
   KSPSetType(ksp,KSPBCGS);
\end{verse}
One could then override this choice at runtime with the option
\begin{verse}
    -ksp\_type tfqmr
\end{verse}
to select the Transpose-Free QMR algorithm. (See Chapter~\ref{ch:sles} for details.)

The remainder of this section discusses details of runtime options.

\subsection{The Options Database}

Each PETSc process maintains a database of option names and values
(stored as text strings). This database is generated with the command
\trl{PETScInitialize()}, which is listed below in its C/C++ and
Fortran variants, respectively:
\begin{verse}
  PetscInitialize(int *argc,char ***args,char *file,char *help);\\
   call PetscInitialize(character file,integer ierr)
\end{verse}
The arguments \trl{argc} and \trl{args} (in the C/C++ version only) are
the usual command line arguments, while the \trl{file} is a name of
a file that can contain additional options. 
By default this file is called \trl{.petscrc} \findex{.petscrc} in the 
user's home directory.  The user can also specify options via the
environmental variable \trl{PETSC_OPTIONS}.  \findex{PETSC_OPTIONS} 
The options are processed in the following order:
\begin{itemize}
\item file
\item environmental variable
\item command line
\end{itemize}
Thus, the command line options supersede the environmental variable
options, which in turn supersede the options file.  

The file format for specifying options is 
\begin{verse}
   -optionname possible\_value\\
   -anotheroptionname possible\_value\\
   ...
\end{verse}
All of the option names must begin with a dash (-) and have no intervening 
spaces.  Note that the option values cannot
have intervening spaces either, and tab characters cannot be used
between the option names and values.
The user can employ any naming convention.  For uniformity throughout
PETSc, we employ the format \trl{-package_option} (for instance, 
\trl{-ksp_type} and \trl{-mat_view_info}).

Users can specify an alias for any option name (to avoid typing the 
sometimes lengthy default name) by adding an alias to the 
\trl{.petscrc} \findex{.petscrc} file in the format
\findex{alias} \sindex{alias}
\begin{verse}
   alias -newname -oldname
\end{verse}
For example,
\begin{verse}
   alias -kspt -ksp\_type\\
   alias -sd -start\_in\_debugger
\end{verse}
Comments can be placed in the .petscrc file by using one of the
following symbols in the first column of a line: \trl{\#, \%}, or \trl{!}.

\subsection{User-Defined PetscOptions}

Any subroutine in a PETSc program can add entries to the database with the 
command \findex{PetscOptionsSetValue()}
\begin{verse}
  PetscOptionsSetValue(char *name,char *value);
\end{verse}
though this is rarely done.
To locate options in the database, one should use the
commands \findex{PetscOptionsHasName()} \findex{PetscOptionsGetInt()}
\begin{verse}
  PetscOptionsHasName(char *pre,char *name,PetscTruth *flg);\\
  PetscOptionsGetInt(char *pre,char *name,int *value,PetscTruth *flg);\\
  PetscOptionsGetReal(char *pre,char *name,double *value,PetscTruth *flg);\\
  PetscOptionsGetString(char *pre,char *name,char *value,int maxlen,PetscTruth *flg);\\
  PetscOptionsGetStringArray(char *pre,char *name,char **values,int *maxlen,PetscTruth *flg);\\
  PetscOptionsGetIntArray(char *pre,char *name,int *value,int *nmax,PetscTruth *flg);\\
  PetscOptionsGetRealArray(char *pre,char *name,double *value, int *nmax,PetscTruth *flg);
\end{verse}
All \findex{PetscOptionsGetReal()}
of \findex{PetscOptionsGetString()} \findex{PetscOptionsGetIntArray()}
these \findex{PetscOptionsGetRealArray()}
routines set \trl{flg=PETSC_TRUE} if the corresponding option was found, \trl{flg=PETSC_FALSE} if it
was not found.  The optional argument
\trl{pre} indicates that the true name of the option is the given name
(with the dash ``-'' removed) prepended by the prefix \trl{pre}.
Usually \trl{pre} should be set to \trl{PETSC_NULL} (or \trl{PETSC_NULL_CHARACTER}
for Fortran); its purpose is to
allow someone to rename all the options in a package without knowing
the names of the individual options.  For example, when using block
Jacobi preconditioning, the KSP and PC methods used on the individual
blocks can be controlled via the options \trl{-sub_ksp_type} and \trl{
-sub_pc_type}. \sindex{block Jacobi}

\subsection{Keeping Track of Options}

One useful means of keeping track of user-specified runtime options is
use of \trl{-optionstable}, which prints to \trl{stdout} during \trl{
PetscFinalize()} a table of all runtime options that the user has
specified.  \findex{-optionstable} A related option is \trl{-optionsleft},
\findex{-optionsleft} which prints the options table and indicates
any options that have {\em not} been requested upon a call to \trl{
PetscFinalize()}.  This feature is useful to check whether an option
has been activated for a particular PETSc object (such as a solver or
matrix format), or whether an option name may have been accidentally
misspelled.

\section{Viewers: Looking at PETSc Objects} \label{sec:viewers}

PETSc employs a consistent scheme for examining, printing, and 
saving objects through commands of the form
\begin{verse}
  XXXView(XXX obj,PetscViewer viewer);
\end{verse}
Here \trl{obj} is any PETSc object of type
\trl{XXX}, \findex{PetscViewer} where \trl{XXX} 
is \trl{Mat}, \trl{Vec}, \trl{SNES}, etc. There are several
predefined viewers:
\begin{itemize}
\item Passing in a zero for the viewer causes the object to be printed 
      to the screen; this is most useful when viewing an object in 
      a debugger.
\item \trl{PETSC_VIEWER_STDOUT_SELF} \findex{PETSC_VIEWER_STDOUT_SELF} and 
      \trl{PESC_VIEWER_STDOUT_WORLD} \findex{PETSC_VIEWER_STDOUT_WORLD}
      cause the object to be printed to the screen.

\item \trl{PETSC_VIEWER_DRAW_SELF} \findex{PETSC_VIEWER_DRAW_SELF} and 
      \trl{PETSC_VIEWER_DRAW_WORLD} \findex{PETSC_VIEWER_DRAW_WORLD} causes the 
      object to be drawn in a default X window.
\item Passing in a viewer obtained by
      \trl{PetscViewerDrawOpenX()} causes the object to be displayed graphically.
\item To save an object to a file in ASCII format, the user creates
      the viewer object with the command
      \trl{PetscViewerASCIIOpen(MPI_Comm comm, char* file, PetscViewer *viewer)}.  
      This object is \findex{PetscViewerASCIIOpen()}
      analogous to \trl{PETSC_VIEWER_STDOUT_SELF} (for a communicator of
      \trl{MPI_COMM_SELF}) and 
      \trl{PETSC_VIEWER_STDOUT_WORLD} (for a parallel communicator).
\item To save an object to a file in binary format, the user creates
      the viewer object with the command
      \trl{PetscViewerBinaryOpen(MPI_Comm} \trl{comm,char* file,PetscViewerBinaryType} \trl{type,
      PetscViewer *viewer)}. \findex{PetscViewerBinaryOpen}  Details of binary
      I/O are discussed below.
\item Vector and matrix objects can be passed to a running Matlab process
      with a viewer created by \trl{PetscViewerMatlabOpen(MPI_Comm comm,
      char *machine, int port, PetscViewer *viewer)}. \findex{PetscViewerMatlabOpen()} 
      On the Matlab side, one must first run \trl{v = openport(int port)}
      and then \trl{A = receive(v)} to obtain the matrix or vector. Once all
      objects have been received, the port can be closed from the Matlab end
      with \trl{closeport(v)}. On the PETSc side, one should destroy
      the viewer object with \findex{PetscViewerDestroy()} \trl{
      PetscViewerDestroy()}. The corresponding Matlab \trl{mex}
      files are located in \trl{${PETSC_DIR}/src/viewer/impls/matlab}.
\end{itemize}

The user can control the format of ASCII printed objects with viewers 
created by \trl{PetscViewerASCIIOpen()} by calling
\begin{verse}
  PetscViewerSetFormat(PetscViewer viewer,int format);
\end{verse}  
\findex{PETSC_VIEWER_ASCII_DEFAULT} \findex{PETSC_VIEWER_ASCII_MATLAB} 
\findex{PETSC_VIEWER_ASCII_IMPL} \findex{PetscViewerSetFormat()}
Possible formats include 
\trl{PETSC_VIEWER_ASCII_DEFAULT}, \trl{PETSC_VIEWER_ASCII_MATLAB}, and
\break \trl{PETSC_VIEWER_ASCII_IMPL}.  The implementation-specific format, 
\trl{PETSC_VIEWER_ASCII_IMPL}, displays the object in the most natural way
for a particular implementation.  For example, when viewing a block 
diagonal matrix that has been created with \trl{MatCreateSeqBDiag()},
\trl{PETSC_VIEWER_ASCII_IMPL} prints by diagonals, while \trl{PETSC_VIEWER_ASCII_DEFAULT}
uses the conventional row-oriented format.

The routines
\begin{verse}
  PetscViewerPushFormat(PetscViewer viewer,int format);\\
  PetscViewerPopFormat(PetscViewer viewer);
\end{verse} 
allow one to temporarily change the format of a viewer.
\findex{PetscViewerPushFormat()} \findex{PetscViewerPopFormat}

As discussed above, one can output PETSc objects in binary format by
first opening a binary viewer with \trl{PetscViewerBinaryOpen()} and
then using \trl{MatView()}, \trl{VecView()}, etc.  The corresponding
routines for input of a binary object have the form \trl{XXXLoad()}.  In
particular, matrix and vector binary input is handled by the
following routines: \findex{MatLoad()} \findex{VecLoad()}
\begin{verse}
  MatLoad(PetscViewer viewer,MatType outtype,Mat *newmat);\\
  VecLoad(PetscViewer viewer,Vec *newvec);
\end{verse}
These routines generate parallel matrices and vectors if the viewer's
communicator has more than one processor.  The particular matrix and
vector formats are determined from the options database; see the
manual pages for details.

One can provide additional information about matrix data for matrices
stored on disk by providing an optional file \trl{matrixfilename.info},
where \trl{matrixfilename} is the name of the file containing the matrix.
The format of the optional file is the same as the \trl{.petscrc} file 
and can (currently) contain the following:
\begin{verse}
   -matload\_block\_size <bs>\\
   -matload\_bdiag\_diags <s1,s2,s3,...>
\end{verse}
The block size indicates the size of blocks to use if the matrix is
read into a block oriented data structure (for example, 
\trl{MATSEQBDIAG} or \trl{MATMPIBAIJ}). The diagonal information 
\trl{s1,s2,s3,...} indicates
which (block) diagonals in the matrix have nonzero values.
% Section \ref{sec:bdiag} gives details.

\section{Debugging} \sindex{debugging} \label{sec:debugging}

PETSc programs may be debugged using one of the two options below.
\begin{itemize}
\item \trl{-start_in_debugger} \trl{[noxterm,dbx,xxgdb]} \trl{[-display name]} 
     - start all processes in debugger
\item \trl{-on_error_attach_debugger} \trl{[noxterm,dbx,xxgdb]}
      \trl{[-display name]} - start debugger only on encountering an error
\end{itemize}
Note that, in general, debugging MPI programs cannot be done in the usual
manner of starting the programming in the debugger (because then it cannot
set up the MPI communication and remote processes).

By default the GNU debugger {\em gdb} is used when \trl{-start_in_debugger}
or \trl{-on_error_attach_debugger} is specified. \sindex{debugging}
To employ either {\em xxgdb} or the common UNIX debugger {\em dbx}, one uses
command line options as indicated above. On HP-UX machines the debugger
{\em xdb} should be used instead of {\em dbx}; on RS/6000 machines the
{\em xldb} debugger is supported as well.
By  default, the debugger will be started in a new xterm (to enable 
running separate debuggers on each process), unless the option 
\trl{noxterm} is used.
In order to handle the MPI startup phase, the debugger command ``cont'' 
should be used to continue execution of the program within the debugger.
Rerunning the program through the debugger requires terminating 
the first job and restarting the processor(s); the usual ``run'' 
option in the debugger will not correctly handle the MPI startup and
should not be used.  Not all debuggers work on all machines, so the user
may have to experiment to find one that works correctly.

You can select a subset of the processors to be debugged (the rest just run 
without the debugger) with the option
\begin{verse}
  -debugger\_nodes node1,node2,...
\end{verse}
where you simply list the nodes you want the debugger to run with.

\section{Error Handling} \sindex{errors} \sindex{debugging} 

Errors are handled through the routine \trl{PetscError()}. 
\findex{PetscError()} This routine
checks a stack of error handlers and calls the one on the top.  
If the stack is empty, it selects \trl{PetscTraceBackErrorHandler()}, 
which \findex{PetscTraceBackErrorHandler()} tries to print a traceback. 
A new error handler can be put on the stack with
\begin{verse}
  PetscPushErrorHandler(int (*HandlerFunction)(int line,char *dir,char *file,\\
                                char *message,int number,void*),void *HandlerContext)
\end{verse}
The arguments to \trl{HandlerFunction()} are the line number where 
the error occurred, the file in which the error was detected, the corresponding
directory, the error message, the error integer, and the \trl{HandlerContext.}
The routine \findex{PetscPushErrorHandler()}
\begin{verse} 
  PetscPopErrorHandler()
\end{verse}
removes the last error handler and discards it. \findex{PetscPopErrorHandler()}

PETSc provides two additional error handlers besides 
\trl{PetscTraceBackErrorHandler()}:
\findex{PetscAbortErrorHandler()} \findex{PetscAttachErrorHandler()}
\begin{verse}
   PetscAbortErrorHandler()\\
   PetscAttachErrorHandler()
\end{verse}
\trl{PetscAbortErrorHandler()} calls abort on encountering an error, while
\trl{PetscAttachErrorHandler()} attaches a debugger to the running process
if an error is detected. At runtime, these error handlers can be set
with the options \trl{-on_error_abort} or \trl{
-on_error_attach_debugger} \trl{[noxterm, dbx, xxgdb, xldb]} \trl{[-display DISPLAY]}.

All PETSc calls can be traced (useful for determining where a program is 
hanging without running in the debugger) with the option
\begin{verse}
  -log\_trace [filename]
\end{verse}
where \trl{filename} is optional. By default the traces are printed to the 
screen.  This can also be set with the 
command \trl{PetscLogTraceBegin(FILE*)}. \findex{-log_trace} \findex{PetscLogTraceBegin()}


It is also possible to trap signals by using the \sindex{signals}
command \findex{PetscPushSignalHandler()}
\begin{verse}
  PetscPushSignalHandler( int (*Handler)(int,void *),void *ctx);
\end{verse}
The default handler \trl{PetscDefaultSignalHandler()} 
calls \findex{PetscDefaultSignalHandler()} 
\trl{PetscError()} and then terminates. In general, a signal in PETSc
indicates a catastrophic failure.  Any error hander that the user provides
should try to clean up only before exiting.  By default all PETSc programs
use the default signal handler, although the user can turn this off 
at runtime with the 
option \trl{-no_signal_handler} \findex{-no_signal_handler}.

There is a separate signal handler for floating-point exceptions.
\sindex{floating-point exceptions} \sindex{IEEE floating point} 
The option \trl{-fp_trap} turns on the floating-point trap at runtime,
and the routine \findex{-fp_trap} \findex{PetscSetFPTrap()} \findex{PETSC_FP_TRAP_ON}
\begin{verse}
  PetscSetFPTrap(int flag);
\end{verse}
can be used in-line.
A \trl{flag} of \trl{PETSC_FP_TRAP_ON} \findex{PETSC_FP_TRAP_OFF}
indicates that floating-point exceptions should be trapped,
while a value of \trl{PETSC_FP_TRAP_OFF} (the default) indicates that they 
should be ignored.  Note that on certain machines, in particular 
the IBM RS/6000, trapping is very expensive.

A small set of macros is used to make the error handling lightweight.
These macros are used throughout the PETSc libraries and can be employed
by the application  \findex{SETERRQ()} \findex{CHKERRQ()}
programmer as well.  When an error is first detected, 
one should set it by calling
\begin{verse}
   SETERRQ(int flag,int pflag,char *message);
\end{verse}
The user should check the return codes for all PETSc routines (and
possibly user-defined routines as well) with 
\begin{verse}
  ierr = PetscRoutine(...);CHKERRQ(int ierr);
\end{verse}
Likewise, all memory allocations should be checked with 
\begin{verse}
  ierr = PetscMalloc(n*sizeof(double),\&ptr);CHKERRQ(ierr);
\end{verse}
If this procedure is followed throughout all of the user's libraries 
and codes, any error will by default generate a clean traceback of 
the location \findex{SETERRQ()} \findex{CHKERRQ()}
of the error. 

Note that the macro \trl{__FUNCT__} is used to keep track of
routine names during error tracebacks.  Users need not worry about this
macro in their application codes; however, users can take advantage of this feature
if desired by setting this macro before each user-defined routine
that may call \trl{SETERRQ()}, \trl{CHKERRQ()}.
A simple example of usage is given below.
\begin{verse}
    \#undef \_\_FUNCT\_\_  \\
    \#define \_\_FUNCT\_\_ "MyRoutine1"\\
    int MyRoutine1() { \\
        /* code here */\\
        return 0;\\
    }
\end{verse}

\section{Incremental Debugging} \sindex{incremental debugging}

When developing large codes, one is often in the position of having a
correctly (or at least believed to be correctly) running code; making
a change to the code then changes the results for some unknown reason.
Often even determining the precise point at which the old and new
codes diverge is a major pain.  In other cases, a code generates
different results when run on different numbers of processors,
although in exact arithmetic the same answer is expected. (Of course,
this assumes that {\em exactly} the same solver and parameters are
used in the two cases.)
 
PETSc provides some support for determining exactly where in the code
the computations lead to different results. First, compile both programs
with different names.  Next, start running
both programs as a single MPI job. This procedure is dependent on the particular
MPI implementation being used.
For example, when using MPICH on workstations, 
{\em procgroup} files can be used to specify the processors on which the job is
to be run.  Thus, to run two programs, \trl{old} and \trl{new},
each on two processors, one should create the procgroup file with the
following contents:
\begin{verse}
   local 0\\
   workstation1 1 /home/bsmith/old\\
   workstation2 1 /home/bsmith/new\\
   workstation3 1 /home/bsmith/new\\
\end{verse}
(Of course, workstation1, etc. can be the same machine.) Then, one can
execute the command
\begin{verse}
   mpirun -p4pg <procgroup\_filemame> old -compare <tolerance> [options]
\end{verse}
Note that the same runtime options must be used for the two programs.
The first time an inner product or norm detects an inconsistency larger
than \trl{<tolerance>}, PETSc will generate an error. The usual runtime
options \trl{-start_in_debugger} and \trl{-on_error_attach_debugger} may 
be used. \findex{-compare}  The user can also place the commands 
\begin{verse}
   PetscCompareDouble()\\
   PetscCompareScalar()\\
   PetscCompareInt()
\end{verse}
\findex{PetscCompareDouble()} \findex{PetscCompareScalar()} \findex{PetscCompareInt()}
in portions of the application code to check for consistency between
the two versions.
 
% -----------------------------------------------------------------------------
\section{Complex Numbers} \sindex{complex numbers} \label{sec:complex}

PETSc supports the use of complex numbers in application programs
written in C, C++, and Fortran.  To do so, we employ C++ versions of
the PETSc libraries in which the basic ``scalar'' datatype, given in
PETSc codes by \trl{Scalar}, is defined as \trl{complex} (or \trl{
complex<double>} for machines using templated complex class
libraries).  To work with complex numbers,
the user should compile the PETSc libraries (including the Fortran
interface library) and the application code with
\trl{BOPT=[g_complex,O_complex]} for debugging, optimized,
and profiling versions, respectively.  The file \trl{${PETSC_DIR}/docs/installation/index.htm}
provides detailed instructions for installing PETSc.

We recommend using optimized Fortran kernels for some key numerical
routines with complex numbers (such as matrix-vector products, vector
norms, etc.) instead of the default C++ routines.  See the ``Complex
Numbers'' section of the file \trl{${PETSC_DIR}/docs/installation/index.htm} for
details on building these kernels.  This implementation exploits the
maturity of Fortran compilers while retaining the identical user
interface.  For example, on rs6000 machines, the base single-node
performance when using the Fortran kernels is 4-5 times faster than
the default C++ code.

Recall that each variant of the PETSc libraries is stored in a
different directory, given by
\break \trl{${PETSC_DIR}/lib/lib${BOPT}/${PETSC_ARCH}}, according to the
architecture and \trl{BOPT} optimization variable.  Thus, the libraries for complex
numbers are maintained separately from those for real
numbers.  When using any of the complex numbers versions of PETSc,
{\em all} vector and matrix elements are treated as complex,
even if their imaginary components are zero.
Of course, one can elect to use only the real parts of the complex
numbers when using the complex versions of the PETSc libraries;
however, when working {\em only} with real numbers in a code,
one should use a version of PETSc for real numbers for best efficiency.

The program \trl{${PETSC_DIR}/src/sles/examples/tutorials/ex11.c}
solves a linear system with a complex
coefficient matrix.  Its Fortran counterpart is
\trl{${PETSC_DIR}/src/sles/examples/tutorials/ex11f.F}.

\section{Emacs Users} \sindex{Emacs} \label{sec:emacs}

\sindex{Emacs} \sindex{etags, in Emacs}
If users develop application codes on UNIX machines using Emacs (which we
highly recommend), the \trl{etags} feature can be used to search PETSc 
files quickly and efficiently.  To use this feature, one should 
first check if the file,
\trl{${PETSC_DIR}/TAGS} exists.  If this file is
not present, it should be generated by
running \trl{make} \trl{etags} from the PETSc home directory. 
Once the file exists, from 
Emacs the user should issue
the command 
\begin{verse}
 M-x visit-tags-table
\end{verse}
 where ``\trl{M}''
denotes the Emacs Meta key, and enter the 
name of the \trl{TAGS} file. Then the command ``\trl{M-.}'' will cause Emacs 
to find the file and line number where a desired PETSc function 
is defined.  Any string in any of the PETSc files can be found with the 
command ``\trl{M-x} \trl{tags-search}''. To find repeated occurrences, 
one can simply use ``\trl{M-,}'' to find the next occurrence.


\section{Parallel Communication}

When used in a message-passing environment, all communication \sindex{MPI}
within
PETSc is done through MPI, the message-passing interface standard
\cite{MPI-final}.  Any file that includes \trl{petsc.h} (or any other 
PETSc include file), can freely use any MPI routine.

% ---------------------------------------------------------------
\section{Graphics}
\sindex{graphics}

PETSc graphics components are not intended to compete with 
high-quality graphics packages.  Instead, they are intended to be 
easy to use interactively with PETSc programs. We urge users
to generate their publication-quality graphics using a
professional graphics package. If a user wants to hook
certain packages in PETSc, he or she should send a message to 
petsc-maint@mcs.anl.gov, and we will see whether it is reasonable
to try to provide direct interfaces.

\subsection{Windows as PetscViewers}
For drawing predefined PETSc objects such as matrices and vectors, one must 
first create a viewer using the 
command \findex{PetscViewerDrawOpenX()}
\begin{verse}
  PetscViewerDrawOpenX(MPI\_Comm comm,char *display,char *title,int x,\\
                       int y,int w,int h,PetscViewer *viewer);
\end{verse}
This viewer may be passed to any of the \trl{XXXView()} routines.
To draw into the viewer, one must obtain the \trl{Draw} object with the
command \findex{PetscViewerDrawGetDraw()}
\begin{verse}
  PetscViewerDrawGetDraw(PetscViewer viewer,PetscDraw *draw);
\end{verse}
Then one can call any of the \trl{DrawXXX} commands on the \trl{draw}
object. If one obtains the \trl{draw} object in this manner, 
one does not call the \trl{DrawOpenX()} command discussed below.

\findex{PETSC_VIEWER_DRAW_WORLD} \findex{PETSC_VIEWER_DRAW_SELF} 
Predefined viewers, \trl{PETSC_VIEWER_DRAW_WORLD} 
and \trl{PETSC_VIEWER_DRAW_SELF}, may be used at any time. Their initial
use will cause the appropriate window to be created.

\medskip
By default, PETSc drawing tools employ a private colormap,
which remedies the problem of poor color choices for contour plots due
to an external program's mangling of the colormap (e.g, Netscape tends
to do this).
Unfortunately, this causes flashing of colors as the mouse is moved
between the PETSc windows and other windows.  Alternatively, a shared
colormap can be used via the option \trl{-draw_x_shared_colormap}.

\subsection{Simple PetscDrawing}

One can open a window that is not associated with a viewer directly 
under the X11 Window System with the
command \findex{DrawOpenX()} \sindex{X windows}
\begin{verse}
  PetscDrawOpenX(MPI\_Comm comm,char *display,char *title,int x,\\
                 int y,int w,int h,PetscDraw *win);
\end{verse}
All drawing routines are done relative to the windows coordinate system 
and viewport. By default the drawing coordinates are from \trl{(0,0)} to 
\trl{(1,1)}, where \trl{(0,0)} indicates the lower left corner of the 
window. The application program can change the window coordinates with the 
command \findex{DrawSetCoordinates()} \sindex{coordinates}
\begin{verse}
  PetscDrawSetCoordinates(PetscDraw win,double xl,double yl,double xr,double yr);
\end{verse}
By default, graphics will be drawn in the entire window. To restrict the 
drawing to a portion of the window, one may 
use the command \findex{DrawSetViewPort()}
\begin{verse}
  PetscDrawSetViewPort(PetscDraw win,double xl,double yl,double xr,double yr);
\end{verse}
These arguments, which indicate the fraction of the window in which the 
drawing should be done, must satisfy 
$ 0 \le {\tt xl} \le {\tt xr} \le 1 $ and $ 0 \le {\tt yl} \le {\tt yr} \le 1.$ 

To draw a line, one uses
 the command \findex{DrawLine()} \sindex{lines, drawing}
\begin{verse}
  PetscDrawLine(PetscDraw win,double xl,double yl,double xr,double yr,int cl);
\end{verse}
The argument \trl{cl} indicates the color (which is an integer between 0 and 255)
of the line. A list of predefined colors may be found in \trl{include/petscdraw.h}
and includes \trl{DRAW_BLACK}, \trl{DRAW_RED}, \trl{DRAW_BLUE} etc.

To ensure that all graphics actually have been displayed, one should use 
\sindex{flushing, graphics} the
command \findex{DrawFlush()}
\begin{verse}
  PetscDrawFlush(PetscDraw win);
\end{verse}
When displaying by using double buffering, which is set with the
command \findex{DrawSetDoubleBuffer()} \sindex{double buffer}
\begin{verse}
  PetscDrawSetDoubleBuffer(PetscDraw win);
\end{verse}
{\em all} processors must call \findex{DrawSynchronizedFlush()}
\begin{verse}
  PetscDrawSynchronizedFlush(PetscDraw win);
\end{verse}
in order to swap the buffers. From the options database one may use 
\trl{-draw_pause n}, which \findex{-draw_pause} causes the PETSc application 
to pause \trl{n} seconds at each \trl{DrawPause()}. A time of \trl{-1}
indicates that the application should pause until receiving mouse 
input from the user.

Text can be drawn with either of the two \findex{DrawStringVertical()}
commands \findex{DrawString()} \sindex{text, drawing}
\begin{verse}
  PetscDrawString(PetscDraw win,double x,double y,int color,char *text);\\
  PetscDrawStringVertical(PetscDraw win,double x,double y,int color,char *text);
\end{verse}
The user can set the text font size or determine it with the 
commands \findex{DrawStringSetSize()} \findex{DrawStringGetSize()}
\begin{verse}
  PetscDrawStringSetSize(PetscDraw win,double width,double height);\\
  PetscDrawStringGetSize(PetscDraw win,double *width,double *height);
\end{verse}

\subsection{Line Graphs}
PETSc includes a set of routines for manipulating simple two-dimensional
graphs. These routines, which begin with \trl{DrawAxisDraw()}, are usually 
not used directly by the application programmer.  Instead, the programmer 
employs the line graph routines to draw simple line graphs.
As shown in the program, within Figure~\ref{fig:plot}, line graphs 
are created with the command \findex{DrawLGCreate()} \sindex{line graphs}
\begin{verse}
  PetscDrawLGCreate(PetscDraw win,int ncurves,PetscDrawLG *ctx);
\end{verse}
The argument \trl{ncurves} indicates how many curves are to be drawn.
Points can be added to each of the curves with the 
command \findex{DrawLGAddPoint()}
\begin{verse}
  PetscDrawLGAddPoint(PetscDrawLG ctx,double *x,double *y);
\end{verse}
The arguments \trl{x} and \trl{y} are arrays containing the next 
point value for each curve.
Several points for each curve may be added with \findex{DrawLGAddPoints()}
\begin{verse}
  PetscDrawLGAddPoints(PetscDrawLG ctx,int n,double **x,double **y);
\end{verse}

The line graph is drawn (or redrawn) with the command \findex{DrawLGDraw()}
\begin{verse}
  PetscDrawLGDraw(PetscDrawLG ctx);
\end{verse}
A line graph that is no longer needed can be destroyed with the 
command \findex{DrawLGDestroy()}
\begin{verse}
  PetscDrawLGDestroy(PetscDrawLG ctx);
\end{verse}
To plot new curves, one can reset a linegraph with the
command \findex{DrawLGReset()}
\begin{verse}
  PetscDrawLGReset(PetscDrawLG ctx);
\end{verse}
The line graph automatically determines the range of values to 
display on the two axes.  The user can change these defaults with the 
command \findex{DrawLGSetLimits()}
\begin{verse}
  PetscDrawLGSetLimits(PetscDrawLG ctx,double xmin,double xmax,double ymin,double ymax);
\end{verse}

It is also possible to change the display of the axes and to label
them. This procedure is done by first obtaining the axes context with the 
command \findex{DrawLGGetAxis()} \sindex{axis, drawing}
\begin{verse}
  PetscDrawLGGetAxis(PetscDrawLG ctx,PetscDrawAxis *axis);
\end{verse}
One can set the axes' colors and labels, respectively, by using the
commands \findex{DrawAxisSetColors()} \findex{DrawAxisSetLabels()}
\begin{verse}
  PetscDrawAxisSetColors(PetscDrawAxis axis,int axis\_lines,int ticks,int text);\\
  PetscDrawAxisSetLabels(PetscDrawAxis axis,char *top,char *x,char *y);
\end{verse}

\begin{figure}[H]
{\small
\fileinclude{../../../src/sys/src/draw/examples/tests/ex3.c}
}
\caption{Example of PetscDrawing Plots}
\label{fig:plot}
\end{figure}

It is possible to turn off all graphics with the option \findex{-nox}
\trl{-nox}. This
will prevent any windows from being opened or any drawing actions to be done.
This is useful for running large jobs when the graphics overhead is too
large, or for timing.

\subsection{Graphical Convergence Monitor}
For both the linear and nonlinear solvers default routines
allow one to graphically monitor convergence of the iterative method.
These are accessed via the command line with 
\trl{-ksp_xmonitor} and \trl{-snes_xmonitor}. \findex{-ksp_xmonitor}
\findex{-snes_xmonitor} See also Sections \ref{sec:kspmonitor} and
\ref{sec:snesmonitor}. 

The two functions used are \trl{KSPLGMonitor()} \findex{KSPLGMonitor()}
and \trl{KSPLGMonitorCreate()}  \findex{KSPLGMonitorCreate()}. These 
can easily be modified to serve specialized needs.


\subsection{Disabling Graphics at Compile Time}

\sindex{graphics, disabling}
To disable all x-window-based graphics, edit the file
\trl{${PETSC_DIR}/bmake/${PETSC_ARCH}/packages} and comment the variable
\trl{PETSC_DHAVE_X11}.  Then (re)compile the PETSc libraries.


%----------------------------------------------------------------------
\chapter{Makefiles}
\label{ch:makefiles}

This chapter describes the design of the PETSc makefiles, which are the
key to managing our code portability across a wide variety of UNIX and Windows systems.

\section{Our Makefile System}

To make a program named \trl{ex1}, one may use the command
\begin{verse}
   make BOPT=[g,O] PETSC\_ARCH=arch  ex1
\end{verse}
which will compile a debugging, optimized, or profiling version
of the example and automatically link the appropriate libraries.  The
architecture, \trl{arch}, is one of \trl{solaris, rs6000, IRIX,
hpux}, etc. Note
that when using command line options with make (as illustrated above),
one must {\em not} place spaces on either side of the ``='' signs.
The variables \trl{BOPT} and 
\trl{PETSC_ARCH} can also be set as environmental
variables.  Although PETSc is written in C, it can be compiled with a 
C++ compiler.  For many C++ users this may be the preferred route. To compile
with the C++ compiler, one should use the option \trl{BOPT=g_c++} or 
\trl{BOPT=O_c++}.  \sindex{C++}
The options \trl{BOPT=g_complex} and \trl{BOPT=O_complex}
will create versions that use complex double-precision numbers. 
\sindex{complex numbers}

\subsection{Makefile Commands} \label{sec:common}

The directory \trl{${PETSC_DIR}/bmake} contains virtually all
makefile commands and customizations to enable portability across
different architectures.  Most makefile commands for maintaining the
PETSc system are defined in the file \trl{${PETSC_DIR}/bmake/common}.  
These commands, which process all appropriate files within the
directory of execution, include
\begin{itemize}
\item \trl{lib} - Updates the PETSc libraries based on the source code
      in the directory.
\item \trl{libfast} - Updates the libraries faster.  Since
      \trl{libfast} recompiles all source files in the directory at once,
      rather than individually, this command saves time when many files
      must be compiled.
\item \trl{clean} - Removes garbage files.
\end{itemize}

The \trl{tree} command enables the user to execute a particular action
within a directory and all of its subdirectories.  The action is specified
by \trl{ACTION=[action]}, where \trl{action} is one of the basic commands
listed above. For example, if the command
\begin{verse}
   make BOPT=g ACTION=lib tree
\end{verse}
were executed from the directory \trl{${PETSC_DIR}/src/sles/ksp},
the debugging library for all Krylov subspace solvers would be built.

\subsection{Customized Makefiles}
\label{sec:custom}

The directory \trl{${PETSC_DIR}/bmake} contains a subdirectory for each 
architecture that contains machine-specific information, enabling the
portability of our makefile system.
For instance, for Sun workstations running OS 5.7, the 
directory is called \trl{solaris}.  Each architecture directory contains
three makefiles:
\begin{itemize}
\item \trl{packages} - locations of all needed packages for a particular site. 
      This file (discussed below) is usually the only one that the user 
      needs to alter.
\item \trl{variables} - definitions of the compilers, linkers, etc.
\item \trl{rules} - some build rules specific to this machine.
\end{itemize}

The architecture independent makefiles, are located in
\trl{${PETSC_DIR}/bmake/common}, and the machine-spcecific
makefiles get included from here.

\section{PETSc Flags}
\label{sec:makeflags}

PETSc has several flags that determine how the source code will be
compiled.  The default flags for particular versions are specified by
the variable \trl{PETSCFLAGS} within the base files of \trl{
${PETSC_DIR}/bmake/${PETSC_ARCH}}, discussed in Section
~\ref{sec:custom}.  The flags include
\begin{itemize}
\item \trl{PETSC_USE_DEBUG} - The PETSc debugging options are activated. We 
      recommend always using this. \findex{PETSC_USE_DEBUG}
\item \trl{PETSC_USE_COMPLEX} - The version with scalars represented 
      as complex numbers is used. \findex{PETSC_USE_COMPLEX}
\item \trl{PETSC_USE_LOG} - Various monitoring statistics on floating-point operations,
      and message-passing activity are kept. \findex{PETSC_USE_LOG}
\end{itemize}

\subsection{Sample Makefiles}

Maintaining portable PETSc makefiles is very simple. In Figures
\ref{fig:make1}, \ref{fig:make2}, and \ref{fig:make3} we present three
sample makefiles.  

The first is a ``minimum'' makefile for maintaining
a single program that uses the PETSc libraires.
The most important line in this makefile is the line starting with \trl{include}:
\begin{verse}
   include \${PETSC\_DIR}/bmake/common/base
\end{verse}
This line includes other makefiles that provide the needed definitions
and rules for the particular base PETSc installation (specified by
\trl{${PETSC_DIR}}) and architecture (specified by
\trl{${PETSC_ARCH}}).  (See \ref{sec:running} for information on
setting these environmental variables.)  As listed in the sample
makefile, the appropriate \trl{include} file is automatically
completely specified; the user should {\em not} alter this statement
within the makefile.

\begin{figure}[H]
{\small
\begin{verbatim}
   ALL: ex2

   CFLAGS   = 
   FFLAGS   = 
   CPPFLAGS =
   FPPFLAGS =

   include ${PETSC_DIR}/bmake/common/base

   ex2: ex2.o chkopts
           ${CLINKER} -o ex2 ex2.o  ${PETSC_LIB}
           ${RM} ex2.o
\end{verbatim}
}
\caption{Sample PETSc Makefile for a Single Program}
\label{fig:make1}
\end{figure}


\findex{PETSC_LIB} \findex{PETSC_LIB}
Note that the variable \trl{${PETSC_LIB}} (as listed on the link
line in the above makefile) specifies {\em all} of the various PETSc
libraries in the appropriate order for correct linking.  For users who
employ only a specific PETSc component, can use alternative variables
like  \trl{${PETSC_SYS_LIB}}, \trl{${PETSC_VEC_LIB}}, 
\trl{${PETSC_MAT_LIB}}, \trl{${PETSC_DM_LIB}},
\trl{${PETSC_SLES_LIB}}, \trl{${PETSC_SNES_LIB}} or
\trl{${PETSC_TS_LIB}}.

The second sample makefile, given in Figure~\ref{fig:make2},
controls the generation of several example programs. 

\begin{figure}[H]
{\small
\begin{verbatim}
   CFLAGS   = 
   FFLAGS   = 
   CPPFLAGS =
   FPPFLAGS =

   include ${PETSC_DIR}/bmake/common/base

   ex1: ex1.o 
        -${CLINKER} -o ex1 ex1.o  ${PETSC_LIB}
        ${RM} ex1.o
   ex2: ex2.o 
        -${CLINKER} -o ex2 ex2.o  ${PETSC_LIB}
        ${RM} ex2.o
   ex3: ex3.o 
        -${FLINKER} -o ex3 ex3.o  ${PETSC_FORTRAN_LIB} ${PETSC_LIB}
        ${RM} ex3.o
   ex4: ex4.o 
        -${CLINKER} -o ex4 ex4.o  ${PETSC_LIB}
        ${RM} ex4.o

   runex1:
        -@${MPIRUN} ex1
   runex2:
        -@${MPIRUN} -np 2 ex2 -mat_seqdense -optionsleft
   runex3:
        -@${MPIRUN} ex3 -v -log_summary
   runex4:
        -@${MPIRUN} -np 4 ex4 -trdump

   RUNEXAMPLES_1 = runex1 runex2
   RUNEXAMPLES_2 = runex4
   RUNEXAMPLES_3 = runex3
   EXAMPLESC     = ex1.c ex2.c ex4.c
   EXAMPLESF     = ex3.F
   EXAMPLES_1    = ex1 ex2
   EXAMPLES_2    = ex4
   EXAMPLES_3    = ex3

   include ${PETSC_DIR}/bmake/common_test
\end{verbatim}
}
\caption{Sample PETSc Makefile for Several Example Programs}
\label{fig:make2}
\end{figure}

Again, the most important line in this makefile is the \trl{include}
line that includes the files defining all of the macro variables.
Some additional variables that can be used in the makefile are defined
as follows:
\begin{itemize}
\item \trl{CFLAGS, FFLAGS} - User specified additional options for the C compiler and
        fortran compiler.
\item \trl{CPPFLAGS, FPPFLAGS} - User specified additional flags for the C preprocessor
        and fortran preprocesor.
\item \trl{CLINKER, FLINKER} - the C and Fortran linkers. 
\item \trl{RM} - the remove command for deleting files.
\item \trl{EXAMPLES_1} - examples that will be built with
             \trl{make BOPT=[g,O] examples} (see Section~\ref{sec:common})
\item \trl{RUNEXAMPLES_1} - examples that will be run with
             \trl{make runexamples} (see Section~\ref{sec:common})
\item \trl{EXAMPLESC} - all C examples that will be checked in/out of RCS
             with \trl{make ci} and \trl{make co} (not generally
             needed by users).
\item \trl{EXAMPLESF} - all Fortran examples that will be checked in/out of
             RCS with \trl{make ci} and \trl{make co} (not generally
             needed by users).
\item \trl{PETSC_LIB} - all of the base PETSc libraries.
             \findex{PETSC_LIB}
\item \trl{PETSC_FORTRAN_LIB} - the PETSc Fortran interface 
             library. \findex{PETSC_FORTRAN_LIB}
\end{itemize}
Note that the PETSc example programs are divided into several
categories, which currently include: 
\begin{tabbing}
1234\= EXAMPLESx\= \kill
\> \trl{EXAMPLES_1} \> - basic C suite used in installation tests\\
\> \trl{EXAMPLES_2} \> - additional C suite including graphics\\
\> \trl{EXAMPLES_3} \> - basic Fortran .F suite\\
\> \trl{EXAMPLES_4} \> - subset of 1 and 2 that runs on only a single processor\\
\> \trl{EXAMPLES_5} \> - examples that require complex numbers\\
\> \trl{EXAMPLES_6} \> - C examples that do not work with complex numbers\\
\> \trl{EXAMPLES_8} \> - Fortran .F examples that do not work with complex numbers\\
\> \trl{EXAMPLES_9} \> - uniprocessor version of 3\\
\> \trl{EXAMPLES_10} \> - Fortran .F examples that require complex numbers\\
\end{tabbing}

We next list in Figure \ref{fig:make3} a makefile that maintains a PETSc 
library.  Although most users do not need to understand or deal with such
makefiles, they are also easily used.

\begin{figure}[H]
{\small
\begin{verse}
   ALL: lib\\

   CFLAGS   =\\  
   SOURCEC  = sp1wd.c spinver.c spnd.c spqmd.c sprcm.c\\
   SOURCEF  = degree.f  fnroot.f genqmd.f qmdqt.f rcm.f fn1wd.f gen1wd.f \ \\
              genrcm.f qmdrch.f rootls.f fndsep.f gennd.f qmdmrg.f qmdupd.f\\
   SOURCEH  = \\
   OBJSC    = sp1wd.o spinver.o spnd.o spqmd.o sprcm.o\\
   OBJSF    = degree.o  fnroot.o genqmd.o qmdqt.o rcm.o fn1wd.o gen1wd.o \ \\
              genrcm.o qmdrch.o rootls.o fndsep.o gennd.o qmdmrg.o qmdupd.o\\
   LIBBASE  = libpetscmat\\
   MANSEC   = Mat

   include \${PETSC\_DIR}/bmake/common/base
\end{verse}
}
\caption{Sample PETSc Makefile for Library Maintenance}
\label{fig:make3}
\end{figure}

The library's name is \trl{libpetscmat.a}, and the source files being added
to it are indicated by \trl{SOURCEC} (for C files) and \trl{SOURCEF} (for 
Fortran files). Note that the \trl{OBJSF} and \trl{OBJSC} are identical 
to \trl{SOURCEF} and \trl{SOURCEC}, respectively, except they use the
suffix \trl{.o} rather than \trl{.c} or \trl{.f}. 

The variable \trl{MANSEC} indicates that any manual pages generated
from this source should be included in the \trl{Mat} section. 

\section{Limitations}

This approach to portable makefiles has some minor limitations, including
the following:
\begin{itemize}
\item Each makefile must be called ``makefile''.
\item Each makefile can maintain at most one archive library.
\end{itemize}


%------------------------------------------------------------------

\chapter{Unimportant and Advanced Features of Matrices and Solvers}
\label{ch:advanced}

This chapter introduces additional features of the PETSc matrices and solvers.
Since most PETSc users should not need to use these features, 
we recommend skipping this chapter during an initial reading.

\medskip \medskip

\section{Extracting Submatrices} \sindex{submatrices}

One can extract a (parallel) submatrix from a given (parallel) using
\begin{verse}
  MatGetSubMatrix(Mat A,IS rows,IS cols,int csize,MatReuse call,Mat *B);
\end{verse}
This extracts the \trl{rows} and \trl{col}umns of the matrix \trl{A} into \trl{B}. If 
call is  \trl{MAT_INITIAL_MATRIX} \findex{MAT_INITIAL_MATRIX} it will create the matrix
\trl{B}. If call is \trl{MAT_REUSE_MATRIX} \findex{MAT_REUSE_MATRIX} it will reuse the \trl{B}
created with a previous call. \findex{MatGetSubMatrix} The argument \trl{csize} is ignored 
on sequential matrices, for parallel matrices it determines the ``local columns'' if the matrix
format supports this concept. Often one can use the default by passing in \trl{PETSC_DECIDE}.
To create a \trl{B} matrix that may be multiplied with a vector \trl{x} one can use
\begin{verse}
  VecGetLocalSize(x,\&csize);\\
  MatGetSubMatrix(Mat A,IS rows,IS cols,int csize,MatReuse call,Mat *B);
\end{verse}


\medskip \medskip

\section{Matrix Factorization} \sindex{factorization}
\label{sec:matfactor}

Normally, PETSc users will access the matrix solvers through the 
SLES interface, as discussed in Chapter \ref{ch:sles}, but the underlying 
factorization and triangular solve routines are also directly 
accessible to the user.

\medskip \medskip

The LU and Cholesky \sindex{Cholesky}
matrix factorizations are split into \sindex{LU}
two or three stages depending on the user's needs. The first stage is 
to calculate an ordering for the matrix.  The ordering generally is 
done to reduce fill in a sparse factorization; it does not make much 
sense for a dense matrix. \findex{MatGetOrdering()} \sindex{reorder}
\begin{verse}
  MatGetOrdering(Mat matrix,MatOrderingType type,IS* rowperm,IS* colperm); 
\end{verse}
The currently available alternatives for the ordering \trl{type} are 
\begin{itemize}
\item \trl{MATORDERING_NATURAL} - Natural
\item \trl{MATORDERING_ND} - Nested Dissection
\item \trl{MATORDERING_1WD} - One-way Dissection
\item \trl{MATORDERING_RCM} - Reverse Cuthill-McKee
\item \trl{MATORDERING_QMD} - Quotient Minimum Degree
\end{itemize}
These orderings can also be set through the options database by specifying 
one of the following:  \trl{-pc_lu_ordering_type} \trl{natural}, \trl{-pc_lu_ordering_type} \trl{nd}, 
\trl{-pc_lu_ordering_type} \trl{1wd}, \trl{-pc_lu_ordering_type} \trl{rcm}, \trl{-pc_lu_ordering_type}
\trl{ qmd}.
\findex{MATORDERING_NATURAL} \findex{MATORDERING_ND} \findex{MATORDERING_1WD}
\findex{MATORDERING_RCM} \findex{MATORDERING_QMD} \sindex{nested dissection}
\sindex{one-way dissection} \sindex{reverse Cuthill-McKee} 
\sindex{quotient minimum degree} \findex{-pc_lu_ordering_type}
Certain matrix formats may support only a subset of these; more options may 
be added. Check the manual pages for up-to-date information. All of these orderings are 
symmetric at the moment; ordering routines that are 
not symmetric may be added. Currently we support orderings only for 
sequential matrices.

Users can add their own ordering routines 
by providing a function with the calling sequence
\begin{verse}
   int reorder(Mat A,MatOrderingType type,IS* rowperm,IS* colperm);
\end{verse}
Here \trl{A} is the matrix for which we wish to generate a new ordering, 
\trl{type} may be ignored and \trl{rowperm} and \trl{colperm} are the row
and column permutations generated by the ordering routine.
The user registers the ordering routine
with the command
\begin{verse}
  MatOrderingRegisterDynamic(MatOrderingType inname,char *path,char *sname,\\
                             int (*reorder)(Mat,MatOrderingType,IS*,IS*)));
\end{verse}
The \findex{MatOrderingRegisterDynamic()} \sindex{ordering} \sindex{matrix ordering}
input argument \trl{inname} is a string of the user's choice, \trl{iname} is either 
an ordering defined in \trl{petscmat.h} or a users string,
to indicate one is introducing a new ordering, while the output
See the code in \trl{src/mat/impls/order/sorder.c} and other files in that 
directory for examples on how the reordering routines may be written.

Once the reordering routine has been registered,
it can be selected for use at runtime with the
command line option \trl{-pc_lu_ordering_type sname}.  If reordering directly,
the user should provide the \trl{name} as the second input argument of
\trl{MatGetOrdering()}.

The following routines perform complete, in-place, symbolic, and numerical 
factorizations for symmetric and nonsymmetric matrices, respectively:
\begin{verse}
  MatCholeskyFactor(Mat matrix,IS permutation,double pf);\\
  MatLUFactor(Mat matrix,IS rowpermutation,IS columnpermutation,MatLUInfo *info); 
\end{verse}
The argument \trl{info->fill > 1} is the predicted fill
expected in the factored matrix, as a ratio of the original fill. 
For example, \trl{info->fill=2.0} would indicate that one expects the factored
matrix to have twice as many nonzeros as the original.
\findex{MatCholeskyFactor()} \findex{MatLUFactor()}

For sparse matrices it is very unlikely that the factorization 
is actually done in-place. More likely, new space is allocated 
for the factored matrix and the old space deallocated, but to the 
user it appears in-place because the factored matrix replaces
the unfactored matrix.

The \sindex{symbolic factorization}
two \findex{MatCholeskyFactorSymbolic()} \findex{MatLUFactorSymbolic()}
factorization 
stages \findex{MatCholeskyFactorNumeric()} \findex{MatLUFactorNumeric()}
can also be performed separately, by using the out-of-place mode:
\begin{verse}
  MatCholeskyFactorSymbolic(Mat matrix,IS perm, double pf,Mat *result);\\
  MatLUFactorSymbolic(Mat matrix,IS rowperm,IS colperm,MatLUInfo *info,Mat *result);\\
  MatCholeskyFactorNumeric(Mat matrix,Mat *result);\\
  MatLUFactorNumeric(Mat matrix, Mat *result);
\end{verse}
In this case, the contents of the matrix \trl{result} is undefined between 
the symbolic and numeric factorization stages. 
It is possible to reuse the symbolic factorization. For the second and 
succeeding factorizations, one simply calls the numerical factorization with a 
new input \trl{matrix} and the {\em same} factored \trl{result} matrix.
It is {\em essential} that the new input matrix 
have   % subjunctive ... must be ``have'', not ``has''
exactly the same nonzero structure as the original factored matrix.
(The numerical factorization merely overwrites the numerical values in the 
factored matrix and does not disturb the symbolic portion, thus enabling
reuse of the symbolic phase.)
In general, calling \trl{XXXFactorSymbolic} with a dense matrix will 
do nothing except allocate the new matrix; the \trl{XXXFactorNumeric} 
routines will do all of the work. 

Why provide the plain \trl{XXXfactor} routines when one could simply 
call the two-stage routines? The answer is that if one desires in-place 
factorization of a sparse matrix, the intermediate stage between the 
symbolic and numeric phases cannot be stored in a \trl{result} matrix, and
it does not make sense to store the intermediate values
inside the original matrix 
that is being transformed.  We originally made the combined factor routines
do either in-place or out-of-place factorization, but then decided that 
this approach was not needed and could easily lead to confusion.

We do not currently support sparse matrix factorization with pivoting
for numerical stability. This is because trying to both reduce fill
and do pivoting can become quite complicated. Instead, we provide a 
poor stepchild substitute. After one has obtained a reordering, with
\trl{MatGetOrdering(Mat A,MatOrdering type,IS *row,IS *col)} one
may call
\begin{verse}
  MatReorderForNonzeroDiagonal(Mat A,double tol,IS row, IS col);
\end{verse}
which will try to reorder the columns to ensure that no values along 
the diagonal are smaller than \trl{tol} in a absolute value. If small 
values are detected and corrected for, a nonsymmetric
permutation of the rows and columns will result. This is not guaranteed to work, 
but may help if one was simply unlucky in the original ordering.
\findex{MatReorderForNonzeroDiagonal} When using the SLES solver interface
the options \trl{-pc_ilu_nonzeros_along_diagonal <tol>} \findex{-pc_ilu_nonzeros_along_diagonal}
and \findex{-pc_lu_nonzeros_along_diagonal}
\trl{-pc_lu_nonzeros_along_diagonal <tol>} may be used.  Here, \trl{tol}
is an optional tolerance to decide if a value is nonzero; by default it
is $ 1.e-10.$ 

Once a matrix has been factored, it is natural to solve linear systems.
The following four routines enable this process: \findex{MatSolve()} 
\begin{verse}
  MatSolve(Mat A,Vec x, Vec y);\\
  MatSolveTrans(Mat A, Vec x, Vec y);\\
  MatSolveAdd(Mat A,Vec x, Vec y, Vec w);\\
  MatSolveTransAdd(Mat A, Vec x, Vec y, Vec w);
\end{verse}
The \findex{MatSolveTrans()} \findex{MatSolveAdd()}
matrix \findex{MatSolveTransAdd()}
\trl{A} of these routines must have been obtained from a 
factorization routine; otherwise, an error will be generated.
In general, the user should use the SLES solvers introduced in the 
next chapter rather than using these factorization and solve routines
directly.

\section{Unimportant Details of KSP}

Again, virtually all users should use KSP through the SLES interface
and, thus, will not need to know the details that follow. 

It is possible to generate a Krylov subspace context with the 
command \findex{KSPCreate()}
\begin{verse}
  KSPCreate(MPI\_Comm comm,KSP *kps);
\end{verse}
Before using the Krylov context, one must set the matrix-vector multiplication routine and
the preconditioner with the 
commands \findex{PCSetOperators()} \findex{KSPSetPC()}
\begin{verse}
  PCSetOperators(PC pc,Mat mat,Mat pmat,MatStructure flag);\\
  KSPSetPC(KSP ksp,PC pc);
\end{verse}
In addition, the KSP solver must be initialized with \findex{KSPSetUp()}
\begin{verse}
  KSPSetUp(KSP ksp);
\end{verse}
Solving a linear system is done with the command \findex{KSPSolve()}
\begin{verse}
  KSPSolve(KSP ksp,int *its);
\end{verse}
Finally, the KSP context should be destroyed with \findex{KSPDestroy()}
\begin{verse}
  KSPDestroy(KSP ksp);
\end{verse}

It may seem strange to put the matrix in the preconditioner rather
than directly in the KSP; this decision was the result of much
agonizing. The reason is that for SSOR with Eisenstat's trick, and 
certain other preconditioners, the
preconditioner has to change the matrix-vector multiply.  This 
procedure could not
be done cleanly if the matrix were stashed in the KSP context that
PC cannot access.

Any preconditioner can supply not 
only the preconditioner, but also a routine that essentially performs a
complete Richardson step. The reason for this is mainly SOR. To 
use SOR in the Richardson framework, that is,
\[
  u^{n+1} = u^{n} + B(f - A u^{n}), 
\]
is much more expensive than just updating the values.
With this addition it is reasonable to state that {\em all} our
iterative methods are obtained by combining a preconditioner from 
the \trl{PC} component with a Krylov method from the \trl{KSP}
component. This strategy makes things much simpler conceptually, so 
(we hope)
clean code will result. {\em Note}: We had this idea already implicitly in 
older versions of SLES, but, for instance, just doing Gauss-Seidel
with Richardson in old SLES was much more expensive than it had to be. 
With PETSc this should not be a problem. 

\section{Unimportant Details of PC}

Most users will obtain their preconditioner contexts from the SLES
context with the command \trl{SLESGetPC()}. It is possible to create,
manipulate, and destroy PC contexts directly, although this capability
should rarely be needed. To create a PC context, one uses the command
\findex{PCCreate()}
\begin{verse}
  PCCreate(MPI\_Comm comm,PC *pc);
\end{verse}
The routine \findex{PCSetType()}
\begin{verse}
  PCSetType(PC pc,PCType method);
\end{verse}
sets the preconditioner method to be used. 
The two routines \findex{PCSetOperators()} \findex{PCSetVector()}
\begin{verse}
  PCSetOperators(PC pc,Mat mat,Mat pmat,MatStructure flag);\\
  PCSetVector(PC pc,Vec vec);
\end{verse}
set the matrices and type of vector that are to be used with 
the preconditioner.  The \trl{vec} argument is needed by the PC routines 
to determine the format of the vectors. 
The routine \findex{PCGetOperators()}
\begin{verse}
  PCGetOperators(PC pc,Mat *mat,Mat *pmat,MatStructure *flag);
\end{verse}
returns the values set with \trl{PCSetOperators()}.

\findex{PCApply()} \findex{PCApplyTrans()} 
The preconditioners in PETSc can be used in several ways.  The two
most basic routines simply apply the preconditioner or its transpose
and are given, respectively, by
\begin{verse}
  PCApply(PC pc,Vec x,Vec y);\\
  PCApplyTrans(PC pc,Vec x,Vec y);
\end{verse}
In particular, for a preconditioner matrix, \trl{B}, that has
been set via \trl{PCSetOperators(pc,A,B,flag)},
the routine \trl{PCApply(pc,x,y)} computes $y = B^{-1} x$
by solving the linear system $By = x$ with the specified preconditioner
method.

\findex{PCApplyBAorAB()}
\findex{PCApplyBAorABTrans()} \findex{PCApplyRichardson()}
Additional preconditioner routines are
\begin{verse}
  PCApplyBAorAB(PC pc,PCSide right,Vec x,Vec y,Vec work,int its);\\
  PCApplyBAorABTrans(PC pc,PCSide right,Vec x,Vec y,Vec work,int its);\\
  PCApplyRichardson(PC pc,Vec x,Vec y,Vec work,PetscReal rtol,PetscReal atol, PetscReal dtolint its);
\end{verse}
The first two routines apply the action of the matrix followed by the
preconditioner or the preconditioner followed by the matrix depending
on whether the \sindex{Richardson's method} \trl{right} is
\trl{PC_LEFT} or \trl{PC_RIGHT}. The final routine applies \trl{its} iterations of
Richardson's method. \findex{PC_RIGHT} \findex{PC_LEFT} \sindex{preconditioning, right and left}
The last three routines are provided to improve
efficiency for certain Krylov subspace methods.

A PC context that is no longer needed can be destroyed with the 
command \findex{PCDestroy()}
\begin{verse}
  PCDestroy(PC pc);
\end{verse}

